# Source Generated with Decompyle++
# File: local_query_executor.cpython-312.pyc (Python 3.12)

'''
Execute local queries that require NO LLM calls.
All data comes from DB, ccxt API, or systemd.
'''
import os
import re
import subprocess
import time as _time
from db_config import get_conn, DB_CONFIG
import exchange_reader
import response_envelope

def _log(msg):
    print(f'[local_query] {msg}', flush=True)

_process_start_time = _time.time()

SYMBOL = os.getenv('SYMBOL', 'BTC/USDT:USDT')
APP_DIR = '/root/trading-bot/app'
REQUIRED_SERVICES = [
    'candles',
    'executor',
    'indicators',
    'news_bot',
    'pnl_watcher']
OPTIONAL_SERVICES = [
    'signal_logger',
    'vol_profile',
    'error_watcher']
WATCHED_SERVICES = REQUIRED_SERVICES + OPTIONAL_SERVICES
SERVICE_NAMES_KO = {
    'candles': 'ìº”ë“¤ ìˆ˜ì§‘',
    'executor': 'ì‹¤í–‰ê¸°(ì»¨íŠ¸ë¡¤ëŸ¬)',
    'indicators': 'ì§€í‘œ ê³„ì‚°',
    'news_bot': 'ë‰´ìŠ¤ ìˆ˜ì§‘',
    'signal_logger': 'ì‹œê·¸ë„ ê¸°ë¡',
    'vol_profile': 'ë³¼ë¥¨ í”„ë¡œíŒŒì¼',
    'error_watcher': 'ì—ëŸ¬ ê°ì‹œ',
    'pnl_watcher': 'ì†ìµ ê°ì‹œ'}

def _db():
    return get_conn(autocommit=True)


def _run(cmd, timeout=25):
    p = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
    out = (p.stdout or '') + (p.stderr or '')
    return (p.returncode, out.strip())


def execute(query_type=None, original_text=None):
    handlers = {
        'status_full': _status_full,
        'health_check': _health_check,
        'btc_price': _btc_price,
        'news_summary': _news_summary,
        'equity_report': _equity_report,
        'daily_report': _daily_report,
        'recent_errors': _recent_errors,
        'indicator_snapshot': _indicator_snapshot,
        'volatility_summary': _volatility_summary,
        'position_info': _position_exch,
        'position_exch': _position_exch,
        'orders_exch': _orders_exch,
        'account_exch': _account_exch,
        'position_strat': _position_strat,
        'risk_config': _risk_config,
        'snapshot': _snapshot,
        'fact_snapshot': _fact_snapshot,
        'score_summary': _score_summary,
        'db_health': _db_health,
        'claude_audit': _claude_audit,
        'macro_summary': _macro_summary,
        'db_monthly_stats': _db_monthly_stats,
        'audit_report': _audit_report,
        'news_applied': _news_applied,
        'news_ignored': _news_ignored,
        'db_coverage': _db_coverage,
        'evidence': _evidence,
        'test_report_full': _test_report_full,
        'debug_version': _debug_version,
        'debug_router': _debug_router,
        'debug_health': _debug_health,
        'debug_db_coverage': _debug_db_coverage,
        'debug_news_sample': _debug_news_sample,
        'debug_news_reaction_sample': _debug_news_reaction_sample,
        'debug_backfill_status': _debug_backfill_status,
        'debug_backfill_dryrun': _debug_backfill_dryrun,
        'debug_state': _debug_state,
        'debug_news_filter_stats': _debug_news_filter_stats,
        'debug_backfill_enable': _debug_backfill_enable,
        'debug_backfill_start': _debug_backfill_start,
        'debug_backfill_pause': _debug_backfill_pause,
        'debug_backfill_resume': _debug_backfill_resume,
        'debug_backfill_stop': _debug_backfill_stop,
        'debug_backfill_log': _debug_backfill_log,
        'debug_news_gap_diagnosis': _debug_news_gap_diagnosis,
        'debug_storage': _debug_storage,
        'debug_news_path_sample': _debug_news_path_sample,
        'debug_news_path_stats': _debug_news_path_stats,
        'debug_system_stability': _debug_system_stability,
        'debug_once_lock_status': _debug_once_lock_status,
        'debug_once_lock_clear': _debug_once_lock_clear,
        'debug_backfill_ack': _debug_backfill_ack,
        'debug_gate_details': _debug_gate_details,
        'debug_order_throttle': _debug_order_throttle,
        'reconcile': _reconcile,
        'mctx_status': _mctx_status,
        'mode_params': _mode_params,
        'combined_snapshot': _combined_snapshot,
        'mode_performance': _mode_performance}
    handler = handlers.get(query_type, _unknown)
    return handler(original_text)


def _status_full(_text=None):
    (rc, out) = _run([
        'python3',
        f'{APP_DIR}/status_full.py'], timeout=35)
    if rc != 0:
        return f'âš  status_full ì‹¤íŒ¨(rc={rc})\n{out[-3500:]}'
    if len(out) > 3500:
        return out[-3500:]
    return out


def _classify_service_state(line, found):
    """Classify service state to OK/DOWN/UNKNOWN."""
    if not found:
        return 'UNKNOWN'
    ll = line.lower()
    if 'active running' in ll:
        return 'OK'
    if 'failed' in ll or 'dead' in ll:
        return 'DOWN'
    if 'inactive' in ll:
        return 'DOWN'
    if 'masked' in ll or 'activating' in ll:
        return 'UNKNOWN'
    return 'UNKNOWN'

STATE_ICONS = {'OK': 'âœ”', 'DOWN': 'âŒ', 'UNKNOWN': 'â“'}
STATE_KR = {'OK': 'ì •ìƒ', 'DOWN': 'ì¤‘ì§€', 'UNKNOWN': 'ë¯¸í™•ì¸'}


def _health_check(_text=None):
    (rc, out) = _run([
        'systemctl',
        'list-units',
        '--type=service'])
    if rc != 0:
        return 'âš  ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨'
    status_lines = []
    states = {}  # svc -> state
    ok_count = 0
    down_count = 0
    unknown_count = 0
    for svc in WATCHED_SERVICES:
        ko = SERVICE_NAMES_KO.get(svc, svc)
        found = False
        matched_line = ''
        for line in out.splitlines():
            if f'{svc}.service' not in line:
                continue
            found = True
            matched_line = line
            break
        state = _classify_service_state(matched_line, found)
        states[svc] = state
        icon = STATE_ICONS[state]
        state_kr = STATE_KR[state]
        is_required = svc in REQUIRED_SERVICES
        req_tag = '' if is_required else ' (ì„ íƒ)'
        if state == 'OK':
            status_lines.append(f'  {icon} {svc} ({ko}) â€” {state_kr}{req_tag}')
            ok_count += 1
        elif state == 'DOWN':
            detail = 'ì˜¤ë¥˜' if found and 'failed' in matched_line.lower() else 'ì¤‘ì§€'
            status_lines.append(f'  {icon} {svc} ({ko}) â€” {detail}{req_tag}')
            down_count += 1
        else:
            detail = 'ë¯¸ë“±ë¡' if not found else 'ë¯¸í™•ì¸'
            status_lines.append(f'  {icon} {svc} ({ko}) â€” {detail}{req_tag}')
            unknown_count += 1
    total = len(WATCHED_SERVICES)
    header = [
        'ğŸ©º ì„œë¹„ìŠ¤ ìƒíƒœ ìš”ì•½',
        f'  ì „ì²´: {total}ê°œ | ì •ìƒ: {ok_count} | ì¤‘ì§€: {down_count} | ë¯¸í™•ì¸: {unknown_count}',
        '']
    # ê²½ê³  ë©”ì‹œì§€
    req_down = [s for s in REQUIRED_SERVICES if states.get(s) == 'DOWN']
    req_unknown = [s for s in REQUIRED_SERVICES if states.get(s) == 'UNKNOWN']
    warnings = []
    if req_down:
        warnings.append(f'âš  í•„ìˆ˜ ì„œë¹„ìŠ¤ ì¤‘ì§€: {", ".join(req_down)}')
    if len(req_unknown) >= 2:
        warnings.append(f'âš  í•„ìˆ˜ ì„œë¹„ìŠ¤ ë¯¸í™•ì¸ {len(req_unknown)}ê°œ: {", ".join(req_unknown)}')
    if warnings:
        status_lines.append('')
        status_lines.extend(warnings)
    # DB ê¸°ë¡
    _log_service_health(states)
    return '\n'.join(header + status_lines)


def _log_service_health(states):
    """service_health_log í…Œì´ë¸”ì— í˜„ì¬ ìƒíƒœ ê¸°ë¡."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            for svc, state in states.items():
                cur.execute("""
                    INSERT INTO service_health_log (service, state)
                    VALUES (%s, %s)
                    ON CONFLICT DO NOTHING;
                """, (svc, state))
    except Exception:
        pass  # DB ë¯¸ìƒì„± ì‹œ ë¬´ì‹œ
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def get_service_health_snapshot():
    """Dual-source health check (systemctl --all + heartbeat DB).
    Same logic as _debug_health() but returns structured dict.
    Returns dict with per-service detail + aggregate counts.
    """
    from datetime import datetime, timezone

    # systemctl --all
    rc, sctl_out = _run(['systemctl', 'list-units', '--type=service', '--all'])

    # heartbeat DB
    heartbeats = {}
    hb_counts = {}
    hb_error = None
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT service, MAX(ts) AS last_ts, COUNT(*)
                FROM service_health_log
                GROUP BY service;
            """)
            for svc, last_ts, cnt in cur.fetchall():
                heartbeats[svc] = last_ts
                hb_counts[svc] = cnt
    except Exception as e:
        hb_error = str(e)
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass

    now_utc = datetime.now(timezone.utc)
    services = {}

    for svc in WATCHED_SERVICES:
        reg = SERVICE_REGISTRY.get(svc, {})
        expected_interval = reg.get('expected_interval_sec', 0)
        unit_name = reg.get('systemctl_unit', f'{svc}.service')

        # â”€â”€ Check 1: Heartbeat from DB â”€â”€
        hb_ts = heartbeats.get(svc)
        hb_state = None
        hb_reason = None
        age_sec = None
        hb_cnt = hb_counts.get(svc, 0)
        if hb_error:
            hb_reason = f'db_error:{hb_error[:40]}'
        elif hb_ts:
            if hb_ts.tzinfo is None:
                hb_ts = hb_ts.replace(tzinfo=timezone.utc)
            age_sec = int((now_utc - hb_ts).total_seconds())
            if expected_interval > 0:
                stale_threshold = 3 * expected_interval
                if age_sec < stale_threshold:
                    hb_state = 'OK'
                    hb_reason = f'heartbeat_fresh (age={age_sec}s)'
                elif hb_cnt <= 1 and age_sec < stale_threshold:
                    # Warmup: only 1 heartbeat row and within threshold â†’ UNKNOWN
                    hb_state = 'UNKNOWN'
                    hb_reason = f'warmup (age={age_sec}s, rows={hb_cnt})'
                else:
                    hb_state = 'DOWN'
                    hb_reason = f'heartbeat_stale (age={age_sec}s, threshold={stale_threshold}s)'
            else:
                hb_reason = 'missing_expected_interval'
        else:
            if hb_cnt <= 1:
                hb_state = 'UNKNOWN'
                hb_reason = 'warmup (no_heartbeat_rows)'
            else:
                hb_reason = 'no_heartbeat_rows'

        # â”€â”€ Check 2: systemctl process â”€â”€
        proc_state = None
        proc_reason = None
        if rc != 0:
            proc_reason = 'systemctl_failed'
        else:
            found = False
            matched_line = ''
            for line in sctl_out.splitlines():
                if unit_name in line:
                    found = True
                    matched_line = line
                    break
            if not found:
                proc_reason = f'unit_not_found ({unit_name})'
            else:
                ll = matched_line.lower()
                if 'active' in ll and 'running' in ll:
                    proc_state = 'OK'
                    proc_reason = 'active_running'
                elif 'failed' in ll:
                    proc_state = 'DOWN'
                    proc_reason = 'systemctl_failed'
                elif 'inactive' in ll and 'dead' in ll:
                    proc_state = 'DOWN'
                    proc_reason = 'inactive_dead'
                elif 'activating' in ll:
                    proc_reason = 'activating'
                elif 'masked' in ll:
                    proc_reason = 'masked'
                else:
                    proc_reason = f'parse_unknown'

        # â”€â”€ Final verdict: process alive + hb stale â†’ OK+WARN (not DOWN) â”€â”€
        if hb_state == 'OK':
            state = 'OK'
            reason = hb_reason
        elif proc_state == 'OK' and hb_state == 'DOWN':
            # Process is alive but heartbeat stale â†’ trust process, WARN only
            state = 'OK'
            reason = f'WARN: {hb_reason} (process alive)'
        elif proc_state == 'OK':
            state = 'OK'
            reason = proc_reason
        elif hb_state == 'DOWN':
            state = 'DOWN'
            reason = hb_reason
        elif proc_state == 'DOWN':
            state = 'DOWN'
            reason = proc_reason
        else:
            state = 'UNKNOWN'
            reasons = []
            if hb_reason:
                reasons.append(f'hb:{hb_reason}')
            if proc_reason:
                reasons.append(f'proc:{proc_reason}')
            reason = '; '.join(reasons) if reasons else 'no_check_source'

        services[svc] = {
            'state': state,
            'reason': reason,
            'age_sec': age_sec,
        }

    ok = [s for s, d in services.items() if d['state'] == 'OK']
    down = [s for s, d in services.items() if d['state'] == 'DOWN']
    unknown = [s for s, d in services.items() if d['state'] == 'UNKNOWN']
    req_down = [s for s in REQUIRED_SERVICES if services.get(s, {}).get('state') == 'DOWN']
    req_unknown = [s for s in REQUIRED_SERVICES if services.get(s, {}).get('state') == 'UNKNOWN']

    return {
        'ok': len(ok),
        'down': down,
        'unknown': unknown,
        'required_down': req_down,
        'required_unknown': req_unknown,
        'services': services,
        'health_check_ts': now_utc.isoformat(),
    }


def get_service_health_summary():
    """safety_manager í•˜ìœ„ í˜¸í™˜ wrapper â†’ get_service_health_snapshot() ìœ„ì„."""
    return get_service_health_snapshot()


def _btc_price(_text=None):
    import ccxt
    ex = ccxt.bybit({
        'enableRateLimit': True})
    t = ex.fetch_ticker('BTC/USDT')
    last = t.get('last')
    ts = t.get('datetime', '')
    return f'BTC/USDT í˜„ì¬ê°€: {last} (bybit) {ts}'


def _news_summary(text=None):
    (minutes, limit) = _parse_minutes_and_limit(text)
    env = os.environ.copy()
    env['DATABASE_URL'] = f"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}"
    env['NEWS_SUMMARY_MINUTES'] = str(minutes)
    env['NEWS_SUMMARY_LIMIT'] = str(limit)
    p = subprocess.run([
        'python3',
        f'{APP_DIR}/news_bot.py',
        '--summary'], capture_output=True, text=True, timeout=25, env=env)
    out = (p.stdout or '').strip()
    err = (p.stderr or '').strip()
    if p.returncode != 0:
        return f'DB ë‰´ìŠ¤ ìš”ì•½ ì‹¤íŒ¨(rc={p.returncode})\n{(err or out)[-3500:]}'
    if out:
        return out[:3500]
    return 'ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ'


def _equity_report(_text=None):
    (rc, out) = _run([
        'python3',
        f'{APP_DIR}/equity_report.py'], timeout=35)
    if rc != 0:
        return f'equity_report ì‹¤íŒ¨(rc={rc})\n{out[-3500:]}'
    if len(out) > 3500:
        return out[-3500:]
    return out


def _daily_report(_text=None):
    (rc, out) = _run([
        'python3',
        f'{APP_DIR}/daily_report.py'], timeout=35)
    if rc != 0:
        return f'daily_report ì‹¤íŒ¨(rc={rc})\n{out[-3500:]}'
    if len(out) > 3500:
        return out[-3500:]
    return out


def _recent_errors(_text=None):
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT ts, service, message
                FROM error_log
                ORDER BY ts DESC
                LIMIT 10;
            """)
            rows = cur.fetchall()
        if not rows:
            return 'ìµœê·¼ ì—ëŸ¬ ì—†ìŒ'
        lines = ['ğŸš¨ ìµœê·¼ ì—ëŸ¬ ëª©ë¡']
        for ts, svc, msg in rows:
            lines.append(f'  [{ts}] {svc}: {msg[:200]}')
        return '\n'.join(lines)
    except Exception as e:
        return f'ì—ëŸ¬ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _indicator_snapshot(_text=None):
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            # BTC current price from latest candle
            cur.execute("""
                SELECT c FROM candles
                WHERE symbol = %s AND tf = '1m'
                ORDER BY ts DESC LIMIT 1;
            """, (SYMBOL,))
            price_row = cur.fetchone()
            btc_price = float(price_row[0]) if price_row else None

            cur.execute("""
                SELECT ts, rsi_14, atr_14, bb_up, bb_mid, bb_dn,
                       ich_tenkan, ich_kijun, vol_spike, ma_50, ma_200
                FROM indicators
                WHERE symbol = %s AND tf = '1m'
                ORDER BY ts DESC LIMIT 1;
            """, (SYMBOL,))
            row = cur.fetchone()
        if not row:
            return 'ì§€í‘œ ë°ì´í„° ì—†ìŒ'
        (ts, rsi, atr, bb_up, bb_mid, bb_dn, ich_t, ich_k, vol_spike,
         ma_50, ma_200) = row
        lines = [
            f'ğŸ“Š ì§€í‘œ ìŠ¤ëƒ…ìƒ· ({ts})',
            f'  BTC í˜„ì¬ê°€: ${btc_price:,.1f}' if btc_price else '  BTC í˜„ì¬ê°€: N/A',
            f'  RSI(14): {rsi}',
            f'  ATR(14): {atr}',
            f'  BB: upper={bb_up} mid={bb_mid} lower={bb_dn}',
            f'  Ichimoku: tenkan={ich_t} kijun={ich_k}',
            f'  MA: 50={ma_50} 200={ma_200}',
            f'  Volume spike: {"YES" if vol_spike else "NO"}',
        ]
        return '\n'.join(lines)
    except Exception as e:
        return f'ì§€í‘œ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _volatility_summary(_text=None):
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT ts, atr_14, bb_up - bb_dn AS bb_width,
                       vol_spike
                FROM indicators
                WHERE symbol = %s AND tf = '1m'
                ORDER BY ts DESC LIMIT 5;
            """, (SYMBOL,))
            rows = cur.fetchall()
        if not rows:
            return 'ë³€ë™ì„± ë°ì´í„° ì—†ìŒ'
        lines = ['ğŸ“ˆ ë³€ë™ì„± ìš”ì•½ (ìµœê·¼ 5ê±´)']
        for ts, atr, bb_w, vs in rows:
            lines.append(f'  [{ts}] ATR={atr} BBí­={bb_w} spike={"Y" if vs else "N"}')
        return '\n'.join(lines)
    except Exception as e:
        return f'ë³€ë™ì„± ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _position_info(_text=None):
    try:
        data = exchange_reader.fetch_position()
        pos = data.get('exchange_position', 'NONE')
        if pos == 'NONE':
            return 'ğŸ“ í¬ì§€ì…˜(ê±°ë˜ì†Œ): ì—†ìŒ'
        qty = data.get('exch_pos_qty', 0)
        entry = data.get('exch_entry_price', 0)
        return f'ğŸ“ í¬ì§€ì…˜(ê±°ë˜ì†Œ): {pos} qty={qty} entry=${entry:,.2f}'
    except Exception as e:
        return f'í¬ì§€ì…˜ ì¡°íšŒ ì‹¤íŒ¨: {e}'


# â”€â”€ OpenClaw v3: Fact handlers (exchange_reader + response_envelope) â”€â”€


def _position_exch(_text=None):
    try:
        data = exchange_reader.fetch_position()
    except Exception as e:
        _log(f'_position_exch error: {e}')
        data = {'data_status': 'ERROR', 'exchange_position': 'UNKNOWN', 'error': str(e)}
    return response_envelope.format_position_exch(data)


def _orders_exch(_text=None):
    try:
        data = exchange_reader.fetch_open_orders()
    except Exception as e:
        _log(f'_orders_exch error: {e}')
        data = {'data_status': 'ERROR', 'orders': [], 'error': str(e)}
    return response_envelope.format_orders_exch(data)


def _account_exch(_text=None):
    try:
        data = exchange_reader.fetch_balance()
    except Exception as e:
        _log(f'_account_exch error: {e}')
        data = {'data_status': 'ERROR', 'total': 0, 'free': 0, 'used': 0, 'error': str(e)}
    return response_envelope.format_account_exch(data)


def _position_strat(_text=None):
    try:
        data = exchange_reader.fetch_position_strat()
    except Exception as e:
        _log(f'_position_strat error: {e}')
        data = {'data_status': 'ERROR', 'strategy_state': 'UNKNOWN', 'error': str(e)}
    return response_envelope.format_position_strat(data)


def _risk_config(_text=None):
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            import safety_manager
            limits = safety_manager._load_safety_limits(cur)
        return response_envelope.format_risk_config(limits)
    except Exception as e:
        _log(f'_risk_config error: {e}')
        return response_envelope.format_risk_config(None)
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _combined_snapshot(_text=None):
    """Combined status: regime + position + score + wait reason."""
    lines = []
    conn = None
    scores = {}
    regime_ctx = {}
    try:
        conn = _db()
        with conn.cursor() as cur:
            # 1. Current regime
            try:
                import regime_reader
                regime_ctx = regime_reader.get_current_regime(cur)
                regime = regime_ctx.get('regime', 'UNKNOWN')
                conf = regime_ctx.get('confidence', 0)
                lines.append(f'ëª¨ë“œ: {regime} (ì‹ ë¢°ë„: {conf}%)')
                if regime_ctx.get('in_transition'):
                    lines.append('  (ë ˆì§ ì „í™˜ ì¿¨ë‹¤ìš´ ì¤‘)')
            except Exception as e:
                lines.append(f'ëª¨ë“œ: ì¡°íšŒ ì‹¤íŒ¨ ({e})')

            # 2. Exchange position (4-block)
            try:
                fact_text = _fact_snapshot(_text)
                lines.append('')
                lines.append(fact_text)
            except Exception as e:
                lines.append(f'ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # 3. Score summary
            try:
                import score_engine
                scores = score_engine.compute_total()
                total = scores.get('total_score', 0)
                dominant = scores.get('dominant_side', '?')
                sig_stage = scores.get('signal_stage', '?')
                stage = scores.get('stage', 0)
                abs_score = scores.get('abs_score', 0)
                lines.append('')
                lines.append(f'ìŠ¤ì½”ì–´: total={total:+.1f}, ê¶Œê³ ê°•ë„: {sig_stage} (abs={abs_score:.0f})')
                lines.append(f'ë°©í–¥: {dominant}, ë¶„í• ë‹¨ê³„: stage {stage}/7')
            except Exception as e:
                lines.append(f'ìŠ¤ì½”ì–´ ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # 4. WAIT reason (what conditions are missing)
            _scores = scores
            _regime_ctx = regime_ctx
            try:
                wr, wd = exchange_reader.compute_wait_reason(cur=cur)
                lines.append('')
                if wr != 'READY':
                    lines.append(f'ëŒ€ê¸° ì‚¬ìœ : {wr}')
                    lines.append(f'  ë¶€ì¡± ì¡°ê±´: {wd}')
                    # Enumerate specific missing conditions
                    missing = _compute_missing_conditions(cur, _scores, _regime_ctx)
                    if missing:
                        for m in missing:
                            lines.append(f'  - {m}')
                else:
                    lines.append('ìƒíƒœ: ì§„ì… ëŒ€ê¸° ì¤‘ (ì¡°ê±´ ì¶©ì¡± ì‹œ ì£¼ë¬¸)')
            except Exception as e:
                lines.append(f'ëŒ€ê¸° ì‚¬ìœ  ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # 5. Signal suppression + Regime + ADD control (v14)
            try:
                import autopilot_daemon
                sp = autopilot_daemon.get_signal_policy_snapshot(cur)

                # 5a. Regime info
                lines.append('')
                lines.append('â”€â”€ REGIME â”€â”€')
                regime = sp.get('regime', 'UNKNOWN')
                regime_conf = sp.get('regime_confidence', 0)
                bbw = sp.get('regime_bbw_ratio')
                adx = sp.get('regime_adx')
                lines.append(f'REGIME: {regime} (conf={regime_conf}%)')
                bbw_str = f'{bbw:.2f}' if bbw is not None else 'N/A'
                adx_str = f'{adx:.1f}' if adx is not None else 'N/A'
                bbw_pct = sp.get('regime_bb_width_pct')
                bbw_pct_str = f'{bbw_pct:.2f}%' if bbw_pct is not None else 'N/A'
                lines.append(f'  ê·¼ê±°: BBW_ratio={bbw_str}, BB_WIDTH={bbw_pct_str}, ADX={adx_str}')
                lines.append(f'max_stage: {sp.get("max_stage", "?")}')
                if sp.get('in_transition'):
                    lines.append('  (ë ˆì§ ì „í™˜ ì¿¨ë‹¤ìš´ ì¤‘)')

                # 5b. Signal suppression
                lines.append('')
                lines.append('â”€â”€ ì‹ í˜¸ ì–µì œ â”€â”€')
                lines.append(f'start_stage: {sp["start_stage_policy"]}')
                lines.append(f'conf: {sp["conf_thresholds"]}')
                cd_long = sp.get('cooldown_LONG_remaining', 0)
                cd_short = sp.get('cooldown_SHORT_remaining', 0)
                lines.append(f'ì¬ì‹ í˜¸ ì¿¨ë‹¤ìš´: {sp["repeat_cooldown_sec"]}s (L={cd_long}s, S={cd_short}s)')
                for d in ('LONG', 'SHORT'):
                    ts_key = f'last_signal_{d}_ts'
                    if ts_key in sp:
                        lines.append(f'ìµœê·¼ {d}: {sp[ts_key]}')
                lines.append(f'ì–µì œ ì‚¬ìœ : {sp["last_suppress_reason"]}')
                if sp.get('stop_cooldown_active'):
                    lines.append(f'ì†ì ˆ ì¿¨ë‹¤ìš´: í™œì„± ({sp["stop_cooldown_remaining"]}s)')
                else:
                    lines.append('ì†ì ˆ ì¿¨ë‹¤ìš´: ë¹„í™œì„±')

                # 5c. ADD control
                lines.append('')
                lines.append('â”€â”€ ADD ì œì–´ â”€â”€')
                lines.append(f'ADD ê°„ê²©: {sp.get("add_min_interval_sec", "?")}s '
                             f'(ì”ì—¬: {sp.get("add_interval_remaining", 0)}s)')
                lines.append(f'ADD 30ë¶„: {sp.get("adds_30m_count", 0)}/{sp.get("adds_30m_limit", "?")}')
                lines.append(f'ë¦¬í…ŒìŠ¤íŠ¸ í•„ìˆ˜: {"YES" if sp.get("add_retest_required") else "NO"}')
                lines.append(f'EVENTâ†’ADD: {"ì°¨ë‹¨" if sp.get("event_add_blocked") else "í—ˆìš©"}')
                lines.append(f'ë™ì¼ë°©í–¥ ì¬ì§„ì… ì¿¨ë‹¤ìš´: {sp.get("same_dir_reentry_cooldown_sec", "?")}s')
                next_add = sp.get('next_add_earliest', 'NOW')
                lines.append(f'ë‹¤ìŒ ADD ê°€ëŠ¥: {next_add}')

                # 5d. Order throttle
                lines.append('')
                lines.append('â”€â”€ ORDER THROTTLE â”€â”€')
                lines.append(f'ì‹œê°„ë‹¹: {sp.get("throttle_hourly", "?")}')
                lines.append(f'10ë¶„ë‹¹: {sp.get("throttle_10min", "?")}')
                if sp.get('throttle_locked'):
                    lines.append(f'ì ê¸ˆ: {sp.get("throttle_lock_reason", "")}')
                try:
                    import order_throttle
                    ots = order_throttle.get_state_snapshot()
                    if ots.get('next_try_str'):
                        lines.append(f'next_try: {ots["next_try_str"]} ({ots.get("next_try_reason", "")})')
                    else:
                        lines.append('next_try: READY')
                    if ots.get('last_reject_reason'):
                        lines.append(f'ë§ˆì§€ë§‰ ê±°ë¶€: {ots["last_reject_reason"]}')
                except Exception:
                    pass

            except Exception as e:
                lines.append(f'ì •ì±… ì¡°íšŒ ì‹¤íŒ¨: {e}')

    except Exception as e:
        lines.append(f'combined_snapshot ì˜¤ë¥˜: {e}')
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass
    return '\n'.join(lines)


def _compute_missing_conditions(cur, scores, regime_ctx):
    """Enumerate specific unmet conditions for entry."""
    missing = []
    try:
        abs_score = scores.get('abs_score', 0) if scores else 0
        regime = regime_ctx.get('regime', 'UNKNOWN') if regime_ctx else 'UNKNOWN'

        if abs_score < 45:
            missing.append(f'total_score ë¶€ì¡± ({abs_score:.0f} < 45)')

        if regime == 'RANGE':
            # Check band proximity
            vah = regime_ctx.get('vah')
            val = regime_ctx.get('val')
            price = scores.get('price') if scores else None
            if vah and val and price:
                try:
                    near_val = abs(price - val) / val * 100 <= 0.3 if val > 0 else False
                    near_vah = abs(price - vah) / vah * 100 <= 0.3 if vah > 0 else False
                    if not near_val and not near_vah:
                        missing.append(f'ë°´ë“œ ê²½ê³„ ë¯¸ë„ë‹¬ (VAL={val:.0f} VAH={vah:.0f} price={price:.0f}, 0.3% ì´ë‚´ í•„ìš”)')
                except Exception:
                    pass

            # Check RSI (5m preferred, 1m fallback)
            rsi_5m = None
            try:
                for _tf in ('5m', '1m'):
                    cur.execute("""
                        SELECT rsi_14 FROM indicators
                        WHERE symbol = 'BTC/USDT:USDT' AND tf = %s
                        ORDER BY ts DESC LIMIT 1;
                    """, (_tf,))
                    rsi_row = cur.fetchone()
                    if rsi_row and rsi_row[0] is not None:
                        rsi_5m = float(rsi_row[0])
                        break
                if rsi_5m is not None and not (rsi_5m <= 30 or rsi_5m >= 70):
                    missing.append(f'RSI ì¡°ê±´ ë¯¸ë‹¬ (í˜„ì¬ {rsi_5m:.0f}, 30 ì´í•˜ ë˜ëŠ” 70 ì´ìƒ í•„ìš”)')
            except Exception:
                pass

        if regime == 'BREAKOUT':
            if not regime_ctx.get('breakout_confirmed'):
                missing.append('5m close-confirm ë¶€ì¡± (2ìº”ë“¤ í•„ìš”)')
    except Exception:
        pass
    return missing


def _fact_snapshot(_text=None):
    """Comprehensive fact snapshot: EXCHANGE + ORDER + STRATEGY_DB + GATE/WAIT.
    Gathers all execution context for full pipeline visibility."""
    try:
        exch_pos = exchange_reader.fetch_position()
    except Exception as e:
        _log(f'_fact_snapshot fetch_position error: {e}')
        exch_pos = {'data_status': 'ERROR', 'exchange_position': 'UNKNOWN', 'error': str(e)}
    try:
        strat_pos = exchange_reader.fetch_position_strat()
    except Exception as e:
        _log(f'_fact_snapshot fetch_position_strat error: {e}')
        strat_pos = {'data_status': 'ERROR', 'strategy_state': 'UNKNOWN', 'error': str(e)}
    try:
        orders = exchange_reader.fetch_open_orders()
    except Exception as e:
        _log(f'_fact_snapshot fetch_open_orders error: {e}')
        orders = {'data_status': 'ERROR', 'orders': [], 'error': str(e)}

    exec_ctx = {}
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            exec_ctx = exchange_reader.fetch_execution_context(cur)
    except Exception as e:
        _log(f'_fact_snapshot exec_ctx error: {e}')
        exec_ctx = {'error': str(e), 'wait_reason': 'UNKNOWN'}
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass

    return response_envelope.format_fact_snapshot(
        exch_pos, strat_pos, orders, exec_ctx)


def _snapshot(_text=None):
    try:
        exch_pos = exchange_reader.fetch_position()
    except Exception as e:
        _log(f'_snapshot fetch_position error: {e}')
        exch_pos = {'data_status': 'ERROR', 'exchange_position': 'UNKNOWN', 'error': str(e)}
    try:
        strat_pos = exchange_reader.fetch_position_strat()
    except Exception as e:
        _log(f'_snapshot fetch_position_strat error: {e}')
        strat_pos = {'data_status': 'ERROR', 'strategy_state': 'UNKNOWN', 'error': str(e)}
    try:
        orders = exchange_reader.fetch_open_orders()
    except Exception as e:
        _log(f'_snapshot fetch_open_orders error: {e}')
        orders = {'data_status': 'ERROR', 'orders': [], 'error': str(e)}

    conn = None
    gate_status = None
    switch_status = None
    wait_reason = None
    capital_info = None
    zone_check = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            import safety_manager
            gate_status = safety_manager.run_all_checks(cur)
            cur.execute(
                "SELECT enabled FROM trade_switch ORDER BY id DESC LIMIT 1;")
            row = cur.fetchone()
            switch_status = row[0] if row else None
            wr = exchange_reader.compute_wait_reason(cur, gate_status=gate_status)
            wait_reason = wr[0] if isinstance(wr, tuple) else wr

            # Capital info for display
            try:
                eq = safety_manager.get_equity_limits(cur)
                cur.execute('SELECT capital_used_usdt, stage FROM position_state WHERE symbol = %s;',
                            ('BTC/USDT:USDT',))
                ps_row = cur.fetchone()
                used = float(ps_row[0]) if ps_row and ps_row[0] else 0
                stage = int(ps_row[1]) if ps_row and ps_row[1] else 0
                # leverage from already-fetched exchange position (avoid redundant API call)
                lev = exch_pos.get('leverage', 0) if exch_pos.get('data_status') == 'OK' else 0
                lev_min = eq.get('leverage_min', 3)
                lev_max = eq.get('leverage_max', 8)
                capital_info = {
                    **eq,
                    'used_usdt': used,
                    'remaining_usdt': round(max(0, eq['operating_cap'] - used), 2),
                    'stage': stage,
                    'leverage_current': lev,
                    'leverage_rule': f'{lev_min}-{lev_max}x',
                }
            except Exception:
                pass

            # Zone check data for ZONE_CHECK section
            try:
                import regime_reader
                import autopilot_daemon
                import order_throttle

                regime_ctx = regime_reader.get_current_regime(cur)
                regime = regime_ctx.get('regime', 'UNKNOWN')

                # Current price
                cur.execute("SELECT mark_price FROM market_data_cache WHERE symbol = %s;",
                            ('BTC/USDT:USDT',))
                zc_price_row = cur.fetchone()
                zc_price = float(zc_price_row[0]) if zc_price_row and zc_price_row[0] else 0

                # BB data
                bb_data = autopilot_daemon._get_bb_data(cur)

                # Compute zones
                zones = autopilot_daemon._compute_entry_zones(zc_price, regime_ctx, bb_data)

                zone_check = {
                    'current_price': zc_price,
                    'regime': regime,
                    **zones,
                }

                # Anti-chase status (RANGE only)
                if regime == 'RANGE':
                    chase_ok, chase_reason = autopilot_daemon._check_anti_chase(cur, regime_ctx, dry_run=True)
                    if not chase_ok:
                        zone_check['chase_block'] = chase_reason

                # Daily trade count (FILLED basis)
                cur.execute("""
                    SELECT count(*) FROM execution_log
                    WHERE symbol = 'BTC/USDT:USDT' AND status = 'FILLED'
                      AND order_type IN ('OPEN', 'ADD')
                      AND last_fill_at >= (now() AT TIME ZONE 'Asia/Seoul')::date AT TIME ZONE 'Asia/Seoul';
                """)
                dtc_row = cur.fetchone()
                zone_check['daily_trade_count'] = int(dtc_row[0]) if dtc_row else 0

                # Throttle attempt count
                ts = order_throttle.get_throttle_status()
                zone_check['throttle_attempts_1h'] = ts.get('hourly_count', 0)
                zone_check['throttle_limit_1h'] = ts.get('hourly_limit', 12)

                # Signal policy snapshot
                try:
                    zone_check['signal_policy'] = autopilot_daemon.get_signal_policy_snapshot(cur)
                except Exception:
                    pass

            except Exception as e:
                _log(f'_snapshot zone_check error: {e}')

    except Exception as e:
        _log(f'_snapshot gate/switch error: {e}')
        if gate_status is None:
            gate_status = (False, f'ì¡°íšŒ ì‹¤íŒ¨: {e}')
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass

    return response_envelope.format_snapshot(
        exch_pos, strat_pos, orders, gate_status, switch_status, wait_reason,
        capital_info=capital_info, zone_check=zone_check)


def _score_summary(_text=None):
    try:
        import score_engine
        r = score_engine.compute_total()
        ne = r.get('news_event_score', 0)
        guarded = r.get('news_event_guarded', False)
        ne_detail = r.get('axis_details', {}).get('news_event', {})
        ne_comp = ne_detail.get('components', {})
        ne_details = ne_detail.get('details', {})
        weights = r.get('weights', {})
        total = r.get('total_score', 0)
        tech = r.get('tech_score', 0)
        pos = r.get('position_score', 0)
        regime = r.get('regime_score', 0)
        dominant = r.get('dominant_side', '?')
        stage = r.get('stage', '?')
        tech_w = weights.get('tech_w', 0.45)
        pos_w = weights.get('position_w', 0.25)
        regime_w = weights.get('regime_w', 0.25)
        news_w = weights.get('news_event_w', 0.05)
        # ì¶•ë³„ ê°€ì¤‘ ê¸°ì—¬ë„
        tech_c = tech * tech_w
        pos_c = pos * pos_w
        regime_c = regime * regime_w
        news_c = ne * news_w
        # Signal stage (from score engine: stg0/stg1/stg2/stg3)
        abs_score = r.get('abs_score', abs(total))
        signal_stage_label = r.get('signal_stage', f'stg{stage}')
        # Position stage (from position_state DB: 0-7 pyramid)
        pos_stage = 0
        pos_capital_pct = 0
        try:
            conn_ps = _db()
            with conn_ps.cursor() as cur_ps:
                cur_ps.execute(
                    "SELECT stage, capital_used_usdt FROM position_state WHERE symbol = %s;",
                    ('BTC/USDT:USDT',))
                ps_row = cur_ps.fetchone()
                if ps_row:
                    pos_stage = int(ps_row[0]) if ps_row[0] else 0
                    used_usdt = float(ps_row[1]) if ps_row[1] else 0
                    import safety_manager
                    eq = safety_manager.get_equity_limits(cur_ps)
                    op_cap = eq.get('operating_cap', 1)
                    pos_capital_pct = round(used_usdt / op_cap * 100, 0) if op_cap > 0 else 0
            conn_ps.close()
        except Exception:
            pass

        lines = [
            f"ğŸ“Š ìŠ¤ì½”ì–´ ì—”ì§„ (4ì¶•)",
            f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
            f"ì´ì : {total:+.1f} â†’ {dominant}",
            f"",
            f"ê¸°ìˆ (TECH):   {tech:+.0f} Ã— {tech_w} = {tech_c:+.1f}",
            f"í¬ì§€ì…˜(POS):  {pos:+.0f} Ã— {pos_w} = {pos_c:+.1f}",
            f"ë ˆì§(REG):    {regime:+.0f} Ã— {regime_w} = {regime_c:+.1f}",
            f"ë‰´ìŠ¤(NEWS):   {ne:+.0f} Ã— {news_w} = {news_c:+.1f}{' [ì°¨ë‹¨ë¨]' if guarded else ''}",
            f"",
            f"ê¶Œê³ ê°•ë„: {signal_stage_label} (score={abs_score:.0f}, stg1>=10, stg2>=45, stg3>=65)",
            f"ë¶„í• ë‹¨ê³„: stage {pos_stage}/7 (capital used: {pos_capital_pct:.0f}%)",
            f"",
            f"ì—”ì§„ê¶Œê³ : {dominant} {signal_stage_label} (ì´ì  {total:+.1f})",
        ]
        # í˜„ì¬ í¬ì§€ì…˜ ì •ë³´
        try:
            pos_info = _position_info()
            lines.append(f"í˜„ì¬í¬ì§€ì…˜: {pos_info.replace('ğŸ“ ', '')}")
        except Exception:
            pass
        if guarded:
            lines.append(f"  âš  {dominant} ê¶Œê³ ì´ë‚˜, TECH/POS ì¤‘ë¦½ìœ¼ë¡œ ë‰´ìŠ¤ ë‹¨ë… ì°¨ë‹¨")
        lines.append(f"")
        lines.append(f"ë‰´ìŠ¤ ì´ë²¤íŠ¸ ë‚´ì—­:")
        lines.append(f"  ì†ŒìŠ¤í’ˆì§ˆ: {ne_comp.get('source_quality', 0):.1f}/20")
        lines.append(f"  ì¹´í…Œê³ ë¦¬: {ne_comp.get('category_weight', 0):.1f}/25")
        lines.append(f"  ìµœì‹ ì„±: {ne_comp.get('recency', 0):.1f}/15")
        lines.append(f"  ì‹œì¥ë°˜ì‘: {ne_comp.get('market_reaction', 0):.1f}/25")
        lines.append(f"  í‚¤ì›Œë“œ: {ne_comp.get('watchlist', 0):.1f}/15")
        score_trace = ne_details.get('score_trace', '')
        if score_trace:
            lines.append(f"  ì¶”ì : {score_trace}")
        lines.append(f"")
        lines.append(f"ì†ì ˆ: {r.get('dynamic_stop_loss_pct', 2.0)}%")
        lines.append(f"BTC: {r.get('price', '?')}")
        return '\n'.join(lines)
    except Exception as e:
        return f'ìŠ¤ì½”ì–´ ì¡°íšŒ ì‹¤íŒ¨: {e}'


def _db_health(_text=None):
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            tables = [
                ('candles', 'ts'),
                ('news', 'ts'),
                ('indicators', 'ts'),
                ('events', 'start_ts'),
                ('pm_decision_log', 'ts'),
                ('execution_log', 'ts'),
                ('score_history', 'ts'),
                ('macro_data', 'ts'),
            ]
            lines = ['ğŸ—„ DB ìƒíƒœ ì ê²€']
            lines.append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')
            for tbl, ts_col in tables:
                try:
                    cur.execute(f"""
                        SELECT count(*),
                               min({ts_col})::text,
                               max({ts_col})::text,
                               count(*) FILTER (WHERE {ts_col} >= now() - interval '24 hours')
                        FROM {tbl};
                    """)
                    row = cur.fetchone()
                    total, min_ts, max_ts, recent = row
                    min_ts = (min_ts or '')[:16]
                    max_ts = (max_ts or '')[:16]
                    lines.append(f'  {tbl}: {total:,}ê±´ (24h: {recent:,}ê±´)')
                    lines.append(f'    ë²”ìœ„: {min_ts} ~ {max_ts}')
                except Exception as e:
                    lines.append(f'  {tbl}: ì¡°íšŒ ì‹¤íŒ¨ ({e})')

            # news_impact_stats
            lines.append('')
            lines.append('[ë‰´ìŠ¤ ì˜í–¥ í†µê³„]')
            try:
                cur.execute("""
                    SELECT stats_version, count(*), sum(sample_count)
                    FROM news_impact_stats
                    GROUP BY stats_version
                    ORDER BY stats_version DESC LIMIT 1;
                """)
                row = cur.fetchone()
                if row:
                    lines.append(f'  ë²„ì „: {row[0]} | ì¹´í…Œê³ ë¦¬: {row[1]}ê°œ | ìƒ˜í”Œ: {row[2]:,}ê±´')
                else:
                    lines.append('  ë°ì´í„° ì—†ìŒ (compute_news_impact_stats.py ì‹¤í–‰ í•„ìš”)')
            except Exception:
                lines.append('  í…Œì´ë¸” ë¯¸ìƒì„±')

            # regime_correlation
            lines.append('')
            lines.append('[BTC-QQQ ìƒê´€ê´€ê³„]')
            try:
                import regime_correlation
                info = regime_correlation.get_correlation_info(cur)
                regime = info.get('regime', '?')
                corr = info.get('correlation')
                age = info.get('cache_age_sec')
                corr_str = f'{corr:.4f}' if corr is not None else 'N/A'
                age_str = f'{age}ì´ˆ ì „' if age is not None else 'N/A'
                lines.append(f'  ë ˆì§: {regime} | ìƒê´€ê³„ìˆ˜: {corr_str} | ìºì‹œ: {age_str}')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨ ({e})')

        return '\n'.join(lines)
    except Exception as e:
        return f'DB ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _claude_audit(_text=None):
    """Claude API ì‚¬ìš©ëŸ‰ ê°ì‚¬ ë¦¬í¬íŠ¸."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            lines = ['ğŸ§  Claude ì‚¬ìš©ëŸ‰ ê°ì‚¬']
            lines.append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')

            # Today's stats from claude_call_log
            cur.execute("""
                SELECT count(*),
                       coalesce(sum(estimated_cost), 0),
                       coalesce(sum(input_tokens), 0),
                       coalesce(sum(output_tokens), 0)
                FROM claude_call_log
                WHERE ts >= date_trunc('day', now() AT TIME ZONE 'Asia/Seoul');
            """)
            row = cur.fetchone()
            today_calls = row[0] or 0
            today_cost = float(row[1] or 0)
            today_input = row[2] or 0
            today_output = row[3] or 0
            lines.append(f'\n[ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰]')
            lines.append(f'  í˜¸ì¶œ: {today_calls}ê±´ | ë¹„ìš©: ${today_cost:.4f}')
            lines.append(f'  ì…ë ¥ í† í°: {today_input:,} | ì¶œë ¥ í† í°: {today_output:,}')

            # By gate_type today
            cur.execute("""
                SELECT gate_type, count(*), coalesce(sum(estimated_cost), 0)
                FROM claude_call_log
                WHERE ts >= date_trunc('day', now() AT TIME ZONE 'Asia/Seoul')
                GROUP BY gate_type ORDER BY count(*) DESC;
            """)
            gate_rows = cur.fetchall()
            if gate_rows:
                lines.append(f'\n[ê²Œì´íŠ¸ë³„ ë¶„ë¥˜]')
                for gr in gate_rows:
                    lines.append(f'  {gr[0] or "?"}: {gr[1]}ê±´ (${float(gr[2]):.4f})')

            # Monthly stats
            cur.execute("""
                SELECT count(*),
                       coalesce(sum(estimated_cost), 0)
                FROM claude_call_log
                WHERE ts >= date_trunc('month', now());
            """)
            row = cur.fetchone()
            month_calls = row[0] or 0
            month_cost = float(row[1] or 0)
            lines.append(f'\n[ì´ë²ˆ ë‹¬ ëˆ„ì ]')
            lines.append(f'  í˜¸ì¶œ: {month_calls}ê±´ | ë¹„ìš©: ${month_cost:.4f}')

            # Budget remaining (from claude_gate)
            try:
                import claude_gate
                lines.append(f'\n[ì˜ˆì‚° í•œë„]')
                lines.append(f'  ì¼ì¼ í˜¸ì¶œ í•œë„: {claude_gate.DAILY_CALL_LIMIT}')
                lines.append(f'  ì¼ì¼ ë¹„ìš© í•œë„: ${claude_gate.DAILY_COST_LIMIT}')
                lines.append(f'  ì›”ê°„ ë¹„ìš© í•œë„: ${claude_gate.MONTHLY_COST_LIMIT}')
                remaining_calls = max(0, claude_gate.DAILY_CALL_LIMIT - today_calls)
                remaining_cost = max(0, claude_gate.DAILY_COST_LIMIT - today_cost)
                lines.append(f'  ë‚¨ì€ í˜¸ì¶œ: {remaining_calls}ê±´ | ë‚¨ì€ ë¹„ìš©: ${remaining_cost:.2f}')
            except Exception:
                pass

            # Recent 5 calls
            cur.execute("""
                SELECT to_char(ts AT TIME ZONE 'Asia/Seoul', 'HH24:MI'),
                       gate_type, call_type,
                       action_result, estimated_cost
                FROM claude_call_log
                ORDER BY ts DESC LIMIT 5;
            """)
            recent = cur.fetchall()
            if recent:
                lines.append(f'\n[ìµœê·¼ í˜¸ì¶œ 5ê±´]')
                for r in recent:
                    cost_str = f'${float(r[4]):.4f}' if r[4] else '$0'
                    lines.append(f'  {r[0]} {r[1] or "?"}/{r[2] or "?"} '
                                 f'â†’ {r[3] or "?"} ({cost_str})')

        return '\n'.join(lines)
    except Exception as e:
        return f'Claude ê°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _audit_report(_text=None):
    """ì¢…í•© ê°ì‚¬ ë¦¬í¬íŠ¸: ì°¨íŠ¸íë¦„ + ì´ë²¤íŠ¸ + ê²°ì • + ì‹¤í–‰ + ë‰´ìŠ¤ê¸°ì—¬ + ê·œì¹™ìœ„ë°˜."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ“‹ ì¢…í•© ê°ì‚¬ ë¦¬í¬íŠ¸']
        lines.append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')
        lines.append('âš  ì¦‰ì‹œ ì ìš© ê¸ˆì§€ â€” ë¶„ì„ ìë£Œ')
        lines.append('')

        with conn.cursor() as cur:
            # 1. ì°¨íŠ¸ íë¦„
            lines.append('[1. ì°¨íŠ¸ íë¦„]')
            try:
                from news_strategy_report import _fetch_chart_flow
                chart = _fetch_chart_flow(cur)
                lines.append(f'  4h ì¶”ì„¸: {chart.get("trend_4h", "?")}({chart.get("trend_4h_pct", 0):+.1f}%)')
                lines.append(f'  12h ì¶”ì„¸: {chart.get("trend_12h", "?")}({chart.get("trend_12h_pct", 0):+.1f}%)')
                lines.append(f'  BB: {chart.get("bb_position", "?")} | Ichimoku: {chart.get("ichimoku_cloud", "?")}')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # 2. ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸ (ìµœê·¼ 24h)
            lines.append('')
            lines.append('[2. ì´ë²¤íŠ¸ íƒ€ì„ë¼ì¸ (24h)]')
            try:
                cur.execute("""
                    SELECT to_char(ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI'),
                           mode, triggers, snapshot_price
                    FROM event_trigger_log
                    WHERE ts >= now() - interval '24 hours'
                    ORDER BY ts DESC LIMIT 10;
                """)
                evt_rows = cur.fetchall()
                if evt_rows:
                    for r in evt_rows:
                        triggers = r[2] or ''
                        if isinstance(triggers, (list, dict)):
                            import json
                            triggers = json.dumps(triggers, ensure_ascii=False)[:80]
                        lines.append(f'  {r[0]} [{r[1]}] {str(triggers)[:60]} price={r[3] or "?"}')
                else:
                    lines.append('  ì´ë²¤íŠ¸ ì—†ìŒ')
            except Exception:
                lines.append('  ì¡°íšŒ ì‹¤íŒ¨')

            # 3. ê²°ì • ë¡œê·¸ (ìµœê·¼ 24h)
            lines.append('')
            lines.append('[3. ê²°ì • ë¡œê·¸ (24h)]')
            try:
                cur.execute("""
                    SELECT to_char(ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI'),
                           final_action, position_side, action_reason
                    FROM pm_decision_log
                    WHERE ts >= now() - interval '24 hours'
                    ORDER BY ts DESC LIMIT 10;
                """)
                dec_rows = cur.fetchall()
                if dec_rows:
                    for r in dec_rows:
                        lines.append(f'  {r[0]} {r[1] or "?"} {r[2] or ""} â€” {(r[3] or "")[:60]}')
                else:
                    lines.append('  ê²°ì • ì—†ìŒ')
            except Exception:
                lines.append('  ì¡°íšŒ ì‹¤íŒ¨')

            # 4. ì‹¤í–‰ ë¡œê·¸ (ìµœê·¼ 24h)
            lines.append('')
            lines.append('[4. ì‹¤í–‰ ë¡œê·¸ (24h)]')
            try:
                cur.execute("""
                    SELECT to_char(ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI'),
                           order_type, direction, status,
                           filled_qty, avg_fill_price, realized_pnl
                    FROM execution_log
                    WHERE ts >= now() - interval '24 hours'
                    ORDER BY ts DESC LIMIT 10;
                """)
                exec_rows = cur.fetchall()
                if exec_rows:
                    for r in exec_rows:
                        pnl_str = f' PnL={float(r[6]):+.4f}' if r[6] else ''
                        lines.append(f'  {r[0]} {r[1]} {r[2] or ""} [{r[3]}]{pnl_str}')
                else:
                    lines.append('  ì‹¤í–‰ ì—†ìŒ')
            except Exception:
                lines.append('  ì¡°íšŒ ì‹¤íŒ¨')

            # 5. ë‰´ìŠ¤ ê¸°ì—¬ (ë‹¹ì¼ ì „ëµ ë°˜ì˜ëœ ë‰´ìŠ¤)
            lines.append('')
            lines.append('[5. ì „ëµ ë°˜ì˜ ë‰´ìŠ¤ (ë‹¹ì¼)]')
            try:
                cur.execute("""
                    SELECT title_ko, tier, impact_score, relevance_score, source
                    FROM news
                    WHERE ts >= date_trunc('day', now() AT TIME ZONE 'Asia/Seoul')
                      AND exclusion_reason IS NULL
                      AND COALESCE(tier, 'UNKNOWN') NOT IN ('TIERX')
                      AND impact_score >= 3
                    ORDER BY impact_score DESC
                    LIMIT 5;
                """)
                news_rows = cur.fetchall()
                if news_rows:
                    for i, r in enumerate(news_rows, 1):
                        title = (r[0] or '?')[:50]
                        lines.append(f'  {i}) [{r[1]}] ({r[2]}/10) {title} rel={r[3] or "?"}')
                else:
                    lines.append('  ë°˜ì˜ ë‰´ìŠ¤ ì—†ìŒ')
            except Exception:
                lines.append('  ì¡°íšŒ ì‹¤íŒ¨')

            # 6. ê·œì¹™ ìœ„ë°˜ / ì•ˆì „ ì°¨ë‹¨ ì´ë ¥
            lines.append('')
            lines.append('[6. ì•ˆì „ ì°¨ë‹¨ ì´ë ¥ (24h)]')
            try:
                # error_logê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í…Œì´ë¸” ì¡´ì¬ í™•ì¸
                cur.execute("""
                    SELECT EXISTS (
                        SELECT 1 FROM information_schema.tables
                        WHERE table_name = 'error_log' AND table_schema = 'public'
                    );
                """)
                has_error_log = cur.fetchone()[0]
                if has_error_log:
                    cur.execute("""
                        SELECT to_char(ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI'),
                               service, level, message
                        FROM error_log
                        WHERE ts >= now() - interval '24 hours'
                          AND (level IN ('CRITICAL', 'WARNING')
                               OR message ILIKE '%%block%%'
                               OR message ILIKE '%%ì°¨ë‹¨%%')
                        ORDER BY ts DESC LIMIT 5;
                    """)
                    err_rows = cur.fetchall()
                    if err_rows:
                        for r in err_rows:
                            lines.append(f'  {r[0]} [{r[2]}] {r[1]}: {(r[3] or "")[:80]}')
                    else:
                        lines.append('  ì°¨ë‹¨/ê²½ê³  ì´ë ¥ ì—†ìŒ')
                else:
                    # error_log í…Œì´ë¸” ë¯¸ìƒì„± â†’ safety_manager ì°¨ë‹¨ ì´ë ¥ìœ¼ë¡œ ëŒ€ì²´
                    cur.execute("""
                        SELECT to_char(ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI'),
                               final_action, action_reason
                        FROM pm_decision_log
                        WHERE ts >= now() - interval '24 hours'
                          AND final_action IN ('HOLD', 'ABORT', 'SKIP')
                        ORDER BY ts DESC LIMIT 5;
                    """)
                    hold_rows = cur.fetchall()
                    if hold_rows:
                        for r in hold_rows:
                            lines.append(f'  {r[0]} {r[1]} â€” {(r[2] or "")[:60]}')
                    else:
                        lines.append('  ì°¨ë‹¨/HOLD ì´ë ¥ ì—†ìŒ')
            except Exception:
                lines.append('  ì¡°íšŒ ì‹¤íŒ¨')

            # 7. 17:30 ì²­ì‚° ìƒíƒœ
            lines.append('')
            lines.append('[7. 17:30 ì²­ì‚° ìƒíƒœ]')
            try:
                cur.execute("""
                    SELECT to_char(ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI'),
                           status, close_reason
                    FROM execution_log
                    WHERE order_type = 'SCHEDULED_CLOSE'
                      AND ts >= now() - interval '48 hours'
                    ORDER BY ts DESC LIMIT 3;
                """)
                sched_rows = cur.fetchall()
                if sched_rows:
                    for r in sched_rows:
                        lines.append(f'  {r[0]} [{r[1]}] {r[2] or ""}')
                else:
                    lines.append('  ìµœê·¼ 48ì‹œê°„ ì˜ˆì•½ ì²­ì‚° ê¸°ë¡ ì—†ìŒ')
            except Exception:
                lines.append('  ì¡°íšŒ ì‹¤íŒ¨')

        lines.append('')
        lines.append('âš  ì¦‰ì‹œ ì ìš© ê¸ˆì§€ â€” ë¶„ì„ ìë£Œ')
        return '\n'.join(lines)
    except Exception as e:
        return f'ê°ì‚¬ ë¦¬í¬íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _news_applied(_text=None):
    """ì „ëµ ë°˜ì˜ ë‰´ìŠ¤ TOP5: tier/topic_class/relevance_score/source_tier/30m/2h ë°˜ì‘."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT n.id, n.title_ko, n.tier, n.topic_class,
                       n.relevance_score, n.source_tier, n.impact_score, n.source,
                       mt.btc_ret_30m, mt.btc_ret_2h,
                       to_char(n.ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI') as ts_kr
                FROM news n
                LEFT JOIN macro_trace mt ON mt.news_id = n.id
                WHERE n.ts >= now() - interval '6 hours'
                  AND n.exclusion_reason IS NULL
                  AND COALESCE(n.tier, 'UNKNOWN') NOT IN ('TIERX')
                  AND n.impact_score >= 3
                ORDER BY n.impact_score DESC, n.ts DESC
                LIMIT 5;
            """)
            rows = cur.fetchall()

        if not rows:
            return 'ğŸ“° ì „ëµ ë°˜ì˜ ë‰´ìŠ¤ TOP5\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nìµœê·¼ 6ì‹œê°„ ë°˜ì˜ ë‰´ìŠ¤ ì—†ìŒ'

        lines = ['ğŸ“° ì „ëµ ë°˜ì˜ ë‰´ìŠ¤ TOP5 (ìµœê·¼ 6ì‹œê°„)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        for i, r in enumerate(rows, 1):
            nid, title, tier, topic, rel, src_tier, impact, source, ret30, ret2h, ts = r
            title = (title or '?')[:60]
            tier = tier or 'UNKNOWN'
            topic = topic or '-'
            rel = f'{rel:.1f}' if rel is not None else '-'
            src_tier = src_tier or '-'
            ret30_str = f'{ret30:+.2f}%' if ret30 is not None else '-'
            ret2h_str = f'{ret2h:+.2f}%' if ret2h is not None else '-'
            lines.append(f'\n{i}) [{tier}] {title}')
            lines.append(f'   topic={topic} | rel={rel} | src={src_tier} | impact={impact}/10')
            lines.append(f'   ë°˜ì‘: 30m={ret30_str} 2h={ret2h_str} | {ts} ({source})')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ì „ëµ ë°˜ì˜ ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _news_ignored(_text=None):
    """ë¬´ì‹œëœ ë‰´ìŠ¤ 10ê°œ + ë¬´ì‹œ ì‚¬ìœ ."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT title_ko, tier, impact_score, exclusion_reason, source,
                       to_char(ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI') as ts_kr
                FROM news
                WHERE ts >= now() - interval '6 hours'
                  AND (exclusion_reason IS NOT NULL
                       OR tier = 'TIERX'
                       OR COALESCE(impact_score, 0) < 3)
                ORDER BY ts DESC
                LIMIT 10;
            """)
            rows = cur.fetchall()

        if not rows:
            return 'ğŸš« ë¬´ì‹œëœ ë‰´ìŠ¤ (ìµœê·¼ 6ì‹œê°„)\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\në¬´ì‹œëœ ë‰´ìŠ¤ ì—†ìŒ'

        lines = ['ğŸš« ë¬´ì‹œëœ ë‰´ìŠ¤ 10ê°œ (ìµœê·¼ 6ì‹œê°„)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        for i, r in enumerate(rows, 1):
            title, tier, impact, reason, source, ts = r
            title = (title or '?')[:55]
            tier = tier or 'UNKNOWN'
            impact = impact if impact is not None else 0
            # Determine reason
            if reason:
                reason_str = reason[:40]
            elif tier == 'TIERX':
                reason_str = 'ê´€ë ¨ë„ ìµœí•˜(TIERX)'
            elif impact < 3:
                reason_str = f'ë‚®ì€ ì˜í–¥ë„({impact}/10)'
            else:
                reason_str = 'ë¯¸ë¶„ë¥˜'
            lines.append(f'{i}) [{tier}] {title}')
            lines.append(f'   ì‚¬ìœ : {reason_str} | {ts} ({source})')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë¬´ì‹œëœ ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _db_coverage(_text=None):
    """DB ì»¤ë²„ë¦¬ì§€: 2023-11ë¶€í„° ì›”ë³„ candles/events/news ê±´ìˆ˜ + news tier=UNKNOWN ë¹„ìœ¨."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ“Š DB ì»¤ë²„ë¦¬ì§€ (2023-11~í˜„ì¬)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        with conn.cursor() as cur:
            tables_config = [
                ('candles', 'ts'),
                ('events', 'start_ts'),
                ('news', 'ts'),
            ]
            for tbl, ts_col in tables_config:
                try:
                    cur.execute(f"""
                        SELECT to_char(date_trunc('month', {ts_col}), 'YYYY-MM') AS month,
                               count(*) AS cnt
                        FROM {tbl}
                        WHERE {ts_col} >= '2023-11-01'
                        GROUP BY month
                        ORDER BY month;
                    """)
                    rows = cur.fetchall()
                    lines.append(f'\n[{tbl}]')
                    if not rows:
                        lines.append('  ë°ì´í„° ì—†ìŒ')
                    else:
                        for month, cnt in rows:
                            lines.append(f'  {month}: {cnt:,}ê±´')
                except Exception as e:
                    lines.append(f'\n[{tbl}] ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # news tier=UNKNOWN ë¹„ìœ¨
            lines.append('\n[ë‰´ìŠ¤ tier ë¶„í¬]')
            try:
                cur.execute("""
                    SELECT COALESCE(tier, 'NULL') AS t,
                           count(*) AS cnt
                    FROM news
                    WHERE ts >= '2023-11-01'
                    GROUP BY t
                    ORDER BY cnt DESC;
                """)
                tier_rows = cur.fetchall()
                total = sum(r[1] for r in tier_rows) if tier_rows else 0
                for t, cnt in tier_rows:
                    pct = cnt / total * 100 if total > 0 else 0
                    lines.append(f'  {t}: {cnt:,}ê±´ ({pct:.1f}%)')
                unknown_cnt = sum(r[1] for r in tier_rows if r[0] in ('UNKNOWN', 'NULL'))
                if total > 0:
                    lines.append(f'  â†’ UNKNOWN+NULL ë¹„ìœ¨: {unknown_cnt/total*100:.1f}%')
            except Exception as e:
                lines.append(f'  tier ë¶„í¬ ì¡°íšŒ ì‹¤íŒ¨: {e}')

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  DB ì»¤ë²„ë¦¬ì§€ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _evidence(_text=None):
    """ë³´ì¡°ì§€í‘œ ê·¼ê±°: price_events ìš”ì•½ + tier1/2 impact í•© + ìœ ì‚¬ ì´ë²¤íŠ¸ Top3."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ“ˆ ë³´ì¡°ì§€í‘œ ê·¼ê±° ì„¹ì…˜', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        with conn.cursor() as cur:
            # 1. ìµœê·¼ price_events (ê¸‰ë“±/ê¸‰ë½, ATR ê¸‰ì¦, 15m ë°©í–¥)
            lines.append('\n[ìµœê·¼ price_events (24h)]')
            try:
                cur.execute("""
                    SELECT trigger_type, direction, move_pct, atr_z,
                           btc_price_at,
                           to_char(start_ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI') as ts_kr
                    FROM price_events
                    WHERE start_ts >= now() - interval '24 hours'
                    ORDER BY start_ts DESC
                    LIMIT 8;
                """)
                pe_rows = cur.fetchall()
                if pe_rows:
                    for r in pe_rows:
                        trigger, dirn, move, atr_z, price, ts = r
                        move_str = f'{move:+.2f}%' if move is not None else '-'
                        atr_str = f'ATR_z={atr_z:.1f}' if atr_z is not None else ''
                        lines.append(f'  [{ts}] {trigger} {dirn} {move_str} {atr_str} @${price:,.0f}' if price else f'  [{ts}] {trigger} {dirn} {move_str} {atr_str}')
                else:
                    lines.append('  ìµœê·¼ 24ì‹œê°„ price_events ì—†ìŒ')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # 2. tier1/2 ë‰´ìŠ¤ impact í•© (ì‹¤ì œ 30m/2h/24h ë°˜ì‘ ê¸°ë°˜)
            lines.append('\n[tier1/tier2 ë‰´ìŠ¤ ë°˜ì‘ í•©ì‚° (24h)]')
            try:
                cur.execute("""
                    SELECT n.tier,
                           count(*) AS cnt,
                           avg(mt.btc_ret_30m) AS avg_30m,
                           avg(mt.btc_ret_2h) AS avg_2h,
                           avg(mt.btc_ret_24h) AS avg_24h
                    FROM news n
                    JOIN macro_trace mt ON mt.news_id = n.id
                    WHERE n.ts >= now() - interval '24 hours'
                      AND n.tier IN ('TIER1', 'TIER2')
                    GROUP BY n.tier
                    ORDER BY n.tier;
                """)
                tier_rows = cur.fetchall()
                if tier_rows:
                    for t, cnt, a30, a2h, a24h in tier_rows:
                        a30s = f'{a30:+.3f}%' if a30 is not None else '-'
                        a2hs = f'{a2h:+.3f}%' if a2h is not None else '-'
                        a24hs = f'{a24h:+.3f}%' if a24h is not None else '-'
                        lines.append(f'  {t}: {cnt}ê±´ | í‰ê· 30m={a30s} 2h={a2hs} 24h={a24hs}')
                else:
                    lines.append('  TIER1/2 ë‰´ìŠ¤ ë°˜ì‘ ë°ì´í„° ì—†ìŒ')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # 3. ìœ ì‚¬ ì´ë²¤íŠ¸ Top3 + ê³¼ê±° í‰ê·  ë°˜ì‘ (news_impact_stats)
            lines.append('\n[ìœ ì‚¬ ì´ë²¤íŠ¸ Top3 (ê³¼ê±° í‰ê·  ë°˜ì‘)]')
            try:
                cur.execute("""
                    SELECT event_type, regime,
                           avg_ret_30m, avg_ret_2h, avg_abs_ret_2h,
                           sample_count, direction_accuracy
                    FROM news_impact_stats
                    WHERE sample_count >= 3
                    ORDER BY avg_abs_ret_2h DESC
                    LIMIT 3;
                """)
                stat_rows = cur.fetchall()
                if stat_rows:
                    for r in stat_rows:
                        etype, regime, a30, a2h, abs2h, cnt, acc = r
                        a30s = f'{a30:+.3f}%' if a30 is not None else '-'
                        a2hs = f'{a2h:+.3f}%' if a2h is not None else '-'
                        acc_s = f'{acc:.0f}%' if acc is not None else '-'
                        lines.append(f'  {etype} ({regime}): N={cnt}')
                        lines.append(f'    í‰ê· 30m={a30s} 2h={a2hs} |abs2h|={abs2h:.3f}% ë°©í–¥ì ì¤‘={acc_s}')
                else:
                    lines.append('  í†µê³„ ë°ì´í„° ì—†ìŒ (compute_news_impact_stats.py ì‹¤í–‰ í•„ìš”)')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨: {e}')

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë³´ì¡°ì§€í‘œ ê·¼ê±° ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _test_report_full(_text=None):
    """í…ŒìŠ¤íŠ¸ ì¢…í•© ë³´ê³ : ì´ë²¤íŠ¸/ì²´ê²°/ì˜¤íŒ/ê°œì„ ì  â€” ì ìš© ê¸ˆì§€."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ§ª í…ŒìŠ¤íŠ¸ ì¢…í•© ë³´ê³  (ì ìš© ê¸ˆì§€)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        with conn.cursor() as cur:
            # 1. ì˜¤ëŠ˜ ì´ë²¤íŠ¸
            lines.append('\n[1. ì˜¤ëŠ˜ ë°œìƒ ì´ë²¤íŠ¸]')
            try:
                cur.execute("""
                    SELECT to_char(start_ts AT TIME ZONE 'Asia/Seoul', 'HH24:MI'),
                           kind, direction, confidence, btc_price_at
                    FROM events
                    WHERE start_ts >= date_trunc('day', now() AT TIME ZONE 'Asia/Seoul')
                    ORDER BY start_ts DESC
                    LIMIT 10;
                """)
                evt_rows = cur.fetchall()
                if evt_rows:
                    for ts, kind, dirn, conf, price in evt_rows:
                        conf_s = f'{conf:.0f}%' if conf is not None else '-'
                        price_s = f'${price:,.0f}' if price else '-'
                        lines.append(f'  {ts} {kind} {dirn or ""} conf={conf_s} {price_s}')
                else:
                    lines.append('  ì˜¤ëŠ˜ ì´ë²¤íŠ¸ ì—†ìŒ')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # 2. ì˜¤ëŠ˜ ì²´ê²°
            lines.append('\n[2. ì˜¤ëŠ˜ ì²´ê²°]')
            try:
                cur.execute("""
                    SELECT to_char(ts AT TIME ZONE 'Asia/Seoul', 'HH24:MI'),
                           order_type, direction, status,
                           filled_qty, avg_fill_price, realized_pnl
                    FROM execution_log
                    WHERE ts >= date_trunc('day', now() AT TIME ZONE 'Asia/Seoul')
                    ORDER BY ts DESC
                    LIMIT 10;
                """)
                exec_rows = cur.fetchall()
                if exec_rows:
                    for ts, otype, dirn, status, qty, price, pnl in exec_rows:
                        pnl_s = f' PnL={float(pnl):+.4f}' if pnl else ''
                        lines.append(f'  {ts} {otype} {dirn or ""} [{status}]{pnl_s}')
                else:
                    lines.append('  ì˜¤ëŠ˜ ì²´ê²° ì—†ìŒ')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # 3. ì˜¤íŒ ë¶„ì„ (ê¶Œê³  vs ì‹¤ì œ í¬ì§€ì…˜ ë¶ˆì¼ì¹˜)
            lines.append('\n[3. ì˜¤íŒ ë¶„ì„ (ê¶Œê³  vs ì‹¤ì œ)]')
            try:
                cur.execute("""
                    SELECT to_char(d.ts AT TIME ZONE 'Asia/Seoul', 'HH24:MI'),
                           d.final_action, d.position_side, d.action_reason
                    FROM pm_decision_log d
                    WHERE d.ts >= date_trunc('day', now() AT TIME ZONE 'Asia/Seoul')
                    ORDER BY d.ts DESC
                    LIMIT 15;
                """)
                dec_rows = cur.fetchall()
                # Get current position
                cur.execute("""
                    SELECT side FROM position_state
                    WHERE symbol = 'BTC/USDT:USDT';
                """)
                pos_row = cur.fetchone()
                current_side = (pos_row[0] or '').upper() if pos_row else 'NONE'
                mismatches = []
                for ts, action, pos_side, reason in dec_rows:
                    pos_side_up = (pos_side or '').upper()
                    # Detect mismatch: engine says SHORT but position is LONG, etc.
                    if action in ('CLOSE', 'REVERSE') and pos_side_up and current_side != 'NONE':
                        if action == 'REVERSE' or (action == 'CLOSE' and pos_side_up == current_side):
                            mismatches.append(
                                f'  âš  {ts} ê¶Œê³ ={action} (ë‹¹ì‹œ={pos_side_up}) í˜„ì¬={current_side}\n'
                                f'    ì‚¬ìœ : {(reason or "")[:60]}')
                if mismatches:
                    lines.extend(mismatches[:5])
                else:
                    lines.append('  ê¶Œê³ -ì‹¤í–‰ ë¶ˆì¼ì¹˜ ì—†ìŒ')
                lines.append(f'  í˜„ì¬ í¬ì§€ì…˜: {current_side}')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # 4. ê°œì„ ì /ë¦¬ìŠ¤í¬ ìš”ì•½
            lines.append('\n[4. ê°œì„ ì /ë¦¬ìŠ¤í¬]')
            try:
                # Recent safety blocks
                cur.execute("""
                    SELECT count(*) FROM pm_decision_log
                    WHERE ts >= date_trunc('day', now() AT TIME ZONE 'Asia/Seoul')
                      AND final_action IN ('HOLD', 'ABORT', 'SKIP');
                """)
                hold_cnt = cur.fetchone()[0] or 0
                cur.execute("""
                    SELECT count(*) FROM pm_decision_log
                    WHERE ts >= date_trunc('day', now() AT TIME ZONE 'Asia/Seoul');
                """)
                total_dec = cur.fetchone()[0] or 0
                if total_dec > 0:
                    hold_pct = hold_cnt / total_dec * 100
                    lines.append(f'  ê²°ì • {total_dec}ê±´ ì¤‘ HOLD/SKIP {hold_cnt}ê±´ ({hold_pct:.0f}%)')
                else:
                    lines.append('  ì˜¤ëŠ˜ ê²°ì • ì—†ìŒ')
                # Claude denied count
                cur.execute("""
                    SELECT count(*) FROM claude_call_log
                    WHERE ts >= date_trunc('day', now() AT TIME ZONE 'Asia/Seoul')
                      AND NOT allowed;
                """)
                denied = cur.fetchone()[0] or 0
                if denied:
                    lines.append(f'  Claude ê±°ë¶€: {denied}ê±´ (ì˜ˆì‚°/ì¿¨ë‹¤ìš´)')
            except Exception:
                pass

        lines.append('\nâš  ì ìš© ê¸ˆì§€ â€” ë¶„ì„ ìë£Œ')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  í…ŒìŠ¤íŠ¸ ë³´ê³  ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


# â”€â”€ /debug handler functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MAX_N_CAP = 200  # absolute max for --n parameter


_META_PARAM_KEYS = {'nonce', 'force_refresh', 'trace_id', 'cache', 'debug'}


def _parse_debug_args(text):
    """Unified debug argument parser.
    Returns dict with: n, mode, allow_only, window, from_month, extra_kv,
                       meta_args (known system params), ignored (truly unknown).
    """
    args = {
        'n': 20,
        'mode': 'latest',
        'allow_only': False,
        'window': '24h',
        'from_month': None,
        'extra_kv': {},
        'meta_args': {},
        'ignored': [],
        'n_capped': False,
    }
    if not text:
        return args

    # --n=<int>
    m = re.search(r'--n=(\d+)', text)
    if m:
        raw_n = int(m.group(1))
        if raw_n > MAX_N_CAP:
            args['n'] = MAX_N_CAP
            args['n_capped'] = True
        else:
            args['n'] = raw_n

    # allow_only=true/false
    m = re.search(r'allow_only\s*=\s*(true|false|1|0)', text, re.IGNORECASE)
    if m:
        args['allow_only'] = m.group(1).lower() in ('true', '1')

    # mode=<value>
    m = re.search(r'mode\s*=\s*(\S+)', text, re.IGNORECASE)
    if m:
        args['mode'] = m.group(1).lower()

    # window=<value>
    m = re.search(r'window\s*=\s*(\S+)', text, re.IGNORECASE)
    if m:
        args['window'] = m.group(1).lower()

    # --from=YYYY-MM
    m = re.search(r'--from=(\d{4}-\d{2})', text)
    if m:
        args['from_month'] = m.group(1)

    # Classify extra params: meta (system) vs ignored (truly unknown)
    known_keys = {'n', 'allow_only', 'mode', 'window', 'from'}
    for pm in re.finditer(r'(?:--|)(\w+)\s*=\s*(\S+)', text):
        pk = pm.group(1).lower()
        pv = pm.group(2)
        if pk in known_keys:
            continue
        if pk in _META_PARAM_KEYS:
            args['meta_args'][pk] = pv
        else:
            args['ignored'].append(pm.group(0))

    return args


# â”€â”€ Service registry (Item 2) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# expected_interval_sec: how often the service should heartbeat
# systemctl_unit: the actual systemd unit name
PRICE_TABLE_CANONICAL = {
    '1m': {'table': 'candles', 'ts_col': 'ts', 'source': 'bybit kline (live collector)'},
    '5m': {'table': 'market_ohlcv', 'ts_col': 'ts', 'source': 'aggregated from candles_1m'},
}

SERVICE_REGISTRY = {
    'candles':        {'expected_interval_sec': 60,  'systemctl_unit': 'candles.service'},
    'executor':       {'expected_interval_sec': 60,  'systemctl_unit': 'dry_run_close_executor.service'},
    'indicators':     {'expected_interval_sec': 60,  'systemctl_unit': 'indicators.service'},
    'news_bot':       {'expected_interval_sec': 300, 'systemctl_unit': 'news_bot.service'},
    'pnl_watcher':    {'expected_interval_sec': 60,  'systemctl_unit': 'pnl_watcher.service'},
    'signal_logger':  {'expected_interval_sec': 120, 'systemctl_unit': 'signal_logger.service'},
    'vol_profile':    {'expected_interval_sec': 300, 'systemctl_unit': 'vol_profile.service'},
    'error_watcher':  {'expected_interval_sec': 300, 'systemctl_unit': 'error_watcher.service'},
}


def _debug_version(_text=None):
    """Build/version/environment info (Item 0: enhanced with timezone + db schema)."""
    import hashlib
    from datetime import datetime, timezone, timedelta
    lines = ['ğŸ”§ ë²„ì „ ì •ë³´', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
    # git sha
    try:
        rc, sha = _run(['git', 'rev-parse', '--short', 'HEAD'], timeout=5)
        lines.append(f'git_sha: {sha if rc == 0 else "unknown"}')
    except Exception:
        lines.append('git_sha: unknown')
    # build_time = mtime of telegram_cmd_poller.py
    try:
        mtime = os.path.getmtime(f'{APP_DIR}/telegram_cmd_poller.py')
        bt = datetime.fromtimestamp(mtime, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')
        lines.append(f'build_time: {bt} UTC')
    except Exception:
        lines.append('build_time: unknown')
    # CONFIG_VERSION
    try:
        from telegram_cmd_poller import CONFIG_VERSION
        lines.append(f'config_version: {CONFIG_VERSION}')
        config_hash = hashlib.md5(CONFIG_VERSION.encode()).hexdigest()[:12]
        lines.append(f'config_hash: {config_hash}')
    except Exception:
        lines.append('config_version: unknown')
    # DB DSN masked â€” Item 0: host/db/schema explicit
    try:
        lines.append(f'db_dsn: host={DB_CONFIG.get("host", "?")} '
                     f'port={DB_CONFIG.get("port", "?")} '
                     f'db={DB_CONFIG.get("dbname", "?")} '
                     f'user={DB_CONFIG.get("user", "?")} schema=public')
    except Exception:
        lines.append('db_dsn: unknown')
    # process uptime
    uptime = _time.time() - _process_start_time
    lines.append(f'process_uptime_sec: {uptime:.1f}')
    # env
    env = os.getenv('ENV', 'production')
    lines.append(f'env: {env}')
    # query_ts with UTC+KST (Item 0)
    now_utc = datetime.now(timezone.utc)
    kst = timezone(timedelta(hours=9))
    now_kst = now_utc.astimezone(kst)
    lines.append(f'server_time: {now_utc.strftime("%Y-%m-%d %H:%M:%S")}UTC '
                 f'/ {now_kst.strftime("%Y-%m-%d %H:%M:%S")}KST')
    return '\n'.join(lines)


def _debug_router(_text=None):
    """Routing debug state (Item 1: always-populated fields)."""
    from datetime import datetime, timezone
    lines = ['ğŸ”€ ë¼ìš°í„° ë””ë²„ê·¸', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
    # Lazy import to avoid circular import
    try:
        import telegram_cmd_poller as _tcp
        ds = _tcp._last_debug_state
        # Item 1: these must be non-empty â€” self-routing fills them
        detected = ds.get('detected_intent') or 'debug_router'
        handler = ds.get('selected_handler') or '_dispatch_debug(router)'
        lines.append(f'detected_intent: {detected}')
        lines.append(f'selected_handler: {handler}')
        lines.append(f'model_used: {ds.get("model_used", "none")}')
        lines.append(f'decision_ts: {ds.get("decision_ts") or datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")}')
        lines.append(f'last_response_hash: {ds.get("last_response_hash") or "null"}')
        # last_llm_error: show null if empty
        llm_err = ds.get('last_llm_error', '')
        lines.append(f'last_llm_error: {llm_err if llm_err else "null"}')
        # Derive fallback_reason
        model = ds.get('model_used', 'none')
        if model == 'none':
            lines.append('fallback_reason: no_llm_call (direct handler)')
        elif 'gpt' in str(model).lower():
            lines.append('fallback_reason: claude_unavailable_or_budget')
        else:
            lines.append('fallback_reason: none')
        lines.append(f'cache_hit: {ds.get("cache_hit", "N/A")}')
    except Exception as e:
        lines.append(f'router_state: load failed ({e})')
    # Parse nonce from text
    nonce = ''
    if _text:
        m = re.search(r'nonce=(\S+)', _text)
        if m:
            nonce = m.group(1)
    lines.append(f'nonce: {nonce if nonce else "null"}')
    # GPT budget
    try:
        import gpt_router
        gpt_state = gpt_router._load_state()
        today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        gpt_calls = gpt_state.get('daily_calls', {}).get(today, 0)
        lines.append(f'gpt_budget: {gpt_calls}/{gpt_router.DAILY_BUDGET_LIMIT}')
    except Exception:
        pass
    # Claude gate budget
    try:
        import claude_gate
        gate_state = claude_gate._load_state()
        lines.append(f'claude_gate: calls={gate_state.get("daily_calls", 0)} '
                     f'cost=${gate_state.get("daily_cost", 0):.4f}')
    except Exception:
        pass
    return '\n'.join(lines)


def _debug_health(_text=None):
    """Service health (Item 2: registry-based, multi-source, specific reasons)."""
    from datetime import datetime, timezone
    lines = ['ğŸ©º ì„œë¹„ìŠ¤ ìƒíƒœ (ìƒì„¸)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']

    # Get systemctl --all output with timing
    t0 = _time.time()
    rc, sctl_out = _run(['systemctl', 'list-units', '--type=service', '--all'])
    sctl_ms = (_time.time() - t0) * 1000

    # Get heartbeat data from DB + observed intervals
    heartbeats = {}
    observed_intervals = {}
    hb_error = None
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT service, MAX(ts) AS last_ts
                FROM service_health_log
                GROUP BY service;
            """)
            for svc, last_ts in cur.fetchall():
                heartbeats[svc] = last_ts
            # Observed median interval per service over last 2h
            try:
                cur.execute("""
                    SELECT service,
                           PERCENTILE_CONT(0.5) WITHIN GROUP (
                               ORDER BY gap_sec
                           ) AS median_interval
                    FROM (
                        SELECT service,
                               EXTRACT(EPOCH FROM (ts - LAG(ts) OVER (
                                   PARTITION BY service ORDER BY ts
                               ))) AS gap_sec
                        FROM service_health_log
                        WHERE ts >= now() - interval '2 hours'
                    ) sub
                    WHERE gap_sec IS NOT NULL AND gap_sec > 0
                    GROUP BY service;
                """)
                for svc, median in cur.fetchall():
                    observed_intervals[svc] = round(float(median), 1) if median else None
            except Exception:
                pass
    except Exception as e:
        hb_error = str(e)
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass

    ok_count = 0
    down_count = 0
    unknown_count = 0
    now_utc = datetime.now(timezone.utc)
    states_for_log = {}

    for svc in WATCHED_SERVICES:
        reg = SERVICE_REGISTRY.get(svc, {})
        expected_interval = reg.get('expected_interval_sec', 0)
        unit_name = reg.get('systemctl_unit', f'{svc}.service')

        t1 = _time.time()

        # â”€â”€ Check 1: Heartbeat from DB â”€â”€
        hb_ts = heartbeats.get(svc)
        hb_state = None
        hb_reason = None
        age_sec = None
        if hb_error:
            hb_reason = f'db_error:{hb_error[:40]}'
        elif hb_ts:
            if hb_ts.tzinfo is None:
                hb_ts = hb_ts.replace(tzinfo=timezone.utc)
            age_sec = int((now_utc - hb_ts).total_seconds())
            if expected_interval > 0:
                stale_threshold = 3 * expected_interval
                if age_sec < stale_threshold:
                    hb_state = 'OK'
                    hb_reason = f'heartbeat_fresh (age={age_sec}s < {stale_threshold}s)'
                else:
                    hb_state = 'DOWN'
                    hb_reason = f'heartbeat_stale (age={age_sec}s >= {stale_threshold}s)'
            else:
                hb_reason = 'missing_expected_interval'
        else:
            hb_reason = 'no_heartbeat_rows'

        # â”€â”€ Check 2: systemctl process â”€â”€
        proc_state = None
        proc_reason = None
        if rc != 0:
            proc_reason = 'systemctl_failed'
        else:
            found = False
            matched_line = ''
            for line in sctl_out.splitlines():
                if unit_name in line:
                    found = True
                    matched_line = line
                    break
            if not found:
                proc_reason = f'unit_not_found ({unit_name})'
            else:
                ll = matched_line.lower()
                if 'active' in ll and 'running' in ll:
                    proc_state = 'OK'
                    proc_reason = 'active_running'
                elif 'failed' in ll:
                    proc_state = 'DOWN'
                    proc_reason = 'systemctl_failed'
                elif 'inactive' in ll and 'dead' in ll:
                    proc_state = 'DOWN'
                    proc_reason = 'inactive_dead'
                elif 'activating' in ll:
                    proc_reason = 'activating'
                elif 'masked' in ll:
                    proc_reason = 'masked'
                else:
                    proc_reason = f'parse_unknown ({matched_line.strip()[:60]})'

        check_ms = (_time.time() - t1) * 1000

        # â”€â”€ Final verdict: process alive + hb stale â†’ OK+WARN (not DOWN) â”€â”€
        if hb_state == 'OK':
            state = 'OK'
            reason = hb_reason
        elif proc_state == 'OK' and hb_state == 'DOWN':
            state = 'OK'
            reason = f'WARN: {hb_reason} (process alive)'
        elif proc_state == 'OK':
            state = 'OK'
            reason = proc_reason
        elif hb_state == 'DOWN':
            state = 'DOWN'
            reason = hb_reason
        elif proc_state == 'DOWN':
            state = 'DOWN'
            reason = proc_reason
        else:
            state = 'UNKNOWN'
            reasons = []
            if hb_reason:
                reasons.append(f'hb:{hb_reason}')
            if proc_reason:
                reasons.append(f'proc:{proc_reason}')
            reason = '; '.join(reasons) if reasons else 'no_check_source'

        states_for_log[svc] = state
        icon = STATE_ICONS[state]

        # Heartbeat display
        if hb_ts and age_sec is not None:
            hb_display = f'{hb_ts.strftime("%m-%d %H:%M")} ({age_sec}s ago)'
        else:
            hb_display = 'null'

        is_required = svc in REQUIRED_SERVICES

        lines.append(f'{icon} {svc}: {state}')
        lines.append(f'  reason={reason}')
        if expected_interval > 0:
            threshold = 2 * expected_interval
            obs = observed_intervals.get(svc)
            obs_str = f'{obs:.0f}s' if obs is not None else 'N/A'
            interval_line = (f'  expected={expected_interval}s '
                             f'threshold={threshold}s '
                             f'observed_interval={obs_str}')
            if obs is not None and obs > threshold:
                interval_line += ' âš  interval_mismatch'
            lines.append(interval_line)
        lines.append(f'  heartbeat={hb_display} | check={check_ms:.0f}ms')

        if state == 'OK':
            ok_count += 1
        elif state == 'DOWN':
            down_count += 1
        else:
            unknown_count += 1

    # Split summary: required vs optional
    req_ok = sum(1 for s in REQUIRED_SERVICES
                 if states_for_log.get(s) == 'OK')
    req_down = sum(1 for s in REQUIRED_SERVICES
                   if states_for_log.get(s) == 'DOWN')
    opt_ok = sum(1 for s in OPTIONAL_SERVICES
                 if states_for_log.get(s) == 'OK')
    opt_down = sum(1 for s in OPTIONAL_SERVICES
                   if states_for_log.get(s) == 'DOWN')

    total = len(WATCHED_SERVICES)
    lines.insert(2, f'ì „ì²´: {total} | OK: {ok_count} | DOWN: {down_count} | UNKNOWN: {unknown_count}')
    lines.insert(3, f'required: {req_ok} OK {req_down} DOWN | '
                    f'optional: {opt_ok} OK {opt_down} DOWN')
    lines.insert(4, f'systemctl_latency: {sctl_ms:.0f}ms')
    lines.insert(5, '')

    _log_service_health(states_for_log)
    return '\n'.join(lines)


def _debug_gate_details(_text=None):
    """Gate details: per-service OK/DOWN/UNKNOWN + reason + age_sec + gate verdict."""
    snapshot = get_service_health_snapshot()
    services = snapshot.get('services', {})

    lines = ['ğŸ”’ Gate ìƒì„¸ (dual-source)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']

    for svc in WATCHED_SERVICES:
        info = services.get(svc, {})
        state = info.get('state', 'UNKNOWN')
        reason = info.get('reason', '?')
        age = info.get('age_sec')
        icon = STATE_ICONS.get(state, 'â“')
        is_req = svc in REQUIRED_SERVICES
        tag = '' if is_req else ' (ì„ íƒ)'
        age_str = f' age={age}s' if age is not None else ''
        lines.append(f'{icon} {svc}: {state}{tag}{age_str}')
        lines.append(f'  reason={reason}')

    req_down = snapshot.get('required_down', [])
    req_unknown = snapshot.get('required_unknown', [])

    lines.append('')
    lines.append(f'OK: {snapshot.get("ok", 0)} | DOWN: {len(snapshot.get("down", []))} | UNKNOWN: {len(snapshot.get("unknown", []))}')
    lines.append(f'required_down: {req_down or "ì—†ìŒ"}')
    lines.append(f'required_unknown: {req_unknown or "ì—†ìŒ"}')

    # Gate verdict
    if req_down:
        verdict = f'BLOCKED (í•„ìˆ˜ ì„œë¹„ìŠ¤ ì¤‘ì§€: {", ".join(req_down)})'
    elif req_unknown:
        verdict = f'WARN (í•„ìˆ˜ ì„œë¹„ìŠ¤ ë¯¸í™•ì¸: {", ".join(req_unknown)} â€” ì°¨ë‹¨ ì•ˆ í•¨)'
    else:
        verdict = 'PASS'
    lines.append(f'gate_verdict: {verdict}')
    lines.append(f'health_check_ts: {snapshot.get("health_check_ts", "?")}')

    # â”€â”€ Extended: Regime + Entry Filter + Throttle + Rejection â”€â”€
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            # Current regime + confidence
            try:
                import regime_reader
                rc = regime_reader.get_current_regime(cur)
                lines.append('')
                lines.append(f'regime: {rc.get("regime")} (confidence={rc.get("confidence")}%, '
                             f'transition={rc.get("in_transition")}, stale={rc.get("stale")})')
            except Exception as e:
                lines.append(f'regime: error ({e})')

            # Trade switch status
            try:
                sw = exchange_reader.fetch_trade_switch_status()
                lines.append(f'trade_switch: {sw}')
            except Exception as e:
                lines.append(f'trade_switch: error ({e})')

            # Throttle status
            try:
                import order_throttle
                state = order_throttle.get_state_snapshot()
                lines.append(f'throttle: attempts_1h={state.get("attempts_1h", "?")} '
                             f'cooldown_remaining={state.get("cooldown_remaining", 0):.0f}s '
                             f'entry_locked={state.get("entry_locked", False)}')
                if state.get('last_reject_reason'):
                    lines.append(f'  last_reject: {state.get("last_reject_reason")} '
                                 f'(cooldown={state.get("rejection_cooldown_remaining", 0):.0f}s)')
            except Exception as e:
                lines.append(f'throttle: error ({e})')
    except Exception:
        pass
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass

    return '\n'.join(lines)


def _debug_db_coverage(_text=None):
    """DB coverage (Item 3: gap diagnosis + alt table discovery + filter transparency)."""
    from_month = '2023-11'
    if _text:
        m = re.search(r'--from=(\d{4}-\d{2})', _text)
        if m:
            from_month = m.group(1)

    conn = None
    try:
        conn = _db()
        lines = [f'ğŸ“Š DB ì»¤ë²„ë¦¬ì§€ ({from_month}~í˜„ì¬)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        with conn.cursor() as cur:
            tables_config = [
                ('candles', 'ts', 'no filter (all symbols/tf)'),
                ('events', 'start_ts', 'no filter'),
                ('news', 'ts', 'no filter (all sources)'),
            ]
            gap_info = {}  # tbl -> [(gap_start, gap_end)]

            for tbl, ts_col, filter_desc in tables_config:
                try:
                    cur.execute(f"""
                        SELECT MIN({ts_col})::text, MAX({ts_col})::text
                        FROM {tbl};
                    """)
                    rng = cur.fetchone()
                    earliest = (rng[0] or '-')[:16] if rng else '-'
                    latest = (rng[1] or '-')[:16] if rng else '-'

                    cur.execute(f"""
                        SELECT to_char(m, 'YYYY-MM') AS month,
                               COALESCE(c.cnt, 0) AS cnt
                        FROM generate_series(
                            %s::date,
                            date_trunc('month', now())::date,
                            '1 month'::interval
                        ) AS m
                        LEFT JOIN (
                            SELECT date_trunc('month', {ts_col}) AS mo,
                                   count(*) AS cnt
                            FROM {tbl}
                            WHERE {ts_col} >= %s::date
                            GROUP BY mo
                        ) c ON c.mo = m
                        ORDER BY m;
                    """, (f'{from_month}-01', f'{from_month}-01'))
                    rows = cur.fetchall()

                    lines.append(f'\n[{tbl}] ts_col={ts_col} | filter: {filter_desc}')
                    lines.append(f'  range: {earliest} ~ {latest}')
                    gap_count = 0
                    gaps = []
                    gap_start = None
                    prev_month = None
                    for month, cnt in rows:
                        gap_tag = ' <<< GAP' if cnt == 0 else ''
                        if cnt == 0:
                            gap_count += 1
                            if gap_start is None:
                                gap_start = month
                        else:
                            if gap_start is not None and prev_month is not None:
                                gaps.append((gap_start, prev_month))
                                gap_start = None
                        prev_month = month
                        lines.append(f'  {month}: {cnt:,}ê±´{gap_tag}')
                    if gap_start is not None and prev_month is not None:
                        gaps.append((gap_start, prev_month))
                    if gap_count > 0:
                        lines.append(f'  GAPS: {gap_count} months with 0 rows')
                    gap_info[tbl] = gaps
                except Exception as e:
                    lines.append(f'\n[{tbl}] ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # â”€â”€ Item 3B: Alternative table discovery â”€â”€
            lines.append('\n[ëŒ€ì²´ í…Œì´ë¸” íƒìƒ‰]')
            try:
                cur.execute("""
                    SELECT table_name FROM information_schema.tables
                    WHERE table_schema = 'public'
                      AND (table_name ILIKE '%%candle%%'
                           OR table_name ILIKE '%%ohlcv%%'
                           OR table_name ILIKE '%%news%%'
                           OR table_name ILIKE '%%article%%')
                    ORDER BY table_name;
                """)
                candidates = [r[0] for r in cur.fetchall()]
                if candidates:
                    for cand in candidates:
                        try:
                            # Find a ts column
                            cur.execute("""
                                SELECT column_name FROM information_schema.columns
                                WHERE table_name = %s AND table_schema = 'public'
                                  AND (column_name ILIKE '%%ts%%'
                                       OR column_name ILIKE '%%date%%'
                                       OR column_name ILIKE '%%time%%'
                                       OR column_name ILIKE '%%created%%')
                                ORDER BY ordinal_position LIMIT 1;
                            """, (cand,))
                            ts_r = cur.fetchone()
                            ts_c = ts_r[0] if ts_r else None
                            cur.execute(f"SELECT count(*) FROM {cand};")
                            cnt = cur.fetchone()[0] or 0
                            if ts_c:
                                cur.execute(f"SELECT MIN({ts_c})::text, MAX({ts_c})::text FROM {cand};")
                                rng = cur.fetchone()
                                e = (rng[0] or '-')[:16] if rng else '-'
                                l = (rng[1] or '-')[:16] if rng else '-'
                                lines.append(f'  {cand}: {cnt:,}ê±´ ({e} ~ {l}) ts_col={ts_c}')
                            else:
                                lines.append(f'  {cand}: {cnt:,}ê±´ (no ts column found)')
                        except Exception:
                            lines.append(f'  {cand}: ì¡°íšŒ ì‹¤íŒ¨')
                else:
                    lines.append('  none found')
            except Exception as e:
                lines.append(f'  íƒìƒ‰ ì‹¤íŒ¨: {e}')

            # â”€â”€ Item 3C: Gap cause analysis â”€â”€
            lines.append('\n[GAP ì›ì¸ ì§„ë‹¨]')
            for tbl, gaps in gap_info.items():
                if not gaps:
                    lines.append(f'  [{tbl}] no gaps')
                    continue
                for gs, ge in gaps:
                    lines.append(f'  [{tbl}] gap={gs}..{ge}')
                    # Determine primary cause with evidence
                    if tbl == 'candles':
                        lines.append(f'    primary: collector started recently '
                                     f'(evidence: data only from latest period)')
                        lines.append(f'    alt1: historical backfill not yet run')
                        lines.append(f'    alt2: data in market_ohlcv table instead')
                    elif tbl == 'news':
                        # Check if events has data in same period (evidence of system running)
                        try:
                            cur.execute("""
                                SELECT count(*) FROM events
                                WHERE start_ts >= %s::date AND start_ts < %s::date + interval '1 month';
                            """, (f'{gs}-01', f'{ge}-01'))
                            evt_cnt = cur.fetchone()[0] or 0
                        except Exception:
                            evt_cnt = -1
                        if evt_cnt > 0:
                            lines.append(f'    primary: news collector stopped or switched table '
                                         f'(evidence: events has {evt_cnt} rows in same period)')
                        else:
                            lines.append(f'    primary: system may not have been running')
                        lines.append(f'    alt1: rows in news_raw/news_market_reaction tables')
                        lines.append(f'    alt2: ts column mismatch (published_at vs ingested_at)')
                    else:
                        lines.append(f'    primary: data collection not active in this period')

            # â”€â”€ News tier UNKNOWN monthly (Item 3D) â”€â”€
            lines.append('\n[ë‰´ìŠ¤ tier=UNKNOWN ì›”ë³„]')
            try:
                cur.execute("""
                    SELECT to_char(m, 'YYYY-MM') AS month,
                           COALESCE(u.unknown_cnt, 0) AS unknown_cnt,
                           COALESCE(t.total_cnt, 0) AS total_cnt
                    FROM generate_series(
                        %s::date,
                        date_trunc('month', now())::date,
                        '1 month'::interval
                    ) AS m
                    LEFT JOIN (
                        SELECT date_trunc('month', ts) AS mo,
                               count(*) AS unknown_cnt
                        FROM news
                        WHERE ts >= %s::date
                          AND (tier IS NULL OR tier = 'UNKNOWN')
                        GROUP BY mo
                    ) u ON u.mo = m
                    LEFT JOIN (
                        SELECT date_trunc('month', ts) AS mo,
                               count(*) AS total_cnt
                        FROM news
                        WHERE ts >= %s::date
                        GROUP BY mo
                    ) t ON t.mo = m
                    ORDER BY m;
                """, (f'{from_month}-01', f'{from_month}-01', f'{from_month}-01'))
                rows = cur.fetchall()
                total_unknown = 0
                total_all = 0
                for month, unk, tot in rows:
                    if tot == 0:
                        lines.append(f'  {month}: N/A (no data)')
                    else:
                        pct = unk / tot * 100
                        lines.append(f'  {month}: {unk}/{tot} ({pct:.1f}%)')
                    total_unknown += unk
                    total_all += tot
                if total_all > 0:
                    overall_pct = total_unknown / total_all * 100
                    lines.append(f'  ì „ì²´: {total_unknown}/{total_all} ({overall_pct:.1f}%)')
                    if overall_pct >= 99:
                        lines.append(f'  diagnosis: classification gated or not executed '
                                     f'(APPROVAL_REQUIRED=True in news_classifier_config)')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # â”€â”€ Item 6: Canonical price tables â”€â”€
            lines.append('\n[canonical price tables]')
            for tf, cfg in PRICE_TABLE_CANONICAL.items():
                tbl = cfg['table']
                ts_c = cfg['ts_col']
                src = cfg['source']
                try:
                    cur.execute(f"""
                        SELECT count(*),
                               MIN({ts_c})::text,
                               MAX({ts_c})::text,
                               count(*) FILTER (WHERE {ts_c} >= now() - interval '24 hours')
                        FROM {tbl};
                    """)
                    r = cur.fetchone()
                    total, mn, mx, recent = r
                    mn = (mn or '-')[:16]
                    mx = (mx or '-')[:16]
                    lines.append(f'  {tf}: {tbl} | {total:,}ê±´ (24h: {recent:,}) | {mn} ~ {mx}')
                    lines.append(f'    source={src}')
                except Exception as e:
                    lines.append(f'  {tf}: {tbl} ì¡°íšŒ ì‹¤íŒ¨ ({e})')

            # â”€â”€ Item 6: Symbol/tf distribution (top 5) â”€â”€
            lines.append('\n[symbol/tf distribution (top 5)]')
            try:
                cur.execute("""
                    SELECT symbol, tf, count(*) AS cnt
                    FROM candles
                    GROUP BY symbol, tf
                    ORDER BY cnt DESC
                    LIMIT 5;
                """)
                dist_rows = cur.fetchall()
                if dist_rows:
                    for sym, tf, cnt in dist_rows:
                        lines.append(f'  {sym} / {tf}: {cnt:,}ê±´')
                else:
                    lines.append('  ë°ì´í„° ì—†ìŒ')
            except Exception as e:
                lines.append(f'  ì¡°íšŒ ì‹¤íŒ¨: {e}')

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  DB ì»¤ë²„ë¦¬ì§€ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_news_sample(_text=None):
    """News sample with modes: latest (default), allow (allow_only), deny_reason:<reason>.
    Unified param parsing via _parse_debug_args."""
    args = _parse_debug_args(_text)
    n = args['n']
    mode = args['mode']
    allow_only = args['allow_only']

    # allow_only=true overrides mode
    if allow_only:
        mode = 'allow'

    # Determine scan window â€” allow/deny modes need wider scan
    max_scan = n if mode == 'latest' else 1000

    # deny_reason filter
    deny_filter = None
    if mode.startswith('deny_reason:'):
        deny_filter = mode.split(':', 1)[1]
        mode = 'deny_reason'

    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            # Always fetch 24h summary stats first
            cur.execute("""
                SELECT count(*),
                       count(*) FILTER (WHERE source ILIKE 'yahoo_finance') AS yahoo_cnt
                FROM news
                WHERE ts >= now() - interval '24 hours';
            """)
            sr = cur.fetchone()
            total_24h = sr[0] or 0
            yahoo_24h = sr[1] or 0

            # Fetch rows for scan
            cur.execute("""
                SELECT id, title_ko, tier, topic_class, relevance_score,
                       source, impact_score, title, summary,
                       to_char(ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI') AS ts_kr
                FROM news
                WHERE ts >= now() - interval '24 hours'
                ORDER BY ts DESC
                LIMIT %s;
            """, (max_scan,))
            rows = cur.fetchall()

        if not rows:
            return f'ğŸ“° ë‰´ìŠ¤ ìƒ˜í”Œ\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\në°ì´í„° ì—†ìŒ (ìµœê·¼ 24ì‹œê°„)'

        # Import preview classifier
        import news_classifier_config as ncc

        # Classify all scanned rows
        classified = []
        deny_stats = {}
        source_stats = {}
        storage_count_24h = 0
        trading_count_24h = 0
        last_trading_ts = None

        for r in rows:
            nid, title_ko, tier, topic, rel, source, impact, title_en, summary, ts = r
            pv = ncc.preview_classify(
                title=title_en or '', source=source or '',
                impact_score=impact, summary=summary or '',
                title_ko=title_ko or '')
            if pv.get('allow_for_storage', False):
                storage_count_24h += 1
            if pv.get('allow_for_trading', False):
                trading_count_24h += 1
                if last_trading_ts is None:
                    last_trading_ts = ts
            for dr in pv.get('deny_reasons', []):
                deny_stats[dr] = deny_stats.get(dr, 0) + 1
            src = (source or 'unknown').lower()
            source_stats[src] = source_stats.get(src, 0) + 1
            classified.append((r, pv))

        # Filter by mode
        if mode == 'allow':
            # allow_only=true â†’ show allow_for_trading items
            filtered = [(r, pv) for r, pv in classified if pv.get('allow_for_trading', False)]
        elif mode == 'deny_reason' and deny_filter:
            filtered = [(r, pv) for r, pv in classified
                        if deny_filter in pv.get('deny_reasons', [])]
        else:
            # latest mode
            filtered = classified

        # Apply N limit
        display_items = filtered[:n]

        # Build 24h summary line
        top_deny = sorted(deny_stats.items(), key=lambda x: -x[1])[:1]
        top_deny_str = f'{top_deny[0][0]}({top_deny[0][1]})' if top_deny else 'none'
        top_src = sorted(source_stats.items(), key=lambda x: -x[1])[:1]
        top_src_str = f'{top_src[0][0]}({top_src[0][1]})' if top_src else 'none'

        # Header
        scanned = len(classified)
        mode_label = mode
        if mode == 'allow':
            mode_label = 'allow_only=true'
        elif mode == 'deny_reason':
            mode_label = f'deny_reason:{deny_filter}'

        lines = [
            f'ğŸ“° ë‰´ìŠ¤ ìƒ˜í”Œ v2 [APPLIED={str(not ncc.APPROVAL_REQUIRED).lower()}]',
            'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”',
            f'last_24h_total={total_24h}',
            f'  allow_storage={storage_count_24h} | allow_trading={trading_count_24h}',
            f'  top_deny={top_deny_str} | top_src={top_src_str}',
        ]
        if args['n_capped']:
            lines.append(f'mode={mode_label} | --n={n} (capped_to={MAX_N_CAP}) | '
                         f'window=24h | scanned={scanned}')
        else:
            lines.append(f'mode={mode_label} | --n={n} | '
                         f'window=24h | scanned={scanned}')
        if args['meta_args']:
            meta_str = ', '.join(f'{k}={v}' for k, v in args['meta_args'].items())
            lines.append(f'meta_args: {meta_str}')
        if args['ignored']:
            lines.append(f'ignored_args={args["ignored"]}')

        if mode == 'allow':
            lines.append(f'allow_trading_found={len(filtered)}')

        lines.append('')

        # If allow mode and nothing found, show cause summary
        if mode == 'allow' and not display_items:
            lines.append('allow_trading_found=0 â€” ì›ì¸ ìš”ì•½:')
            top3_deny = sorted(deny_stats.items(), key=lambda x: -x[1])[:3]
            for dr, cnt in top3_deny:
                lines.append(f'  deny: {dr} = {cnt}ê±´')
            top3_src = sorted(source_stats.items(), key=lambda x: -x[1])[:3]
            for s, cnt in top3_src:
                lines.append(f'  source: {s} = {cnt}ê±´')
            if last_trading_ts:
                lines.append(f'  last_trading_ts={last_trading_ts}')
            else:
                lines.append(f'  last_trading_ts=none (24h ë‚´ trading í›„ë³´ ì—†ìŒ)')
            return '\n'.join(lines)

        # Display items
        for r, pv in display_items:
            nid, title_ko, tier, topic, rel, source, impact, title_en, summary, ts = r
            display = (title_ko or title_en or '?')[:50]
            impact_s = f'{impact}/10' if impact is not None else '-'
            s_icon = 'S' if pv.get('allow_for_storage', False) else '-'
            t_icon = 'T' if pv.get('allow_for_trading', False) else '-'
            deny = ','.join(pv['deny_reasons']) if pv['deny_reasons'] else '-'
            sw = pv.get('source_weight', pv.get('source_quality_preview', 0))
            lines.append(f'[{pv["tier_preview"]}] {display}')
            lines.append(f'  topic={pv["topic_class_preview"]} '
                         f'rel={pv["relevance_score_preview"]:.2f} '
                         f'w={sw:.2f} impact={impact_s}')
            lines.append(f'  S={s_icon} T={t_icon} deny={deny} '
                         f'src={source or "-"} {ts}')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë‰´ìŠ¤ ìƒ˜í”Œ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_news_reaction_sample(_text=None):
    """News reaction sample with raw/eligible coverage split."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            # Horizon-specific coverage
            cur.execute("""
                SELECT
                    count(*) AS total_traced,
                    count(*) FILTER (WHERE btc_ret_30m IS NOT NULL) AS has_30m,
                    count(*) FILTER (WHERE btc_ret_2h IS NOT NULL) AS has_2h,
                    count(*) FILTER (WHERE btc_ret_24h IS NOT NULL) AS has_24h
                FROM macro_trace;
            """)
            cov = cur.fetchone()
            total_traced = cov[0] or 0
            has_30m = cov[1] or 0
            has_2h = cov[2] or 0
            has_24h = cov[3] or 0

            cur.execute("SELECT count(*) FROM news;")
            total_news = cur.fetchone()[0] or 0
            raw_pct = (total_traced / total_news * 100) if total_news > 0 else 0

            # Eligible news: news within candle coverage range + 24h lookahead
            cur.execute("""
                SELECT count(*) FROM news n
                WHERE n.ts >= (SELECT MIN(ts) FROM candles WHERE tf='1m')
                  AND n.ts + interval '24 hours' <= (SELECT MAX(ts) FROM candles WHERE tf='1m')
                  AND n.ts < now() - interval '24 hours';
            """)
            eligible_news = cur.fetchone()[0] or 0

            # Eligible traced
            cur.execute("""
                SELECT count(*) FROM macro_trace mt
                JOIN news n ON n.id = mt.news_id
                WHERE n.ts >= (SELECT MIN(ts) FROM candles WHERE tf='1m')
                  AND n.ts + interval '24 hours' <= (SELECT MAX(ts) FROM candles WHERE tf='1m');
            """)
            eligible_traced = cur.fetchone()[0] or 0
            eligible_pct = (eligible_traced / eligible_news * 100) if eligible_news > 0 else 0
            traced_pct = raw_pct

            # Prefer rows with most complete data (ORDER BY completeness)
            cur.execute("""
                SELECT n.tier, n.title_ko, n.impact_score,
                       mt.btc_ret_30m, mt.btc_ret_2h, mt.btc_ret_24h,
                       mt.label, mt.regime_at_time,
                       to_char(n.ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI') AS ts_kr,
                       n.ts AS raw_ts,
                       mt.computed_at
                FROM news n
                JOIN macro_trace mt ON mt.news_id = n.id
                WHERE mt.btc_ret_30m IS NOT NULL
                ORDER BY
                    (CASE WHEN mt.btc_ret_24h IS NOT NULL THEN 3
                          WHEN mt.btc_ret_2h IS NOT NULL THEN 2
                          ELSE 1 END) DESC,
                    n.ts DESC
                LIMIT 10;
            """)
            rows = cur.fetchall()

        lines = [
            'ğŸ“° ë‰´ìŠ¤ ë°˜ì‘ ìƒ˜í”Œ',
            'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”',
            f'raw_coverage: {total_traced}/{total_news} ({raw_pct:.1f}%)',
            f'eligible_coverage: {eligible_traced}/{eligible_news} ({eligible_pct:.1f}%)',
            f'  (eligible=ìº”ë“¤ ì»¤ë²„ë¦¬ì§€ ë²”ìœ„ ë‚´ + 24h ë£©ì–´í—¤ë“œ ì¡´ì¬)',
            f'  30m: {has_30m}/{total_traced} | '
            f'2h: {has_2h}/{total_traced} | '
            f'24h: {has_24h}/{total_traced}',
            '',
        ]
        if not rows:
            lines.append('ë°˜ì‘ ë°ì´í„° ì—†ìŒ')
        else:
            from datetime import datetime, timezone, timedelta
            now_utc = datetime.now(timezone.utc)
            for r in rows:
                tier, title, impact, r30, r2h, r24h, label, regime, ts, raw_ts, computed_at = r
                title = (title or '?')[:45]
                tier = tier or 'UNKNOWN'
                r30s = f'{r30:+.2f}%' if r30 is not None else '-'
                # For 2h/24h: show pending reason if null
                if r2h is not None:
                    r2hs = f'{r2h:+.2f}%'
                elif raw_ts:
                    if raw_ts.tzinfo is None:
                        raw_ts = raw_ts.replace(tzinfo=timezone.utc)
                    age_h = (now_utc - raw_ts).total_seconds() / 3600
                    if age_h < 2:
                        ready = raw_ts + timedelta(hours=2)
                        r2hs = f'pending(ready={ready.strftime("%H:%M")}UTC)'
                    else:
                        r2hs = 'missing(horizon_passed)'
                else:
                    r2hs = '-'

                if r24h is not None:
                    r24hs = f'{r24h:+.2f}%'
                elif raw_ts:
                    if raw_ts.tzinfo is None:
                        raw_ts = raw_ts.replace(tzinfo=timezone.utc)
                    age_h = (now_utc - raw_ts).total_seconds() / 3600
                    if age_h < 24:
                        ready = raw_ts + timedelta(hours=24)
                        r24hs = f'pending(ready={ready.strftime("%m-%d %H:%M")}UTC)'
                    else:
                        r24hs = 'missing(horizon_passed)'
                else:
                    r24hs = '-'

                impact_s = f'{impact}/10' if impact is not None else '-'
                lines.append(f'[{tier}] {title}')
                lines.append(f'  30m={r30s} 2h={r2hs} 24h={r24hs}')
                lines.append(f'  label={label or "-"} regime={regime or "-"} '
                             f'impact={impact_s} {ts}')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë‰´ìŠ¤ ë°˜ì‘ ìƒ˜í”Œ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _backfill_dryrun_summary(cur):
    """Compact 1-line-per-job dryrun summary for embedding in backfill_status.
    Returns list of summary lines."""
    summary = []
    try:
        cur.execute("""
            SELECT EXTRACT(EPOCH FROM (now() - '2023-11-01'::timestamp)) / 60 AS expected,
                   (SELECT count(*) FROM candles WHERE tf = '1m') AS actual;
        """)
        r = cur.fetchone()
        exp, act = int(r[0] or 0), int(r[1] or 0)
        summary.append(f'  candles_1m: remaining={max(0, exp - act):,}')
    except Exception:
        summary.append(f'  candles_1m: ì¡°íšŒ ì‹¤íŒ¨')

    try:
        cur.execute("""
            SELECT EXTRACT(EPOCH FROM (now() - '2023-11-01'::timestamp)) / 300 AS expected,
                   (SELECT count(*) FROM market_ohlcv) AS actual;
        """)
        r = cur.fetchone()
        exp, act = int(r[0] or 0), int(r[1] or 0)
        summary.append(f'  ohlcv_5m: remaining={max(0, exp - act):,}')
    except Exception:
        summary.append(f'  ohlcv_5m: ì¡°íšŒ ì‹¤íŒ¨')

    try:
        cur.execute("""
            SELECT count(*) FILTER (WHERE tier IS NULL OR tier = 'UNKNOWN'),
                   count(*)
            FROM news;
        """)
        r = cur.fetchone()
        summary.append(f'  news_classify: remaining={r[0] or 0:,}')
    except Exception:
        summary.append(f'  news_classify: ì¡°íšŒ ì‹¤íŒ¨')

    try:
        cur.execute("""
            SELECT (SELECT count(*) FROM news) - (SELECT count(DISTINCT news_id) FROM macro_trace);
        """)
        r = cur.fetchone()
        summary.append(f'  macro_trace: remaining={max(0, r[0] or 0):,}')
    except Exception:
        summary.append(f'  macro_trace: ì¡°íšŒ ì‹¤íŒ¨')

    try:
        cur.execute("SELECT count(*) FROM price_events;")
        cnt = cur.fetchone()[0] or 0
        if cnt == 0:
            summary.append(f'  âš  price_events: never_run (0ê±´) â€” ë°±í•„ í•„ìˆ˜')
        else:
            summary.append(f'  price_events: {cnt:,}ê±´')
    except Exception:
        summary.append(f'  âš  price_events: í…Œì´ë¸” ë¯¸ìƒì„± â€” ë°±í•„ í•„ìˆ˜')

    return summary


def _debug_backfill_status(_text=None):
    """Backfill status (Item 6: progress % + ETA + gated note)."""
    from backfill_utils import (
        get_running_pid, is_backfill_enabled, read_exit_status,
        PAUSE_FILE, STOP_FILE,
    )

    conn = None
    try:
        conn = _db()

        # Runner status header
        runner_pid = get_running_pid()
        enabled = is_backfill_enabled()
        gate_str = 'âœ… enabled' if enabled else 'ğŸ”’ disabled'

        runner_lines = []
        if runner_pid:
            state = 'RUNNING'
            if os.path.exists(PAUSE_FILE):
                state = 'PAUSED'
            elif os.path.exists(STOP_FILE):
                state = 'STOPPING'
            runner_lines.append(f'[Runner] PID={runner_pid} state={state} gate={gate_str}')
        else:
            runner_lines.append(f'[Runner] ë¯¸ì‹¤í–‰ (gate={gate_str})')
            # Show last exit reason if runner is dead
            exit_info = read_exit_status()
            if exit_info:
                runner_lines.append(
                    f'[Last Exit] {exit_info.get("status", "?")} '
                    f'@ {exit_info.get("ts", "?")} '
                    f'job={exit_info.get("job_key", "?")}'
                )
                reason = exit_info.get('reason', '')
                if reason:
                    runner_lines.append(f'  reason: {reason[:200]}')

        with conn.cursor() as cur:
            cur.execute("""
                SELECT DISTINCT ON (job_name)
                       job_name, status, inserted, updated, failed,
                       started_at, finished_at, last_cursor, error, metadata
                FROM backfill_job_runs
                ORDER BY job_name, started_at DESC;
            """)
            rows = cur.fetchall()

        if not rows:
            lines = ['ğŸ“¦ ë°±í•„ ì‘ì—… í˜„í™©', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
            lines.extend(runner_lines)
            lines.extend(['', 'backfill_job_runs ë°ì´í„° ì—†ìŒ (ì•„ì§ ì‹¤í–‰í•œ ì  ì—†ìŒ)', ''])
            lines.append('[ì”ì—¬ ì‘ì—… ìš”ì•½ (dryrun)]')
            try:
                with conn.cursor() as cur2:
                    lines.extend(_backfill_dryrun_summary(cur2))
            except Exception:
                lines.append('  dryrun ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨')
            lines.append('')
            if not enabled:
                lines.append('âš  /debug backfill_enable on â†’ /debug backfill_start job=... write=true')
            return '\n'.join(lines)

        status_icons = {'SUCCESS': 'âœ…', 'RUNNING': 'ğŸ”„', 'FAILED': 'âŒ',
                        'COMPLETED': 'âœ…', 'PARTIAL': 'â¸'}
        lines = ['ğŸ“¦ ë°±í•„ ì‘ì—… í˜„í™©', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        lines.extend(runner_lines)
        lines.append('')
        # Build reverse alias map: canonical -> [aliases]
        _reverse_aliases = {}
        for alias, canonical in _BACKFILL_JOB_ALIASES.items():
            _reverse_aliases.setdefault(canonical, []).append(alias)

        for r in rows:
            job, status, ins, upd, fail, started, finished, cursor, err, meta = r
            icon = status_icons.get(status, 'â“')
            ins = ins or 0
            upd = upd or 0
            fail = fail or 0

            # Elapsed
            if started and finished:
                elapsed_sec = (finished - started).total_seconds()
                elapsed_str = f'{elapsed_sec / 60:.1f}min'
            elif started:
                elapsed_sec = _time.time() - started.timestamp()
                elapsed_str = f'{elapsed_sec / 60:.1f}min (running)'
            else:
                elapsed_sec = 0
                elapsed_str = '-'

            # Progress % from metadata if available
            remaining = None
            if meta and isinstance(meta, dict):
                remaining = meta.get('remaining')
            total_est = (ins + (remaining or 0)) if remaining else None
            alias_list = _reverse_aliases.get(job, [])
            alias_tag = f' (alias: {", ".join(alias_list)})' if alias_list else ''
            if total_est and total_est > 0:
                pct = ins / total_est * 100
                lines.append(f'{icon} {job}{alias_tag}: {status} ({pct:.1f}%)')
            else:
                lines.append(f'{icon} {job}{alias_tag}: {status}')

            lines.append(f'  inserted={ins:,} updated={upd:,} failed={fail:,} '
                         f'elapsed={elapsed_str}')
            lines.append(f'  started={str(started)[:19] if started else "-"}')

            # Show start/end range from metadata
            if meta and isinstance(meta, dict):
                start_ms = meta.get('start_ms')
                end_ms = meta.get('end_ms')
                if start_ms:
                    from datetime import datetime as _dt, timezone as _tz
                    try:
                        s_dt = _dt.fromtimestamp(start_ms / 1000, tz=_tz.utc)
                        range_str = f'  range: {s_dt.strftime("%Y-%m-%d")}'
                        if end_ms:
                            e_dt = _dt.fromtimestamp(end_ms / 1000, tz=_tz.utc)
                            range_str += f' â†’ {e_dt.strftime("%Y-%m-%d")}'
                        lines.append(range_str)
                    except Exception:
                        pass

            # Rate + ETA for RUNNING jobs
            if status in ('RUNNING', 'PARTIAL') and started and elapsed_sec > 0 and ins > 0:
                rate = ins / elapsed_sec
                lines.append(f'  rate={rate:.1f} rows/sec')
                if remaining and remaining > 0:
                    eta_sec = remaining / rate
                    lines.append(f'  eta={eta_sec / 60:.0f}min ({remaining:,} remaining)')

            # Cursor-based progress % for RUNNING jobs
            if status in ('RUNNING', 'PARTIAL') and meta and isinstance(meta, dict) and cursor:
                try:
                    import json as _json_p
                    cursor_dict = cursor if isinstance(cursor, dict) else _json_p.loads(cursor)
                    start_ms = meta.get('start_ms')
                    end_ms = meta.get('end_ms')
                    since_val = cursor_dict.get('since_ms')
                    if start_ms and end_ms and since_val and end_ms > start_ms:
                        pct = (since_val - start_ms) / (end_ms - start_ms) * 100
                        pct = max(0, min(100, pct))
                        lines.append(f'  progress={pct:.1f}%')
                except Exception:
                    pass

            # Cursor: parse since_ms + detailed metrics
            if cursor:
                cursor_display = str(cursor)[:80]
                try:
                    import json as _json
                    cursor_dict = cursor if isinstance(cursor, dict) else _json.loads(cursor)
                    since_val = cursor_dict.get('since_ms')
                    if since_val:
                        from datetime import datetime as _dt2, timezone as _tz2
                        c_dt = _dt2.fromtimestamp(since_val / 1000, tz=_tz2.utc)
                        cursor_display = f'at {c_dt.strftime("%Y-%m-%d %H:%M")}'

                    # Show detailed metrics if available (from new backfill_candles)
                    conflict_cnt = cursor_dict.get('conflict_count')
                    error_cnt = cursor_dict.get('error_count')
                    latency = cursor_dict.get('last_api_latency_ms')
                    returned = cursor_dict.get('last_returned_rows')
                    stall = cursor_dict.get('last_stall_reason')
                    last_err = cursor_dict.get('last_error')

                    if conflict_cnt is not None or error_cnt is not None:
                        detail_parts = []
                        if conflict_cnt is not None:
                            detail_parts.append(f'dup={conflict_cnt}')
                        if error_cnt is not None:
                            detail_parts.append(f'err={error_cnt}')
                        if latency is not None:
                            detail_parts.append(f'latency={latency}ms')
                        if returned is not None:
                            detail_parts.append(f'last_rows={returned}')
                        lines.append(f'  cursor={cursor_display}')
                        lines.append(f'  metrics: {" ".join(detail_parts)}')
                        if stall:
                            lines.append(f'  stall: {stall}')
                        if last_err:
                            lines.append(f'  last_error: {str(last_err)[:100]}')
                    else:
                        lines.append(f'  cursor={cursor_display}')
                except Exception:
                    lines.append(f'  cursor={cursor_display}')
            if err:
                lines.append(f'  error={str(err)[:200]}')

        # Data range display
        lines.append('')
        lines.append('[ë°ì´í„° ë²”ìœ„]')
        try:
            with conn.cursor() as cur_range:
                cur_range.execute("""
                    SELECT MIN(ts), MAX(ts), COUNT(*) FROM candles WHERE tf='1m';
                """)
                c1m = cur_range.fetchone()
                cur_range.execute("""
                    SELECT MIN(ts), MAX(ts), COUNT(*) FROM market_ohlcv WHERE tf='5m';
                """)
                o5m = cur_range.fetchone()
            if c1m and c1m[2]:
                lines.append(f'  candles(1m):      {str(c1m[0])[:10]} ~ {str(c1m[1])[:10]} ({c1m[2]:,} rows)')
            else:
                lines.append('  candles(1m):      ë°ì´í„° ì—†ìŒ')
            if o5m and o5m[2]:
                lines.append(f'  market_ohlcv(5m): {str(o5m[0])[:10]} ~ {str(o5m[1])[:10]} ({o5m[2]:,} rows)')
            else:
                lines.append('  market_ohlcv(5m): ë°ì´í„° ì—†ìŒ')
        except Exception:
            lines.append('  ë°ì´í„° ë²”ìœ„ ì¡°íšŒ ì‹¤íŒ¨')

        lines.append('')
        lines.append('[ì”ì—¬ ì‘ì—… ìš”ì•½ (dryrun)]')
        try:
            with conn.cursor() as cur2:
                lines.extend(_backfill_dryrun_summary(cur2))
        except Exception:
            lines.append('  dryrun ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨')
        lines.append('')
        if not enabled:
            lines.append('âš  /debug backfill_enable on â†’ /debug backfill_start job=... write=true')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë°±í•„ í˜„í™© ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


BACKFILL_JOB_DEPS = {
    'candles_1m': {
        'source': 'bybit REST API (kline)',
        'target': 'candles (tf=1m)',
        'script': 'backfill_ohlcv.py',
        'depends_on': [],
    },
    'ohlcv_5m': {
        'source': 'candles (tf=1m) aggregate',
        'target': 'market_ohlcv',
        'script': 'aggregate_candles.py',
        'depends_on': ['candles_1m'],
    },
    'news_classify': {
        'source': 'news (raw rows)',
        'target': 'news (tier/topic_class columns)',
        'script': 'backfill_news_classification_and_reaction.py',
        'depends_on': [],
    },
    'macro_trace': {
        'source': 'news + candles',
        'target': 'macro_trace (btc_ret_30m/2h/24h)',
        'script': 'backfill_macro_trace.py',
        'depends_on': ['candles_1m'],
    },
    'price_events': {
        'source': 'candles + indicators',
        'target': 'price_events',
        'script': 'build_price_events.py',
        'depends_on': ['candles_1m'],
    },
}


def _debug_backfill_dryrun(_text=None):
    """Backfill dryrun (Item 6: per-table, per-field estimates + gated)."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ“¦ ë°±í•„ ì”ì—¬ëŸ‰ ì¶”ì • (dryrun)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        with conn.cursor() as cur:
            def _append_dep_info(job_key):
                dep = BACKFILL_JOB_DEPS.get(job_key, {})
                if dep:
                    lines.append(f'  source={dep["source"]} â†’ target={dep["target"]}')
                    lines.append(f'  script={dep["script"]}')
                    if dep.get('depends_on'):
                        lines.append(f'  depends_on: {", ".join(dep["depends_on"])}')

            # candles_1m
            try:
                cur.execute("""
                    SELECT EXTRACT(EPOCH FROM (now() - '2023-11-01'::timestamp)) / 60 AS expected,
                           (SELECT count(*) FROM candles WHERE tf = '1m') AS actual;
                """)
                r = cur.fetchone()
                expected = int(r[0]) if r[0] else 0
                actual = int(r[1]) if r[1] else 0
                remaining = max(0, expected - actual)
                pct = (actual / expected * 100) if expected > 0 else 0
                lines.append(f'candles_1m: {actual:,}/{expected:,} ({pct:.1f}%) '
                             f'remaining={remaining:,}')
                _append_dep_info('candles_1m')
            except Exception as e:
                lines.append(f'candles_1m: ì¡°íšŒ ì‹¤íŒ¨ ({e})')

            # ohlcv_5m
            try:
                cur.execute("""
                    SELECT EXTRACT(EPOCH FROM (now() - '2023-11-01'::timestamp)) / 300 AS expected,
                           (SELECT count(*) FROM market_ohlcv) AS actual;
                """)
                r = cur.fetchone()
                expected = int(r[0]) if r[0] else 0
                actual = int(r[1]) if r[1] else 0
                remaining = max(0, expected - actual)
                pct = (actual / expected * 100) if expected > 0 else 0
                lines.append(f'ohlcv_5m: {actual:,}/{expected:,} ({pct:.1f}%) '
                             f'remaining={remaining:,}')
                _append_dep_info('ohlcv_5m')
            except Exception as e:
                lines.append(f'ohlcv_5m: ì¡°íšŒ ì‹¤íŒ¨ ({e})')

            # news_classify
            try:
                cur.execute("""
                    SELECT count(*) FILTER (WHERE tier IS NULL OR tier = 'UNKNOWN') AS unclassified,
                           count(*) AS total
                    FROM news;
                """)
                r = cur.fetchone()
                unc = r[0] or 0
                total = r[1] or 0
                pct = ((total - unc) / total * 100) if total > 0 else 0
                lines.append(f'news_classify: classified={total - unc}/{total} ({pct:.1f}%) '
                             f'remaining={unc:,}')
                _append_dep_info('news_classify')
            except Exception as e:
                lines.append(f'news_classify: ì¡°íšŒ ì‹¤íŒ¨ ({e})')

            # macro_trace
            try:
                cur.execute("""
                    SELECT (SELECT count(*) FROM news) AS total_news,
                           (SELECT count(DISTINCT news_id) FROM macro_trace) AS traced;
                """)
                r = cur.fetchone()
                total = r[0] or 0
                traced = r[1] or 0
                missing = max(0, total - traced)
                pct = (traced / total * 100) if total > 0 else 0
                lines.append(f'macro_trace: traced={traced}/{total} ({pct:.1f}%) '
                             f'remaining={missing:,}')
                # Per-horizon completion
                cur.execute("""
                    SELECT count(*) FILTER (WHERE btc_ret_2h IS NULL AND btc_ret_30m IS NOT NULL) AS need_2h,
                           count(*) FILTER (WHERE btc_ret_24h IS NULL AND btc_ret_30m IS NOT NULL) AS need_24h
                    FROM macro_trace;
                """)
                hr = cur.fetchone()
                lines.append(f'  need_2h_update={hr[0] or 0} need_24h_update={hr[1] or 0}')
                _append_dep_info('macro_trace')
            except Exception as e:
                lines.append(f'macro_trace: ì¡°íšŒ ì‹¤íŒ¨ ({e})')

            # price_events
            try:
                cur.execute("SELECT count(*) FROM price_events;")
                cnt = cur.fetchone()[0] or 0
                status = f'{cnt:,}ê±´' if cnt > 0 else 'Never run (0ê±´)'
                lines.append(f'price_events: {status}')
                _append_dep_info('price_events')
            except Exception as e:
                lines.append(f'price_events: ì¡°íšŒ ì‹¤íŒ¨ ({e})')

        lines.append('')
        lines.append('âš  dryrun only â€” ì‹¤í–‰ì€ ìŠ¹ì¸ í›„ enable (gated)')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë°±í•„ ì”ì—¬ëŸ‰ ì¶”ì • ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_news_filter_stats(_text=None):
    """24h news v2 filter stats (two-tier allow: storage vs trading)."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ“° ë‰´ìŠ¤ í•„í„° í†µê³„ v2 (24h)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        with conn.cursor() as cur:
            cur.execute("""
                SELECT id, title, source, impact_score, summary, title_ko
                FROM news
                WHERE ts >= now() - interval '24 hours'
                ORDER BY ts DESC;
            """)
            rows = cur.fetchall()

        if not rows:
            return '\n'.join(lines + ['ë°ì´í„° ì—†ìŒ (ìµœê·¼ 24ì‹œê°„)'])

        import news_classifier_config as ncc
        total = len(rows)
        tier_counts = {}
        topic_counts = {}
        deny_counts = {}
        source_counts = {}
        allow_storage_count = 0
        allow_trading_count = 0

        for r in rows:
            nid, title, source, impact, summary, title_ko = r
            result = ncc.preview_classify(
                title or '', source or '', impact or 0,
                summary=summary or '', title_ko=title_ko or '')
            tier = result.get('tier_preview', 'UNKNOWN')
            topic = result.get('topic_class_preview', 'unclassified')
            deny_reasons = result.get('deny_reasons', [])

            tier_counts[tier] = tier_counts.get(tier, 0) + 1
            topic_counts[topic] = topic_counts.get(topic, 0) + 1
            src = (source or 'unknown').lower()
            source_counts[src] = source_counts.get(src, 0) + 1
            if result.get('allow_for_storage', False):
                allow_storage_count += 1
            if result.get('allow_for_trading', False):
                allow_trading_count += 1
            for dr in deny_reasons:
                deny_counts[dr] = deny_counts.get(dr, 0) + 1

        deny_count = total - allow_storage_count
        storage_pct = allow_storage_count / total * 100 if total else 0
        trading_pct = allow_trading_count / total * 100 if total else 0
        lines.append(f'total={total}')
        lines.append(f'  allow_storage={allow_storage_count} ({storage_pct:.1f}%)')
        lines.append(f'  allow_trading={allow_trading_count} ({trading_pct:.1f}%)')
        lines.append(f'  deny={deny_count}')
        lines.append('')

        # Tier distribution
        lines.append('[tier distribution]')
        for t in sorted(tier_counts, key=tier_counts.get, reverse=True):
            pct = tier_counts[t] / total * 100
            lines.append(f'  {t}: {tier_counts[t]} ({pct:.1f}%)')

        # Topic distribution
        lines.append('\n[topic distribution]')
        for t in sorted(topic_counts, key=topic_counts.get, reverse=True):
            pct = topic_counts[t] / total * 100
            lines.append(f'  {t}: {topic_counts[t]} ({pct:.1f}%)')

        # Deny reason distribution
        lines.append('\n[deny_reason distribution]')
        if deny_counts:
            for dr in sorted(deny_counts, key=deny_counts.get, reverse=True):
                lines.append(f'  {dr}: {deny_counts[dr]}')
        else:
            lines.append('  none')

        # Source distribution (top 10)
        lines.append('\n[source distribution (top 10)]')
        for s in sorted(source_counts, key=source_counts.get, reverse=True)[:10]:
            lines.append(f'  {s}: {source_counts[s]}')

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë‰´ìŠ¤ í•„í„° í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_state(_text=None):
    """System state (Item 7: feature flags, directives, backfill summary)."""
    lines = ['ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ ë³€ìˆ˜', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
    conn = None

    try:
        conn = _db()
        with conn.cursor() as cur:
            # trade_switch
            try:
                cur.execute("""
                    SELECT enabled, updated_at
                    FROM trade_switch
                    ORDER BY id DESC LIMIT 1;
                """)
                row = cur.fetchone()
                if row:
                    lines.append(f'trade_switch: enabled={row[0]} '
                                 f'updated_at={str(row[1])[:19] if row[1] else "-"}')
                else:
                    lines.append('trade_switch: ë ˆì½”ë“œ ì—†ìŒ')
            except Exception as e:
                lines.append(f'trade_switch: ì¡°íšŒ ì‹¤íŒ¨ ({e})')

            # â”€â”€ openclaw_policies + directives â”€â”€
            try:
                cur.execute("""
                    SELECT key, value, updated_at, description
                    FROM openclaw_policies
                    ORDER BY key;
                """)
                rows = cur.fetchall()
                if rows:
                    lines.append('')
                    lines.append('[openclaw_policies / directives]')
                    for k, v, updated_at, desc in rows:
                        v_str = str(v)[:60] if v else '-'
                        updated = str(updated_at)[:19] if updated_at else '-'
                        lines.append(f'  {k}: {v_str}')
                        lines.append(f'    set_at={updated} desc={str(desc)[:40] if desc else "-"}')
                else:
                    lines.append('openclaw_policies: ë°ì´í„° ì—†ìŒ')
            except Exception:
                lines.append('openclaw_policies: í…Œì´ë¸” ë¯¸ì¡´ì¬')

            # â”€â”€ Backfill summary â”€â”€
            lines.append('')
            lines.append('[backfill_summary]')
            try:
                cur.execute("""
                    SELECT DISTINCT ON (job_name) job_name, status, finished_at
                    FROM backfill_job_runs
                    ORDER BY job_name, started_at DESC;
                """)
                bf_rows = cur.fetchall()
                if bf_rows:
                    for job, status, finished in bf_rows:
                        fin = str(finished)[:19] if finished else '-'
                        lines.append(f'  {job}: {status} (finished={fin})')
                else:
                    lines.append('  no backfill jobs recorded')
            except Exception:
                lines.append('  backfill_job_runs: ì¡°íšŒ ì‹¤íŒ¨')
    except Exception as e:
        lines.append(f'DB ì—°ê²° ì‹¤íŒ¨: {e}')
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass

    # â”€â”€ Exposure cap / symbol whitelist â”€â”€
    lines.append('')
    lines.append('[exposure_cap]')
    _cap_source = 'dynamic'
    _cap = 900
    try:
        from trading_config import ALLOWED_SYMBOLS
        import safety_manager
        eq = safety_manager.get_equity_limits()
        _cap = eq['operating_cap']
        lines.append(f'  ALLOWED_SYMBOLS: {", ".join(sorted(ALLOWED_SYMBOLS))}')
        lines.append(f'  operating_cap: {eq["operating_cap"]} (equity={eq["equity"]}, src={eq["source"]})')
    except Exception:
        _cap_source = 'FALLBACK'
        lines.append('  (equity_limits ì¡°íšŒ ì‹¤íŒ¨)')
    conn2 = None
    try:
        conn2 = _db()
        with conn2.cursor() as cur2:
            cur2.execute("""
                SELECT side, total_qty, capital_used_usdt
                FROM position_state WHERE symbol = %s;
            """, ('BTC/USDT:USDT',))
            ps = cur2.fetchone()
            if ps and ps[0]:
                side_str = ps[0]
                qty_val = float(ps[1] or 0)
                cap_used = float(ps[2] or 0)
                remaining = max(0, _cap - cap_used)
                lines.append(f'  position: {side_str} qty={qty_val} capital_used={cap_used:.1f}')
                lines.append(f'  remaining_cap: {remaining:.1f} USDT [{_cap_source}]')
            else:
                lines.append('  position: FLAT')
                lines.append(f'  remaining_cap: {_cap} USDT [{_cap_source}]')
            # Cap block/shrink counts
            cur2.execute("""
                SELECT event, count(*)
                FROM live_executor_log
                WHERE event IN ('CAP_BLOCKED', 'CAP_SHRINK')
                  AND ts >= now() - interval '24 hours'
                GROUP BY event;
            """)
            cap_rows = cur2.fetchall()
            if cap_rows:
                for ev, cnt in cap_rows:
                    lines.append(f'  {ev}_24h: {cnt}ê±´')
            else:
                lines.append('  cap_events_24h: 0ê±´')
    except Exception as e:
        lines.append(f'  exposure ì¡°íšŒ ì‹¤íŒ¨: {e}')
    finally:
        if conn2:
            try:
                conn2.close()
            except Exception:
                pass

    # test_mode (no DB needed)
    try:
        import test_utils
        test_mode = test_utils.load_test_mode()
        is_active = test_utils.is_test_active()
        lines.append(f'test_mode: loaded={test_mode is not None} active={is_active}')
    except Exception as e:
        lines.append(f'test_mode: ë¡œë“œ ì‹¤íŒ¨ ({e})')

    # LIVE_TRADING env var
    live = os.getenv('LIVE_TRADING', 'unset')
    lines.append(f'LIVE_TRADING: {live}')

    # â”€â”€ Feature flags / approval pending â”€â”€
    lines.append('')
    lines.append('[feature_flags]')
    try:
        from backfill_utils import is_backfill_enabled
        bf_enabled = is_backfill_enabled()
        lines.append(f'  backfill: ENABLED={bf_enabled} '
                     f'(/debug backfill_enable on|off)')
    except Exception:
        lines.append(f'  backfill: check failed')
    try:
        import news_classifier_config as ncc
        _applied = not ncc.APPROVAL_REQUIRED
        lines.append(f'  news_classifier: APPLIED={_applied} '
                     f'(APPROVAL_REQUIRED={ncc.APPROVAL_REQUIRED})')
    except Exception:
        lines.append(f'  news_classifier: import failed')

    # state_mode from telegram_cmd_poller
    lines.append('')
    try:
        import telegram_cmd_poller as _tcp
        ds = _tcp._last_debug_state
        lines.append(f'state_mode: {ds.get("state_mode", "chat")}')
        lines.append(f'last_detected_intent: {ds.get("detected_intent") or "null"}')
        lines.append(f'last_decision_ts: {ds.get("decision_ts") or "null"}')
    except Exception:
        lines.append('state_mode: unknown')

    # â”€â”€ LLM routing policy â”€â”€
    lines.append('')
    lines.append('[LLM ë¼ìš°íŒ… ì •ì±…]')
    lines.append('  ë©”ì¸: GPT (gpt-4o-mini) â€” ëª¨ë“  ìì—°ì–´ ëŒ€í™”')
    lines.append('  ë³´ì¡°: Claude (sonnet) â€” "í´ë¡œë“œ/claude/ê¸´ê¸‰" í‚¤ì›Œë“œ ì‹œë§Œ')
    lines.append('  Fallback: keyword_fallback â†’ local_query_executor')

    # Analysis-only reminder
    lines.append('')
    lines.append('âš  ì ìš© ê¸ˆì§€ (analysis only)')

    return '\n'.join(lines)


# â”€â”€ Backfill control handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Available backfill jobs
_BACKFILL_JOBS = {
    'candles_1m':    'backfill_candles.py â€” 1m ìº”ë“¤ (Bybit â†’ candles)',
    'ohlcv_5m':      'backfill_ohlcv.py â€” 5m OHLCV (Bybit â†’ market_ohlcv)',
    'aggregate_5m':  'aggregate_candles.py â€” 1mâ†’5m/15m/1h ì§‘ê³„',
    'price_events':  'build_price_events.py â€” ê°€ê²© ì´ë²¤íŠ¸ íƒì§€',
    'macro_trace':   'backfill_macro_trace.py â€” ë‰´ìŠ¤ BTC ë°˜ì‘',
    'news_classify': 'backfill_news_classification_and_reaction.py â€” ë‰´ìŠ¤ ë¶„ë¥˜',
    'news_reaction': 'backfill_news_classification_and_reaction.py â€” ì‹œì¥ ë°˜ì‘',
    'link_events':   'link_event_to_news.py â€” ì´ë²¤íŠ¸â†”ë‰´ìŠ¤ ì—°ê²°',
    'news_path':     'backfill_news_path.py â€” ë‰´ìŠ¤ 24h ê²½ë¡œ ë¶„ì„',
    'prune_1m':      'prune_candles_1m.py â€” ì˜¤ë˜ëœ 1m ìº”ë“¤ ì •ë¦¬ (>180d)',
    'archive':       'backfill_archive.py â€” Binance ì•„ì¹´ì´ë¸Œ ë²Œí¬ ì ì¬ (cold store)',
}

# Aliases for common alternative job names
_BACKFILL_JOB_ALIASES = {
    'prune_candles_1m': 'prune_1m',
    'backfill_candles_1m': 'candles_1m',
    'backfill_ohlcv_5m': 'ohlcv_5m',
    'backfill_news_path': 'news_path',
    'aggregate_candles': 'aggregate_5m',
}


def _parse_backfill_args(text):
    """Parse backfill args: job=X from=Y to=Z tf=W write=true"""
    args = {}
    if not text:
        return args
    # Strip the command prefix
    t = text.strip()
    for part in t.split():
        if '=' in part:
            k, v = part.split('=', 1)
            args[k.lower()] = v
    return args


def _debug_backfill_enable(_text=None):
    """Enable or disable backfill execution."""
    from backfill_utils import is_backfill_enabled, set_backfill_enabled

    t = (_text or '').strip().lower()
    # Parse on/off from text
    if 'on' in t or 'true' in t or 'enable' in t:
        set_backfill_enabled(True)
        return ('âœ… ë°±í•„ ì‹¤í–‰ ENABLED\n'
                'backfill_start write=true ì‹¤í–‰ ê°€ëŠ¥\n'
                'ë¹„í™œì„±í™”: /debug backfill_enable off')
    elif 'off' in t or 'false' in t or 'disable' in t:
        set_backfill_enabled(False)
        return ('ğŸ”’ ë°±í•„ ì‹¤í–‰ DISABLED\n'
                'backfill_start write=true ì°¨ë‹¨ë¨')
    else:
        enabled = is_backfill_enabled()
        state = 'ON (ì‹¤í–‰ ê°€ëŠ¥)' if enabled else 'OFF (ì°¨ë‹¨ ì¤‘)'
        return (f'ğŸ”§ ë°±í•„ ì‹¤í–‰ ê²Œì´íŠ¸: {state}\n\n'
                f'/debug backfill_enable on â€” í™œì„±í™”\n'
                f'/debug backfill_enable off â€” ë¹„í™œì„±í™”')


def _debug_backfill_start(_text=None):
    """Start a backfill job via backfill_runner.py subprocess."""
    from backfill_utils import get_running_pid, check_trade_switch_off, is_backfill_enabled

    args = _parse_backfill_args(_text)
    job = args.get('job', '')

    # Resolve aliases
    if job in _BACKFILL_JOB_ALIASES:
        job = _BACKFILL_JOB_ALIASES[job]

    # No job specified: show usage
    if not job:
        enabled = is_backfill_enabled()
        gate_str = 'âœ… ENABLED' if enabled else 'ğŸ”’ DISABLED (/debug backfill_enable on í•„ìš”)'
        lines = ['ğŸ“¦ ë°±í•„ ì‹œì‘ (backfill_start)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        lines.append(f'gate: {gate_str}')
        lines.append('')
        lines.append('Usage: /debug backfill_start job=<name> [from=YYYY-MM-DD] [to=YYYY-MM-DD] [write=true]')
        lines.append('')
        lines.append('[ì‚¬ìš© ê°€ëŠ¥í•œ job]')
        for k, desc in _BACKFILL_JOBS.items():
            lines.append(f'  {k} â€” {desc}')
        lines.append('')
        lines.append('write=true ì—†ìœ¼ë©´ dryrun (ë¯¸ë¦¬ë³´ê¸°ë§Œ)')
        return '\n'.join(lines)

    # Unknown job
    if job not in _BACKFILL_JOBS:
        return f'âš  ì•Œ ìˆ˜ ì—†ëŠ” job: {job}\nì‚¬ìš© ê°€ëŠ¥: {", ".join(_BACKFILL_JOBS.keys())}'

    from_date = args.get('from', '')
    to_date = args.get('to', '')
    tf = args.get('tf', '')
    write = args.get('write', 'false').lower() == 'true'

    # Dryrun preview (default) â€” no gate checks needed for preview
    if not write:
        enabled = is_backfill_enabled()
        gate_str = 'âœ… ENABLED' if enabled else 'ğŸ”’ DISABLED'
        lines = ['ğŸ“¦ ë°±í•„ Dryrun (ë¯¸ë¦¬ë³´ê¸°)', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        lines.append(f'gate: {gate_str}')
        lines.append(f'job: {job}')
        lines.append(f'script: {_BACKFILL_JOBS[job]}')
        if from_date:
            lines.append(f'from: {from_date}')
        else:
            lines.append('from: --resume (ë§ˆì§€ë§‰ ì»¤ì„œë¶€í„°)')
        if to_date:
            lines.append(f'to: {to_date}')
        else:
            lines.append('to: now')
        if tf:
            lines.append(f'tf: {tf}')
        lines.append('')
        if not enabled:
            lines.append('âš  ì‹¤í–‰ ì „ /debug backfill_enable on í•„ìš”')
        lines.append('ì‹¤í–‰í•˜ë ¤ë©´: /debug backfill_start job={} {}{}{}write=true'.format(
            job,
            f'from={from_date} ' if from_date else '',
            f'to={to_date} ' if to_date else '',
            f'tf={tf} ' if tf else '',
        ))
        return '\n'.join(lines)

    # === write=true: all gates checked BEFORE launching ===

    # Gate 1: backfill must be enabled
    if not is_backfill_enabled():
        return ('âš  DENIED: ë°±í•„ ì‹¤í–‰ì´ ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤.\n\n'
                'í™œì„±í™”: /debug backfill_enable on\n'
                'ê·¸ ë‹¤ìŒ: /debug backfill_start job={} {}{}{}write=true'.format(
                    job,
                    f'from={from_date} ' if from_date else '',
                    f'to={to_date} ' if to_date else '',
                    f'tf={tf} ' if tf else '',
                ))

    # Gate 2: trade_switch must be OFF
    if not check_trade_switch_off():
        return 'âš  DENIED: trade_switchê°€ ON ìƒíƒœì…ë‹ˆë‹¤.\në°±í•„ì€ trade_switch OFFì¼ ë•Œë§Œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.'

    # Gate 3: concurrency check
    running_pid = get_running_pid()
    if running_pid:
        return f'âš  DENIED: ë‹¤ë¥¸ ë°±í•„ì´ ì‹¤í–‰ ì¤‘ (PID={running_pid})\në¨¼ì € /debug backfill_stop ìœ¼ë¡œ ì¢…ë£Œí•˜ì„¸ìš”.'

    # Build runner command
    cmd = ['python3', f'{APP_DIR}/backfill_runner.py', job]
    if from_date:
        cmd.extend(['--from', from_date])
    if to_date and to_date.lower() != 'now':
        cmd.extend(['--to', to_date])
    if tf:
        cmd.extend(['--tf', tf])

    # Launch background process
    log_path = f'/tmp/backfill_{job}.log'
    log_file = open(log_path, 'w')
    proc = subprocess.Popen(
        cmd,
        stdout=log_file,
        stderr=subprocess.STDOUT,
        cwd=APP_DIR,
        start_new_session=True,
    )
    # Close parent's copy of the fd â€” child process keeps its own copy
    log_file.close()

    # Wait briefly to check if runner dies immediately
    _time.sleep(1.5)
    try:
        os.kill(proc.pid, 0)
    except ProcessLookupError:
        # Runner already dead â€” read exit status
        from backfill_utils import read_exit_status
        exit_info = read_exit_status()
        if exit_info:
            return (f'âš  FAILED: runner ì¦‰ì‹œ ì¢…ë£Œë¨\n'
                    f'  status: {exit_info.get("status", "?")}\n'
                    f'  reason: {exit_info.get("reason", "?")}\n'
                    f'  ts: {exit_info.get("ts", "?")}')
        # Fallback: read log
        try:
            with open(log_path) as f:
                log_tail = f.read().strip()[-500:]
        except Exception:
            log_tail = ''
        return f'âš  FAILED: runner ì¦‰ì‹œ ì¢…ë£Œë¨ (PID={proc.pid})\nlog: {log_tail or "empty"}'
    except PermissionError:
        pass  # process exists

    lines = ['ğŸ“¦ ë°±í•„ STARTED', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
    lines.append(f'job: {job}')
    lines.append(f'runner PID: {proc.pid}')
    lines.append(f'log: {log_path}')
    lines.append(f'cmd: {" ".join(cmd)}')
    lines.append('')
    lines.append('ì œì–´ ëª…ë ¹:')
    lines.append('  /debug backfill_status â€” ì§„í–‰ ìƒí™©')
    lines.append('  /debug backfill_pause â€” ì¼ì‹œì •ì§€')
    lines.append('  /debug backfill_resume â€” ì¬ê°œ')
    lines.append('  /debug backfill_stop â€” ì•ˆì „ ì¢…ë£Œ')
    return '\n'.join(lines)


def _debug_backfill_pause(_text=None):
    """Pause running backfill."""
    from backfill_utils import get_running_pid, signal_pause, PAUSE_FILE

    pid = get_running_pid()
    if not pid:
        return 'âš  ì‹¤í–‰ ì¤‘ì¸ ë°±í•„ì´ ì—†ìŠµë‹ˆë‹¤.'

    if os.path.exists(PAUSE_FILE):
        return 'âš  ì´ë¯¸ ì¼ì‹œì •ì§€ ìƒíƒœì…ë‹ˆë‹¤. /debug backfill_resume ìœ¼ë¡œ ì¬ê°œí•˜ì„¸ìš”.'

    signal_pause()
    return f'â¸ PAUSE ì‹ í˜¸ ì „ì†¡ (PID={pid})\ní˜„ì¬ ë°°ì¹˜ ì™„ë£Œ í›„ ì¼ì‹œì •ì§€ë©ë‹ˆë‹¤.'


def _debug_backfill_resume(_text=None):
    """Resume paused backfill."""
    from backfill_utils import get_running_pid, signal_resume, PAUSE_FILE

    pid = get_running_pid()
    if not pid:
        return 'âš  ì‹¤í–‰ ì¤‘ì¸ ë°±í•„ì´ ì—†ìŠµë‹ˆë‹¤.'

    if not os.path.exists(PAUSE_FILE):
        return 'âš  ì¼ì‹œì •ì§€ ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤.'

    signal_resume()
    return f'â–¶ RESUME ì‹ í˜¸ ì „ì†¡ (PID={pid})\në°±í•„ì´ ì¬ê°œë©ë‹ˆë‹¤.'


def _debug_backfill_stop(_text=None):
    """Stop running backfill gracefully."""
    from backfill_utils import get_running_pid, signal_stop

    pid = get_running_pid()
    if not pid:
        return 'âš  ì‹¤í–‰ ì¤‘ì¸ ë°±í•„ì´ ì—†ìŠµë‹ˆë‹¤.'

    signal_stop()
    return (f'â¹ STOP ì‹ í˜¸ ì „ì†¡ (PID={pid})\n'
            f'í˜„ì¬ ë°°ì¹˜ ì»¤ë°‹ í›„ ì•ˆì „ ì¢…ë£Œë©ë‹ˆë‹¤.\n'
            f'/debug backfill_status ë¡œ ì¢…ë£Œ í™•ì¸í•˜ì„¸ìš”.')


def _debug_backfill_log(_text=None):
    """Show last lines of backfill child log."""
    from backfill_utils import get_running_pid

    # Parse optional args: job=candles_1m lines=30
    args = _parse_backfill_args(_text or '')
    job_key = args.get('job', '')
    n_lines = 30
    try:
        n_lines = int(args.get('lines', '30'))
    except ValueError:
        pass
    n_lines = min(n_lines, 100)

    # Find log file
    if job_key:
        log_path = f'/tmp/backfill_{job_key}_child.log'
        runner_log = f'/tmp/backfill_{job_key}.log'
    else:
        # Try to find most recent log
        import glob as _glob
        child_logs = sorted(_glob.glob('/tmp/backfill_*_child.log'),
                            key=lambda f: os.path.getmtime(f) if os.path.exists(f) else 0,
                            reverse=True)
        if child_logs:
            log_path = child_logs[0]
            # Extract job_key from filename
            base = os.path.basename(log_path)
            job_key = base.replace('backfill_', '').replace('_child.log', '')
            runner_log = f'/tmp/backfill_{job_key}.log'
        else:
            return 'ë¡œê·¸ íŒŒì¼ ì—†ìŒ. job= ì§€ì • ë˜ëŠ” ë°±í•„ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.'

    lines = []
    pid = get_running_pid()
    lines.append(f'ğŸ“‹ ë°±í•„ ë¡œê·¸ (job={job_key}, runner={"PID=" + str(pid) if pid else "ë¯¸ì‹¤í–‰"})')
    lines.append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')

    # Child log (main output)
    if os.path.exists(log_path):
        try:
            with open(log_path) as f:
                all_lines = f.readlines()
            total = len(all_lines)
            tail = all_lines[-n_lines:]
            lines.append(f'\n[Child log] {log_path} ({total} lines, last {len(tail)}):')
            for l in tail:
                lines.append(l.rstrip())
        except Exception as e:
            lines.append(f'Child log ì½ê¸° ì‹¤íŒ¨: {e}')
    else:
        lines.append(f'Child log ì—†ìŒ: {log_path}')

    # Runner log (brief)
    if os.path.exists(runner_log):
        try:
            with open(runner_log) as f:
                r_lines = f.readlines()
            r_tail = r_lines[-5:]
            lines.append(f'\n[Runner log] {runner_log} (last {len(r_tail)}):')
            for l in r_tail:
                lines.append(l.rstrip())
        except Exception:
            pass

    return '\n'.join(lines)


def _debug_storage(_text=None):
    """Show DB table sizes and row counts."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ’¾ DB ìŠ¤í† ë¦¬ì§€ í˜„í™©', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']

        with conn.cursor() as cur:
            # Total DB size
            cur.execute("SELECT pg_size_pretty(pg_database_size(current_database()));")
            db_size = cur.fetchone()[0]
            lines.append(f'ì´ DB í¬ê¸°: {db_size}')
            lines.append('')

            # Top tables by size
            cur.execute("""
                SELECT relname,
                       pg_size_pretty(pg_total_relation_size(relid)) AS size,
                       pg_total_relation_size(relid) AS size_bytes,
                       n_live_tup AS row_count
                FROM pg_stat_user_tables
                ORDER BY pg_total_relation_size(relid) DESC
                LIMIT 15;
            """)
            table_rows = cur.fetchall()

            if table_rows:
                lines.append(f'{"í…Œì´ë¸”":<28} {"í¬ê¸°":>10} {"í–‰ìˆ˜":>12}')
                for name, size, _, rows in table_rows:
                    lines.append(f'  {name:<26} {size:>10} {rows:>10,}')

            # Data ranges
            lines.append('')
            lines.append('[ë°ì´í„° ë²”ìœ„]')

            cur.execute("SELECT MIN(ts), MAX(ts) FROM candles WHERE tf='1m';")
            c1m = cur.fetchone()
            if c1m and c1m[0]:
                lines.append(f'  candles(1m): {str(c1m[0])[:10]} ~ {str(c1m[1])[:10]}')
            else:
                lines.append('  candles(1m): ë°ì´í„° ì—†ìŒ')

            cur.execute("SELECT MIN(ts), MAX(ts) FROM market_ohlcv WHERE tf='5m';")
            o5m = cur.fetchone()
            if o5m and o5m[0]:
                lines.append(f'  market_ohlcv(5m): {str(o5m[0])[:10]} ~ {str(o5m[1])[:10]}')
            else:
                lines.append('  market_ohlcv(5m): ë°ì´í„° ì—†ìŒ')

            # Retention policy section
            lines.append('')
            lines.append('[ë³´ì¡´ ì •ì±…]')
            lines.append('  candles(1m): 180ì¼ (prune_candles_1m.py)')
            lines.append('  pm_decision_log: 90ì¼ (cleanup_old_data)')
            lines.append('  score_history: 60ì¼ / event_trigger_log: 30ì¼ / claude_call_log: 60ì¼')

            # Count prunable 1m candles
            try:
                from datetime import timedelta
                cur.execute("""
                    SELECT COUNT(*) FROM candles
                    WHERE tf='1m' AND ts < now() - interval '180 days';
                """)
                prune_count = cur.fetchone()[0]
                lines.append(f'  -> candles(1m) í”„ë£¨ë‹ ëŒ€ìƒ: {prune_count:,}í–‰')
            except Exception:
                pass

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ìŠ¤í† ë¦¬ì§€ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_news_path_sample(_text=None):
    """Show recent news price path analysis samples."""
    import re as _re
    conn = None
    try:
        # Parse --n=N from text
        n_limit = 10
        if _text:
            m = _re.search(r'--n=(\d+)', _text)
            if m:
                n_limit = min(int(m.group(1)), 50)

        conn = _db()
        lines = ['ğŸ“Š ë‰´ìŠ¤ ê²½ë¡œ ë¶„ì„ (ìµœê·¼ {}ê±´)'.format(n_limit), 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']

        with conn.cursor() as cur:
            cur.execute("""
                SELECT npp.news_id,
                       COALESCE(n.title_ko, LEFT(n.title, 40)) AS display_title,
                       COALESCE(n.topic_class, n.tier, '-') AS topic,
                       npp.ts_news, npp.path_class,
                       npp.initial_move_dir, npp.follow_through_dir,
                       npp.recovered_flag,
                       npp.max_drawdown_24h, npp.max_runup_24h,
                       npp.end_ret_24h, npp.recovery_minutes,
                       npp.path_shape, npp.end_state_24h
                FROM news_price_path npp
                JOIN news n ON n.id = npp.news_id
                ORDER BY npp.ts_news DESC
                LIMIT %s;
            """, (n_limit,))
            rows = cur.fetchall()

        if not rows:
            lines.append('ë°ì´í„° ì—†ìŒ')
            return '\n'.join(lines)

        for i, row in enumerate(rows, 1):
            (nid, title, topic, ts, pc, imd, ftd, rec,
             dd, ru, r24, rec_min, ps, es) = row
            title_short = (title or '?')[:40]
            ts_str = str(ts)[:16] if ts else '?'
            rec_icon = 'âœ…' if rec else 'âŒ'
            pc_str = pc or ps or '?'
            lines.append(
                f'{i}) [{topic or "?"}] {title_short} ({ts_str})')
            lines.append(
                f'   path_class={pc_str} | initial={imd or "?"} '
                f'| follow={ftd or "?"} | recovered={rec_icon}')
            dd_s = f'{dd:+.1f}%' if dd is not None else '?'
            ru_s = f'{ru:+.1f}%' if ru is not None else '?'
            r24_s = f'{r24:+.1f}%' if r24 is not None else '?'
            rec_s = f'{rec_min}min' if rec_min is not None else '-'
            lines.append(
                f'   DD={dd_s} | RU={ru_s} | ret_24h={r24_s} | recovery={rec_s}')

        # Coverage stats: raw + eligible
        with conn.cursor() as cur2:
            cur2.execute("""
                SELECT
                    count(*) AS total_path,
                    count(*) FILTER (WHERE path_class IS NOT NULL) AS classified
                FROM news_price_path;
            """)
            tot_row = cur2.fetchone()
            total_c, classified_c = (tot_row or (0, 0))
            pending = total_c - classified_c

            cur2.execute("SELECT count(*) FROM news WHERE ts < now() - interval '24 hours';")
            total_news = cur2.fetchone()[0] or 0
            raw_pct = (total_c / total_news * 100) if total_news > 0 else 0

            # Eligible: news within candle coverage + 24h lookahead
            cur2.execute("""
                SELECT count(*) FROM news n
                WHERE n.ts >= (SELECT COALESCE(MIN(ts), now()) FROM candles WHERE tf='1m')
                  AND n.ts + interval '24 hours' <= (SELECT COALESCE(MAX(ts), now()) FROM candles WHERE tf='1m')
                  AND n.ts < now() - interval '24 hours';
            """)
            eligible = cur2.fetchone()[0] or 0

            # Eligible traced
            cur2.execute("""
                SELECT count(*) FROM news_price_path npp
                JOIN news n ON n.id = npp.news_id
                WHERE n.ts >= (SELECT COALESCE(MIN(ts), now()) FROM candles WHERE tf='1m')
                  AND n.ts + interval '24 hours' <= (SELECT COALESCE(MAX(ts), now()) FROM candles WHERE tf='1m');
            """)
            eligible_traced = cur2.fetchone()[0] or 0
            elig_pct = (eligible_traced / eligible * 100) if eligible > 0 else 0

            lines.append('')
            lines.append(f'[ì»¤ë²„ë¦¬ì§€]')
            lines.append(f'  raw: {total_c}/{total_news} ({raw_pct:.1f}%) | '
                         f'eligible: {eligible_traced}/{eligible} ({elig_pct:.1f}%)')
            lines.append(f'  ë¶„ë¥˜ì™„ë£Œ={classified_c} | ë¯¸ë¶„ë¥˜={pending}')

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë‰´ìŠ¤ ê²½ë¡œ ìƒ˜í”Œ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_news_path_stats(_text=None):
    """Show news price path 7-class distribution statistics."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ“Š ë‰´ìŠ¤ ê²½ë¡œ í†µê³„', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']

        with conn.cursor() as cur:
            # Overall distribution
            cur.execute("""
                SELECT path_class,
                       count(*) AS cnt,
                       round(avg(end_ret_24h)::numeric, 2) AS avg_ret,
                       round(avg(max_drawdown_24h)::numeric, 2) AS avg_dd
                FROM news_price_path
                WHERE path_class IS NOT NULL
                GROUP BY path_class
                ORDER BY cnt DESC;
            """)
            rows = cur.fetchall()

        if not rows:
            lines.append('path_class ë°ì´í„° ì—†ìŒ (--recompute ì‹¤í–‰ í•„ìš”)')
            return '\n'.join(lines)

        lines.append(f'{"path_class":<16} {"ê±´ìˆ˜":>5} {"avg_ret_24h":>12} {"avg_dd":>10}')
        for pc, cnt, avg_ret, avg_dd in rows:
            r_s = f'{avg_ret:+.2f}%' if avg_ret is not None else '?'
            d_s = f'{avg_dd:+.2f}%' if avg_dd is not None else '?'
            lines.append(f'  {pc or "?":<14} {cnt:>5} {r_s:>12} {d_s:>10}')

        # Category breakdown
        with conn.cursor() as cur:
            cur.execute("""
                SELECT n.topic_class, npp.path_class, count(*) AS cnt
                FROM news_price_path npp
                JOIN news n ON n.id = npp.news_id
                WHERE npp.path_class IS NOT NULL AND n.topic_class IS NOT NULL
                GROUP BY n.topic_class, npp.path_class
                ORDER BY n.topic_class, cnt DESC;
            """)
            cat_rows = cur.fetchall()

        if cat_rows:
            lines.append('')
            lines.append('[ì¹´í…Œê³ ë¦¬ë³„ ê²½ë¡œ ë¶„í¬]')
            # Group by topic_class
            current_topic = None
            for topic, pc, cnt in cat_rows:
                if topic != current_topic:
                    current_topic = topic
                    lines.append(f'  {topic}:')
                lines.append(f'    {pc}: {cnt}ê±´')

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë‰´ìŠ¤ ê²½ë¡œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_news_gap_diagnosis(_text=None):
    """News monthly gap diagnosis: counts, classification rate, reaction coverage."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ“° ë‰´ìŠ¤ ì›”ë³„ ê°­ ì§„ë‹¨', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']

        with conn.cursor() as cur:
            # Monthly news counts + classification status
            cur.execute("""
                SELECT to_char(ts, 'YYYY-MM') AS month,
                       count(*) AS total,
                       count(*) FILTER (WHERE tier IS NULL OR tier = 'UNKNOWN') AS unclassified,
                       count(*) FILTER (WHERE tier = 'TIERX') AS tierx,
                       count(*) FILTER (WHERE impact_score IS NULL OR impact_score = 0) AS no_impact
                FROM news
                GROUP BY month
                ORDER BY month;
            """)
            news_rows = cur.fetchall()

            # Monthly reaction coverage
            cur.execute("""
                SELECT to_char(n.ts, 'YYYY-MM') AS month,
                       count(DISTINCT nmr.news_id) AS reaction_count
                FROM news n
                LEFT JOIN news_market_reaction nmr ON nmr.news_id = n.id
                WHERE nmr.id IS NOT NULL
                GROUP BY month
                ORDER BY month;
            """)
            reaction_map = {}
            for r in cur.fetchall():
                reaction_map[r[0]] = r[1]

            # Monthly event_news_link coverage
            link_map = {}
            try:
                cur.execute("""
                    SELECT to_char(n.ts, 'YYYY-MM') AS month,
                           count(DISTINCT enl.news_id) AS linked_count
                    FROM news n
                    JOIN event_news_link enl ON enl.news_id = n.id
                    GROUP BY month
                    ORDER BY month;
                """)
                for r in cur.fetchall():
                    link_map[r[0]] = r[1]
            except Exception:
                pass  # table may not exist

        if not news_rows:
            return '\n'.join(lines + ['ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ'])

        # Header
        lines.append(f'{"ì›”":>8} {"ì „ì²´":>6} {"ë¯¸ë¶„ë¥˜":>6} {"TIERX":>6} '
                     f'{"no_imp":>6} {"ë¶„ë¥˜%":>6} {"ë°˜ì‘":>6} {"ë§í¬":>6} {"FLAG":>6}')
        lines.append('-' * 65)

        flagged_months = []
        for month, total, unclassified, tierx, no_impact in news_rows:
            classified_pct = ((total - unclassified) / total * 100) if total > 0 else 0
            reaction = reaction_map.get(month, 0)
            linked = link_map.get(month, 0)

            # Flag conditions
            flags = []
            if total < 100:
                flags.append('LOW')
            if classified_pct < 50:
                flags.append('UNCLASS')

            flag_str = ','.join(flags) if flags else '-'
            if flags:
                flagged_months.append(month)

            lines.append(
                f'{month:>8} {total:>6} {unclassified:>6} {tierx:>6} '
                f'{no_impact:>6} {classified_pct:>5.0f}% {reaction:>6} {linked:>6} {flag_str:>6}'
            )

        # Summary
        lines.append('')
        if flagged_months:
            lines.append(f'âš  ì£¼ì˜ ì›”: {", ".join(flagged_months)}')
            lines.append('  LOW = ë‰´ìŠ¤ <100ê±´ | UNCLASS = ë¶„ë¥˜ <50%')
        else:
            lines.append('ëª¨ë“  ì›” ì •ìƒ')

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë‰´ìŠ¤ ê°­ ì§„ë‹¨ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_system_stability(_text=None):
    """System stability composite scores â€” data integrity, news, backfill, execution."""
    conn = None
    try:
        conn = _db()
        from data_integrity import compute_stability_scores, format_stability_report
        from data_integrity import check_pre_live_gate, format_pre_live_gate_report
        scores = compute_stability_scores(conn)
        lines = [format_stability_report(scores)]

        # Pre-live gate status
        status, blocks, warns = check_pre_live_gate(conn)
        lines.append('')
        lines.append(format_pre_live_gate_report(status, blocks, warns))
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  system_stability ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_once_lock_status(_text=None):
    """once_lock ìƒíƒœ + TTL ì”ì—¬ì‹œê°„ í‘œì‹œ."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT symbol, opened_at, expires_at,
                       CASE WHEN expires_at IS NOT NULL
                            THEN EXTRACT(EPOCH FROM (expires_at - now())) / 60
                            ELSE NULL END AS ttl_min
                FROM live_order_once_lock
                ORDER BY opened_at DESC;
            """)
            rows = cur.fetchall()
        if not rows:
            return 'ğŸ”“ once_lock: ì—†ìŒ (empty)'
        lines = ['ğŸ”’ once_lock ìƒíƒœ', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
        for symbol, opened, expires, ttl_min in rows:
            ttl_str = f'{ttl_min:.1f}min left' if ttl_min is not None else 'no TTL'
            expired = ttl_min is not None and ttl_min <= 0
            status = '(EXPIRED)' if expired else ''
            lines.append(f'  {symbol}: opened={opened} | {ttl_str} {status}')
        lines.append(f'\ntotal: {len(rows)}')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  once_lock_status ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_once_lock_clear(_text=None):
    """ìˆ˜ë™ once_lock ì „ì²´ ì‚­ì œ."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("DELETE FROM live_order_once_lock;")
            deleted = cur.rowcount
        return f'ğŸ”“ once_lock ìˆ˜ë™ ì‚­ì œ ì™„ë£Œ: {deleted}ê±´'
    except Exception as e:
        return f'âš  once_lock_clear ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_backfill_ack(_text=None):
    """/debug backfill_ack <job_id> â€” FAILED job acknowledged ì²˜ë¦¬."""
    job_id = None
    if _text:
        import re as _re
        m = _re.search(r'(\d+)', _text)
        if m:
            job_id = int(m.group(1))
    if not job_id:
        return 'âš  ì‚¬ìš©ë²•: /debug backfill_ack <job_id>'
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("""
                UPDATE backfill_job_runs
                SET acked_at = now(), acked_by = 'operator'
                WHERE id = %s AND status = 'FAILED' AND acked_at IS NULL
                RETURNING id, job_name, started_at;
            """, (job_id,))
            row = cur.fetchone()
        if not row:
            return f'âš  job_id={job_id}: FAILED+unacked ìƒíƒœê°€ ì•„ë‹ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ'
        return f'âœ… backfill job acked: id={row[0]} name={row[1]} started={row[2]}'
    except Exception as e:
        return f'âš  backfill_ack ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _mode_performance(_text=None):
    """Strategy v2 mode performance report: per-mode (A/B/C) win rate, avg PnL, etc."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            # Check if table exists
            cur.execute("""
                SELECT EXISTS (
                    SELECT 1 FROM information_schema.tables
                    WHERE table_name = 'strategy_decision_log'
                )
            """)
            if not cur.fetchone()[0]:
                return 'ğŸ“Š Strategy v2 ëª¨ë“œ ì„±ëŠ¥\n\ní…Œì´ë¸” ì—†ìŒ â€” strategy v2 ë¯¸í™œì„±í™” ìƒíƒœ'

            # Decision counts per mode
            cur.execute("""
                SELECT mode,
                       COUNT(*) AS total_decisions,
                       COUNT(*) FILTER (WHERE action = 'ENTER') AS enters,
                       COUNT(*) FILTER (WHERE action = 'ADD') AS adds,
                       COUNT(*) FILTER (WHERE action = 'EXIT') AS exits,
                       COUNT(*) FILTER (WHERE action = 'HOLD') AS holds,
                       COUNT(*) FILTER (WHERE dedupe_hit = true) AS deduped,
                       COUNT(*) FILTER (WHERE chase_entry = true) AS chased
                FROM strategy_decision_log
                WHERE ts >= now() - interval '24 hours'
                GROUP BY mode
                ORDER BY mode
            """)
            rows = cur.fetchall()

            if not rows:
                return 'ğŸ“Š Strategy v2 ëª¨ë“œ ì„±ëŠ¥\n\nìµœê·¼ 24ì‹œê°„ ê²°ì • ì—†ìŒ'

            lines = ['ğŸ“Š Strategy v2 ëª¨ë“œ ì„±ëŠ¥ (24h)\n']
            for r in rows:
                mode, total, enters, adds, exits, holds, deduped, chased = r
                lines.append(f'MODE_{mode}: {total}ê±´')
                lines.append(f'  ENTER={enters} ADD={adds} EXIT={exits} HOLD={holds}')
                lines.append(f'  dedupe={deduped} chase_blocked={chased}')
                lines.append('')

            # PnL by mode (join with execution_log if mode column exists)
            try:
                cur.execute("""
                    SELECT mode,
                           COUNT(*) AS filled,
                           ROUND(AVG(realized_pnl)::numeric, 4) AS avg_pnl,
                           ROUND(SUM(realized_pnl)::numeric, 4) AS total_pnl,
                           COUNT(*) FILTER (WHERE realized_pnl > 0) AS wins,
                           COUNT(*) FILTER (WHERE realized_pnl <= 0) AS losses
                    FROM execution_log
                    WHERE mode IS NOT NULL
                      AND status = 'FILLED'
                      AND realized_pnl IS NOT NULL
                      AND COALESCE(last_fill_at, ts) >= now() - interval '24 hours'
                    GROUP BY mode
                    ORDER BY mode
                """)
                pnl_rows = cur.fetchall()
                if pnl_rows:
                    lines.append('â”€â”€â”€ PnL by Mode â”€â”€â”€')
                    for r in pnl_rows:
                        mode, filled, avg_pnl, total_pnl, wins, losses = r
                        wr = wins / filled * 100 if filled > 0 else 0
                        lines.append(f'MODE_{mode}: {filled}ê±´ WR={wr:.0f}% '
                                     f'avg={avg_pnl} total={total_pnl}')
            except Exception:
                pass  # mode column may not exist yet

            # Gate block stats
            try:
                cur.execute("""
                    SELECT gate_status, COUNT(*)
                    FROM strategy_decision_log
                    WHERE ts >= now() - interval '24 hours'
                    GROUP BY gate_status
                    ORDER BY gate_status
                """)
                gate_rows = cur.fetchall()
                if gate_rows:
                    lines.append('')
                    lines.append('â”€â”€â”€ Gate Status â”€â”€â”€')
                    for gs, cnt in gate_rows:
                        lines.append(f'  {gs}: {cnt}ê±´')
            except Exception:
                pass

            return '\n'.join(lines)

    except Exception as e:
        return f'ì¡°íšŒ ì‹¤íŒ¨: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _unknown(_text=None):
    return 'ì•Œ ìˆ˜ ì—†ëŠ” ì¡°íšŒ ìœ í˜•ì…ë‹ˆë‹¤. /help ì„ ì°¸ê³ í•˜ì„¸ìš”.'


def _parse_minutes_and_limit(text=None):
    '''Migrated from telegram_cmd_poller.py.'''
    t = (text or '').strip().lower()
    minutes = 1440
    limit = 20
    if 'ì˜¤ëŠ˜' in t:
        minutes = 1440
    if 'ìµœê·¼' not in t and re.search('\\d+\\s*(ë¶„|min|minute|ì‹œê°„|h|hour)', t):
        minutes = 60
    m = re.search('(\\d+)\\s*(ë¶„|min|minute)', t)
    if m:
        minutes = int(m.group(1))
    h = re.search('(\\d+)\\s*(ì‹œê°„|h|hour)', t)
    if h:
        minutes = int(h.group(1)) * 60
    n = re.search('(\\d+)\\s*(ê°œ|ê±´)', t)
    if n:
        limit = int(n.group(1))
    minutes = max(5, min(minutes, 10080))
    limit = max(1, min(limit, 50))
    return (minutes, limit)


def _macro_summary(_text=None):
    """ë§¤í¬ë¡œ/ê±°ì‹œê²½ì œ ì§€í‘œ ìµœì‹  ê°’ ì¡°íšŒ."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT DISTINCT ON (source) source, price, ts
                FROM macro_data
                ORDER BY source, ts DESC;
            """)
            rows = cur.fetchall()
        if not rows:
            return 'ğŸ“Š ë§¤í¬ë¡œ ë°ì´í„° ì—†ìŒ'
        lines = ['ğŸ“Š ê±°ì‹œê²½ì œ ì§€í‘œ í˜„í™©']
        source_kr = {
            'QQQ': 'QQQ(ë‚˜ìŠ¤ë‹¥ ì¶”ì¢…)',
            'SPY': 'SPY(S&P500)',
            'DXY': 'DXY(ë‹¬ëŸ¬ ì¸ë±ìŠ¤)',
            'US10Y': 'US10Y(ë¯¸êµ­ 10ë…„ë¬¼)',
            'VIX': 'VIX(ê³µí¬ ì§€ìˆ˜)',
        }
        for row in rows:
            src = row[0]
            price = float(row[1]) if row[1] else 0
            ts = str(row[2])[:16] if row[2] else '?'
            label = source_kr.get(src, src)
            lines.append(f'- {label}: {price:,.2f} ({ts})')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ë§¤í¬ë¡œ ì¡°íšŒ ì˜¤ë¥˜: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _db_monthly_stats(_text=None):
    """ì›”ë³„ ë°ì´í„° ì €ì¥ëŸ‰ ë¦¬í¬íŠ¸."""
    conn = None
    try:
        conn = _db()
        lines = ['ğŸ“Š DB ì›”ë³„ í†µê³„']
        lines.append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')

        # Section 1: Cross-table summary (total, 24h, date range)
        with conn.cursor() as cur:
            summary_query = """
                SELECT 'news' as tbl, COUNT(*) as total,
                       COUNT(*) FILTER (WHERE ts >= now() - interval '24 hours') as last_24h,
                       MIN(ts) as earliest, MAX(ts) as latest
                FROM news
                UNION ALL
                SELECT 'candles', COUNT(*),
                       COUNT(*) FILTER (WHERE ts >= now() - interval '24 hours'),
                       MIN(ts), MAX(ts)
                FROM candles
                UNION ALL
                SELECT 'macro_data', COUNT(*),
                       COUNT(*) FILTER (WHERE ts >= now() - interval '24 hours'),
                       MIN(ts), MAX(ts)
                FROM macro_data
                UNION ALL
                SELECT 'execution_log', COUNT(*),
                       COUNT(*) FILTER (WHERE ts >= now() - interval '24 hours'),
                       MIN(ts), MAX(ts)
                FROM execution_log
                UNION ALL
                SELECT 'score_history', COUNT(*),
                       COUNT(*) FILTER (WHERE ts >= now() - interval '24 hours'),
                       MIN(ts), MAX(ts)
                FROM score_history;
            """
            try:
                cur.execute(summary_query)
                rows = cur.fetchall()
                lines.append('\n[í…Œì´ë¸” ìš”ì•½]')
                for row in rows:
                    tbl, total, last_24h, earliest, latest = row
                    earliest_str = str(earliest)[:16] if earliest else '-'
                    latest_str = str(latest)[:16] if latest else '-'
                    lines.append(f'  {tbl}: {total:,}ê±´ (24h: {last_24h:,}ê±´)')
                    lines.append(f'    {earliest_str} ~ {latest_str}')
            except Exception as e:
                lines.append(f'\n[í…Œì´ë¸” ìš”ì•½] ì¡°íšŒ ì‹¤íŒ¨: {e}')

            # macro_events summary (new table)
            try:
                cur.execute("""
                    SELECT COUNT(*),
                           MIN(event_date)::text,
                           MAX(event_date)::text
                    FROM macro_events;
                """)
                row = cur.fetchone()
                if row and row[0]:
                    lines.append(f'  macro_events: {row[0]:,}ê±´')
                    lines.append(f'    {(row[1] or "-")} ~ {(row[2] or "-")}')
                else:
                    lines.append('  macro_events: 0ê±´')
            except Exception:
                lines.append('  macro_events: í…Œì´ë¸” ë¯¸ìƒì„±')

        # Section 2: Monthly breakdown per table
        tables = [
            ('candles', 'ts'),
            ('news', 'ts'),
            ('indicators', 'ts'),
            ('events', 'start_ts'),
            ('pm_decision_log', 'ts'),
            ('macro_data', 'ts'),
            ('score_history', 'ts'),
            ('macro_events', 'event_date'),
        ]
        lines.append('\n[ì›”ë³„ ë°ì´í„°ëŸ‰]')
        with conn.cursor() as cur:
            for table, ts_col in tables:
                try:
                    cur.execute(f"""
                        SELECT date_trunc('month', {ts_col}) AS month,
                               count(*) AS cnt
                        FROM {table}
                        GROUP BY month
                        ORDER BY month DESC
                        LIMIT 6;
                    """)
                    rows = cur.fetchall()
                    lines.append(f'\n  [{table}]')
                    if not rows:
                        lines.append('    ë°ì´í„° ì—†ìŒ')
                    else:
                        for row in rows:
                            month_str = str(row[0])[:7] if row[0] else '?'
                            lines.append(f'    {month_str}: {row[1]:,}ê±´')
                except Exception:
                    lines.append(f'\n  [{table}] ì¡°íšŒ ì‹¤íŒ¨')
        return '\n'.join(lines)
    except Exception as e:
        return f'âš  ì›”ë³„ í†µê³„ ì˜¤ë¥˜: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _debug_order_throttle(_text=None):
    """ì£¼ë¬¸ ì†ë„ ì œí•œ ìƒíƒœ + 60ë¶„ íƒ€ì„ë¼ì¸."""
    import order_throttle
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            status = order_throttle.get_throttle_status(cur)
            lines = ['ğŸš¦ Order Throttle Guard', 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”']
            # Rate limits
            lines.append(f'ğŸ“Š Rate Limits')
            lines.append(f'  1h: {status["hourly_count"]}/{status["hourly_limit"]}')
            lines.append(f'  10m: {status["10min_count"]}/{status["10min_limit"]}')
            # Entry lock
            if status.get('entry_locked'):
                lines.append(f'ğŸ”’ ENTRY LOCKED: {status["lock_reason"]}')
                lines.append(f'  expires: {status["lock_expires_str"]}')
            else:
                lines.append('ğŸ”“ Entry: UNLOCKED')
            # Cooldowns
            lines.append(f'\nâ± Cooldowns')
            for action, remaining in status.get('cooldowns', {}).items():
                icon = 'â³' if remaining > 0 else 'âœ…'
                lines.append(f'  {icon} {action}: {remaining:.0f}s' if remaining > 0 else f'  {icon} {action}: ready')
            # Last reject
            if status.get('last_reject'):
                lines.append(f'\nâŒ Last Reject')
                lines.append(f'  {status["last_reject"][:100]}')
                lines.append(f'  at: {status.get("last_reject_ts_str", "?")}')
            # Backoff state
            if status.get('network_consecutive', 0) > 0:
                lines.append(f'ğŸŒ Network errors: {status["network_consecutive"]} consecutive')
            if status.get('db_error_consecutive', 0) > 0:
                lines.append(f'ğŸ’¾ DB errors: {status["db_error_consecutive"]} consecutive')
            # 60-min timeline from DB
            cur.execute("""
                SELECT date_trunc('minute', ts) AS m, count(*),
                       count(*) FILTER (WHERE outcome='SUCCESS'),
                       count(*) FILTER (WHERE outcome IN ('REJECTED','ERROR','BLOCKED'))
                FROM order_attempt_log
                WHERE symbol='BTC/USDT:USDT' AND ts >= now()-interval '60 minutes'
                GROUP BY m ORDER BY m;
            """)
            rows = cur.fetchall()
            if rows:
                lines.append(f'\nğŸ“ˆ 60-min Timeline ({len(rows)} active minutes)')
                for m, cnt, ok, fail in rows[-20:]:
                    bar = 'â–ˆ' * min(ok, 10) + 'â–‘' * min(fail, 10)
                    lines.append(f'  {m.strftime("%H:%M")} {bar} ({ok}/{fail})')
            return '\n'.join(lines)
    except Exception as e:
        return f'âš  order_throttle error: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _reconcile(_text=None):
    """Compare exchange position vs strategy DB state and report mismatches."""
    try:
        exch = exchange_reader.fetch_position(SYMBOL)
        strat = exchange_reader.fetch_position_strat(SYMBOL)
        orders = exchange_reader.fetch_open_orders(SYMBOL)

        exch_pos = exch.get('exchange_position', 'UNKNOWN')
        exch_qty = exch.get('exch_pos_qty', 0)
        exch_status = exch.get('data_status', 'ERROR')

        strat_side = (strat.get('strat_side') or '').upper()
        strat_state = strat.get('strat_state', 'UNKNOWN')
        strat_qty = float(strat.get('strat_qty') or 0)
        order_state = strat.get('order_state', '')

        open_count = len(orders.get('orders', []))

        lines = ['[RECONCILE] ê±°ë˜ì†Œ vs ì „ëµDB ëŒ€ì¡°']
        lines.append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')

        # Section 1: Exchange
        lines.append(f'\n[ê±°ë˜ì†Œ(Bybit)]')
        if exch_status != 'OK':
            lines.append(f'  ìƒíƒœ: ERROR â€” {exch.get("error", "API í˜¸ì¶œ ì‹¤íŒ¨")}')
        else:
            lines.append(f'  í¬ì§€ì…˜: {exch_pos}')
            lines.append(f'  ìˆ˜ëŸ‰: {exch_qty}')
            if exch_pos != 'NONE':
                lines.append(f'  ì§„ì…ê°€: {exch.get("exch_entry_price", 0)}')
                lines.append(f'  ë¯¸ì‹¤í˜„PnL: {exch.get("upnl", 0):.2f} USDT')
        lines.append(f'  ë¯¸ì²´ê²°ì£¼ë¬¸: {open_count}ê±´')

        # Section 2: Strategy DB
        lines.append(f'\n[ì „ëµDB]')
        lines.append(f'  ìƒíƒœ: {strat_state}')
        lines.append(f'  ë°©í–¥: {strat_side or "NONE"}')
        lines.append(f'  ìˆ˜ëŸ‰: {strat_qty}')
        if order_state:
            lines.append(f'  order_state: {order_state}')

        # Section 3: Comparison
        lines.append(f'\n[ëŒ€ì¡° ê²°ê³¼]')

        exch_dir = exch_pos if exch_pos != 'NONE' else 'NONE'
        strat_dir = strat_side if strat_side else 'NONE'

        if exch_status != 'OK':
            verdict = 'UNKNOWN â€” ê±°ë˜ì†Œ API ì˜¤ë¥˜ë¡œ ë¹„êµ ë¶ˆê°€'
        elif exch_dir == 'NONE' and strat_dir == 'NONE':
            verdict = 'MATCH â€” ì–‘ìª½ ëª¨ë‘ í¬ì§€ì…˜ ì—†ìŒ'
        elif exch_dir == 'NONE' and strat_dir != 'NONE':
            if order_state in ('SENT', 'PENDING', 'ACKED'):
                verdict = f'PENDING â€” ì „ëµDB {strat_dir} ì˜ë„, ì£¼ë¬¸ ë¯¸ì²´ê²° ëŒ€ê¸° ì¤‘ (order_state={order_state})'
            else:
                verdict = f'MISMATCH â€” ê±°ë˜ì†Œ NONE, ì „ëµDB {strat_dir} ({strat_state})'
        elif exch_dir != 'NONE' and strat_dir == 'NONE':
            verdict = f'MISMATCH â€” ê±°ë˜ì†Œ {exch_dir}, ì „ëµDB NONE'
        elif exch_dir == strat_dir:
            verdict = f'MATCH â€” ì–‘ìª½ ëª¨ë‘ {exch_dir}'
        else:
            verdict = f'MISMATCH â€” ê±°ë˜ì†Œ {exch_dir}, ì „ëµDB {strat_dir}'

        lines.append(f'  {verdict}')

        if 'MISMATCH' in verdict:
            lines.append(f'\nâš  ë¶ˆì¼ì¹˜ ê°ì§€ â€” exchange_reader ìë™ ë³µêµ¬ ëŒ€ê¸° ì¤‘')
            lines.append(f'  ìˆ˜ë™ ë³µêµ¬: /debug gate_details ì—ì„œ ìƒíƒœ í™•ì¸')

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  reconcile ì˜¤ë¥˜: {e}'


def _mctx_status(_text=None):
    """MCTX status: regime, features, vol_pct, spread_ok, liquidity_ok, drift."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            import regime_reader
            ctx = regime_reader.get_current_regime(cur)

            from strategy.common.features import build_feature_snapshot
            features = build_feature_snapshot(cur, SYMBOL)

        if not ctx.get('available'):
            return '[MCTX] ë°ì´í„° ì—†ìŒ (FAIL-OPEN: UNKNOWN ëª¨ë“œ)'

        import mctx_formatter
        return mctx_formatter.format_mctx(features, ctx)
    except Exception as e:
        return f'MCTX error: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass


def _mode_params(_text=None):
    """Current regime mode parameters: TP/SL/leverage/stage/entry filter."""
    conn = None
    try:
        conn = _db()
        with conn.cursor() as cur:
            import regime_reader
            ctx = regime_reader.get_current_regime(cur)

        regime = ctx.get('regime', 'UNKNOWN')
        shock_type = ctx.get('shock_type')
        params = regime_reader.get_regime_params(regime, shock_type)

        lines = [f'[MODE] {regime} ëª¨ë“œ íŒŒë¼ë¯¸í„°']
        lines.append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')
        tp_mode = params.get('tp_mode', 'fixed')
        if tp_mode == 'fixed':
            lines.append(f'  TP: fixed {params.get("tp_pct_min", 0)}-{params.get("tp_pct_max", 0)}%')
        elif tp_mode == 'trailing':
            lines.append(f'  TP: trailing (activate={params.get("trail_activate_pct", 0)}%, '
                         f'trail={params.get("trail_pct", 0)}%)')
        lines.append(f'  SL: {params.get("sl_pct", 2.0)}%')
        lines.append(f'  ë ˆë²„ë¦¬ì§€: {params.get("leverage_min", 3)}-{params.get("leverage_max", 8)}x')
        lines.append(f'  ìµœëŒ€ ìŠ¤í…Œì´ì§€: {params.get("stage_max", 7)}')
        lines.append(f'  ì§„ì… í•„í„°: {params.get("entry_filter", "none")}')
        lines.append(f'  ADD ì ìˆ˜ ê¸°ì¤€: {params.get("add_score_threshold", 45)}')

        if not ctx.get('available'):
            lines.append('\n  â€» MCTX ë¯¸ê°€ìš© â€” UNKNOWN(FAIL-OPEN) ì ìš© ì¤‘')

        return '\n'.join(lines)
    except Exception as e:
        return f'âš  MODE ì˜¤ë¥˜: {e}'
    finally:
        if conn:
            try:
                conn.close()
            except Exception:
                pass
