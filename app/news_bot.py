import os, time, json, traceback, sys, re, html
from collections import deque
import feedparser
import psycopg2
from openai import OpenAI
from db_config import get_conn
import news_classifier_config as _ncc
NEWS_POLL_SEC = int(os.getenv("NEWS_POLL_SEC", "300"))
FEED_AGENT = os.getenv("NEWS_FEED_AGENT", "Mozilla/5.0 trading-bot-news/1.0")

# Circuit breaker constants
MAX_CONSECUTIVE_DB_ERRORS = 10
BACKOFF_BASE_SEC = 5
MAX_BACKOFF_SEC = 120

FEEDS = [
    # ÏïîÌò∏ÌôîÌèê
    ("coindesk", "https://www.coindesk.com/arc/outboundfeeds/rss/"),
    ("cointelegraph", "https://cointelegraph.com/rss"),
    # ÎØ∏Íµ≠/Í±∞Ïãú/ÎπÑÏ¶àÎãàÏä§
    ("bloomberg", "https://feeds.bloomberg.com/markets/news.rss"),
    ("bbc_business", "http://feeds.bbci.co.uk/news/business/rss.xml"),
    ("bbc_world", "http://feeds.bbci.co.uk/news/world/rss.xml"),
    # ÎØ∏Íµ≠ Ï£ºÏãù/Í≤ΩÏ†ú (Ïã†Í∑ú)
    ("cnbc", "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10001147"),
    ("yahoo_finance", "https://finance.yahoo.com/news/rssconf"),
    ("investing", "https://www.investing.com/rss/news.rss"),
    ("marketwatch", "https://feeds.content.dowjones.io/public/rss/mw_topstories"),
]

KEYWORDS = [
    # crypto
    "bitcoin","btc","crypto","etf","sec",
    # fed/rates
    "fed","fomc","cpi","inflation","rate","powell","nfp","pce","unemployment","jobs report",
    # treasury/dollar
    "treasury","bond","dollar","yields","us10y","dxy",
    # equities
    "nasdaq","qqq","spx","sp500","equit","risk-off","risk off",
    # politics
    "election","trump","tariff","white house","congress","gop",
    # geopolitics
    "war","missile","sanction","china","boj",
    # crypto-specific
    "hack","exploit","liquidation","bank","regulation",
    # P1: US politics / macro expansion
    "presidential","midterm","executive order","debt ceiling","government shutdown",
    "indictment","impeachment","special counsel",
    "goldman sachs","jpmorgan","morgan stanley",
    "semiconductor","chip ban","ai regulation",
    "immigration","deportation","border",
    "treasury yield","credit spread","bond auction","term premium","tlt",
]

# ‚îÄ‚îÄ Í∞ÄÏã≠/ÎÖ∏Ïù¥Ï¶à ÌïòÎìú ÌïÑÌÑ∞ ‚îÄ‚îÄ
GOSSIP_BLOCKLIST = {
    'celebrity', 'divorce', 'lifestyle', 'entertainment', 'sports', 'fashion',
    'recipe', 'dating', 'top 10', '5 things', 'quiz', 'roundup', 'picks',
    'best movies', 'tv show', 'oscars', 'grammy', 'wedding', 'vacation',
    'horoscope', 'zodiac', 'beauty', 'fitness', 'weight loss', 'diet',
    'real estate tips', 'home decor', 'travel guide', 'pet', 'puppy',
    'tiktok', 'instagram', 'influencer', 'viral video', 'meme',
}

LOW_VALUE_SOURCES = {'yahoo_finance'}  # insertÏóê impact_score >= 6 Ï°∞Í±¥ Ï†ÅÏö©


# ‚îÄ‚îÄ Word-boundary keyword matching ‚îÄ‚îÄ
# Short keywords (‚â§4 chars) that need \b to avoid substring false positives
_SHORT_KW_THRESHOLD = 4

def _build_kw_regex(keywords):
    """Build a single compiled regex from a keyword set/list for word-boundary matching.
    ASCII keywords use \\b word boundaries to prevent substring false positives.
    Korean/CJK keywords use simple alternation (\\b doesn't work with CJK).
    """
    parts = []
    for kw in sorted(keywords, key=len, reverse=True):  # longer first for greedy match
        if kw.isascii():
            parts.append(r'\b' + re.escape(kw) + r'\b')
        else:
            parts.append(re.escape(kw))
    return re.compile('|'.join(parts), re.IGNORECASE) if parts else re.compile(r'(?!)')

def _kw_match(text, compiled_re):
    """Return True if any keyword matches in text."""
    return bool(compiled_re.search(text))

# Cache for active_keywords regex (avoids recompiling 180 times per tick)
_active_kw_cache = (None, None)  # (tuple_key, compiled_regex)

def _get_active_kw_regex(active_keywords):
    """Return cached compiled regex for active_keywords list."""
    global _active_kw_cache
    key = tuple(sorted(active_keywords))
    if _active_kw_cache[0] == key:
        return _active_kw_cache[1]
    compiled = _build_kw_regex(active_keywords)
    _active_kw_cache = (key, compiled)
    return compiled

# ‚îÄ‚îÄ ÏÜåÏä§ Ìã∞Ïñ¥ Î∂ÑÎ•ò (v2: bbc/investing ‚Üí TIER2Î°ú ÏäπÍ≤©) ‚îÄ‚îÄ
SOURCE_TIERS = {
    'TIER1_SOURCE': {'reuters', 'bloomberg', 'wsj', 'ft', 'ap'},
    'TIER2_SOURCE': {'cnbc', 'coindesk', 'marketwatch', 'cointelegraph',
                     'bbc_business', 'bbc_world', 'investing'},
    'REFERENCE_ONLY': {'yahoo_finance'},
}

CRYPTO_FEEDS = {'coindesk', 'cointelegraph'}

# ‚îÄ‚îÄ Shadow ‚Üí GPT category normalization ‚îÄ‚îÄ
# Shadow classifier enum names differ from GPT enum; normalize to GPT names
# so topic_class column consistently uses the GPT enum (which the scorer reads).
_SHADOW_TO_GPT_CATEGORY = {
    'FED_FOMC': 'FED_RATES',
    'MACRO_INDICATORS': 'CPI_JOBS',
    'ETF_FLOWS': 'CRYPTO_SPECIFIC',
    'SEC_REGULATION': 'REGULATION_SEC_ETF',
    'EQUITY_MOVES': 'NASDAQ_EQUITIES',
    'INSTITUTIONAL_BTC': 'CRYPTO_SPECIFIC',
    'GEOPOLITICAL': 'WAR',
    'US_FISCAL_DEBT': 'US_FISCAL',
}


def _get_source_tier(source: str) -> str:
    """ÏÜåÏä§Î™Ö ‚Üí Ìã∞Ïñ¥ Î∞òÌôò. ÎØ∏Îì±Î°ù ‚Üí REFERENCE_ONLY."""
    s = (source or '').lower().strip()
    for tier, sources in SOURCE_TIERS.items():
        if s in sources:
            return tier
    return 'REFERENCE_ONLY'


# ‚îÄ‚îÄ ÌïòÎìú Ï†úÏô∏ Ìå®ÌÑ¥ (Ï†ïÍ∑úÏãù) ‚îÄ‚îÄ
HARD_EXCLUDE_PATTERNS = [
    r'is\s+\w+\s+(stock\s+)?a\s+good\s+buy',
    r'\d+\s+(best|top)\s+(stocks?|picks?|buys?)',
    r'(should\s+you|is\s+it\s+time\s+to)\s+(buy|sell|invest)',
    r'stock(s)?\s+(to|you\s+should)\s+(buy|sell|watch)',
    r'(my|his|her|their)\s+(husband|wife|journey|story|experience|portfolio)',
    r'how\s+i\s+(made|lost|earned|invest)',
    r'(personal\s+finance|retirement\s+plan|mortgage|student\s+loan)',
    r'^\d+\s+(things|ways|tips|reasons|steps)',
]
_HARD_EXCLUDE_RE = [re.compile(p, re.IGNORECASE) for p in HARD_EXCLUDE_PATTERNS]


def _is_hard_excluded(title: str) -> bool:
    """ÌïòÎìú Ï†úÏô∏ Ìå®ÌÑ¥ Îß§Ïπò Ïãú True."""
    t = (title or '').strip()
    return any(pat.search(t) for pat in _HARD_EXCLUDE_RE)


# ‚îÄ‚îÄ AND-Í∏∞Î∞ò ÌÇ§ÏõåÎìú Îß§Ïπ≠ ‚îÄ‚îÄ
CRYPTO_CORE_KEYWORDS = {
    'bitcoin', 'btc', 'crypto', 'ethereum', 'blockchain',
    'defi', 'stablecoin', 'halving', 'mining',
}
IMPACT_KEYWORDS = {
    'fed', 'fomc', 'cpi', 'inflation', 'rate', 'powell', 'nfp', 'pce',
    'treasury', 'bond', 'yields', 'dxy',
    'nasdaq', 'qqq', 'sp500', 'risk-off', 'risk off', 'risk-on', 'risk on',
    'war', 'missile', 'sanction', 'tariff',
    'sec', 'etf', 'regulation', 'ban',
    'liquidation', 'hack', 'exploit',
    # macro indicators
    'pmi', 'ism', 'gdp', 'retail sales', 'jobless',
    'credit spread', 'yield curve', 'us10y', 'earnings',
    # central banks
    'boj', 'ecb', 'rate decision', 'rate cut', 'rate hike',
    # P1: US politics / macro expansion
    'election', 'presidential', 'impeachment', 'indictment',
    'debt ceiling', 'government shutdown',
    'immigration', 'deportation',
    'semiconductor', 'credit spread', 'bond auction', 'term premium',
}
MACRO_STANDALONE_KEYWORDS = {
    # existing
    'fed', 'fomc', 'cpi', 'nfp', 'ppi', 'pce', 'powell',
    'war', 'missile', 'tariff', 'sanction',
    # macro indicators
    'dxy', 'pmi', 'ism', 'gdp', 'retail sales', 'jobless claims',
    'consumer confidence', 'consumer sentiment',
    # treasury/credit
    'treasury yields', 'yield curve', 'credit spread', 'us10y',
    # central banks
    'boj', 'ecb', 'rate decision', 'rate cut', 'rate hike',
    # geopolitical
    'nato', 'invasion', 'embargo',
    # mega-cap earnings (high BTC correlation)
    'earnings',
    # P1: US politics / macro expansion ‚Äî pass worth_llm() without crypto keywords
    'election', 'presidential', 'midterm', 'executive order',
    'debt ceiling', 'government shutdown', 'shutdown', 'fiscal cliff',
    'indictment', 'impeachment', 'special counsel',
    'goldman sachs', 'jpmorgan', 'morgan stanley',
    'immigration', 'deportation',
    'semiconductor', 'chip ban', 'ai regulation',
    'treasury yield', 'credit spread', 'bond auction', 'term premium', 'tlt',
}

# Pre-compiled regexes for word-boundary safe matching
_RE_KEYWORDS = _build_kw_regex(KEYWORDS)
_RE_CRYPTO_CORE = _build_kw_regex(CRYPTO_CORE_KEYWORDS)
_RE_IMPACT = _build_kw_regex(IMPACT_KEYWORDS)
_RE_MACRO_STANDALONE = _build_kw_regex(MACRO_STANDALONE_KEYWORDS)
_RE_GOSSIP = _build_kw_regex(GOSSIP_BLOCKLIST)


def _is_gossip(title: str) -> bool:
    """Í∞ÄÏã≠/ÎÖ∏Ïù¥Ï¶à ÌÇ§ÏõåÎìú Îß§Ïπ≠ Ïãú True ‚Üí GPT Ìò∏Ï∂ú ÏóÜÏù¥ Ïä§ÌÇµ."""
    t = (title or '').lower()
    return _kw_match(t, _RE_GOSSIP)

# ‚îÄ‚îÄ DB ÏóêÎü¨ ÏïåÎ¶º ‚îÄ‚îÄ
_error_alert_cache = {}  # {dedup_key: (last_ts, count)}
_ERROR_ALERT_COOLDOWN_SEC = 1800  # 30Î∂Ñ
_ERROR_CACHE_MAX_SIZE = 200  # max entries before pruning

# ‚îÄ‚îÄ Alert threshold (v2) ‚îÄ‚îÄ rolling 10min error windows per error_type+source
_ALERT_THRESHOLD_COUNT = 5
_ALERT_WINDOW_SEC = 600  # 10 minutes
_error_windows = {}  # {type:source -> deque of timestamps}
_alert_sent_windows = {}  # {type:source -> last_alert_ts}

# ‚îÄ‚îÄ Stats file for /debug news_health ‚îÄ‚îÄ
_STATS_FILE = '/tmp/news_bot_stats.json'
_stats = {
    'last_fetch_ok_ts': '',
    'last_insert_ok_ts': '',
    'parse_fail_10m': 0,
    'fetch_fail_10m': 0,
    'duplicate_ignored_10m': 0,
    'insert_ok_10m': 0,
    'last_error_type': '',
    'last_error_source': '',
    'last_error_msg': '',
    'last_error_ts': '',
    'tick_ts': '',
}
_event_log = deque(maxlen=2000)  # (ts, event_type) for 10-min windowing

# Telegram config cache (avoid disk read on every alert)
_tg_config_cache = None

def _load_tg_config():
    """Load Telegram token/chat_id from env file (cached)."""
    global _tg_config_cache
    if _tg_config_cache is not None:
        return _tg_config_cache
    token, chat_id = '', ''
    try:
        with open('/root/trading-bot/app/telegram_cmd.env', 'r') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#') or '=' not in line:
                    continue
                k, v = line.split('=', 1)
                if k.strip() == 'TELEGRAM_BOT_TOKEN':
                    token = v.strip()
                elif k.strip() == 'TELEGRAM_ALLOWED_CHAT_ID':
                    chat_id = v.strip()
    except Exception:
        pass
    _tg_config_cache = (token, chat_id)
    return _tg_config_cache

def _prune_error_cache():
    """Remove expired entries from _error_alert_cache to prevent unbounded growth."""
    if len(_error_alert_cache) <= _ERROR_CACHE_MAX_SIZE:
        return
    now = time.time()
    expired = [k for k, (ts, _) in _error_alert_cache.items()
               if now - ts > _ERROR_ALERT_COOLDOWN_SEC * 2]
    for k in expired:
        del _error_alert_cache[k]

def _send_error_alert(source, title, exception, dedup_key=None):
    """DB ÏóêÎü¨ Î∞úÏÉù Ïãú Telegram ÏïåÎ¶º. 30Î∂Ñ Ïø®Îã§Ïö¥ÏúºÎ°ú Ïä§Ìå∏ Î∞©ÏßÄ."""
    import urllib.parse, urllib.request
    now = time.time()
    if dedup_key is None:
        dedup_key = f"{type(exception).__name__}:{source}"

    # Prune old entries periodically
    _prune_error_cache()

    cached = _error_alert_cache.get(dedup_key)
    if cached:
        last_ts, suppressed = cached
        if now - last_ts < _ERROR_ALERT_COOLDOWN_SEC:
            _error_alert_cache[dedup_key] = (last_ts, suppressed + 1)
            return  # Ïä§Ìå∏ Î∞©ÏßÄ

    # Cooldown expired or first alert: report suppressed count + 1 (current)
    count = (cached[1] + 1) if cached else 1
    _error_alert_cache[dedup_key] = (now, 0)  # reset suppressed count for next window

    err_type = type(exception).__name__
    err_msg = str(exception)[:200]
    text = (
        f"[news_bot] DB Ïò§Î•ò\n"
        f"- ÏÜåÏä§: {source}\n"
        f"- Ï†úÎ™©: {(title or '')[:60]}\n"
        f"- Ïò§Î•ò: {err_type}: {err_msg}\n"
        f"- Ïó∞ÏÜç: {count}Ìöå"
    )

    try:
        token, chat_id = _load_tg_config()
        if token and chat_id:
            url = f'https://api.telegram.org/bot{token}/sendMessage'
            data = urllib.parse.urlencode({'chat_id': chat_id, 'text': text}).encode('utf-8')
            req = urllib.request.Request(url, data=data, method='POST')
            urllib.request.urlopen(req, timeout=10)
    except Exception:
        pass


def _record_event(event_type):
    """Record a timestamped event for 10-min windowed stats."""
    _event_log.append((time.time(), event_type))


def _count_events_10m(event_type):
    """Count events of given type in last 10 minutes."""
    cutoff = time.time() - 600
    return sum(1 for ts, et in _event_log if ts >= cutoff and et == event_type)


def _send_threshold_alert(error_type, source, count, sample_msg=''):
    """Send aggregated alert when threshold reached (v2 alert logic)."""
    key = f'{error_type}:{source}'
    now = time.time()
    last_sent = _alert_sent_windows.get(key, 0)
    if now - last_sent < _ALERT_WINDOW_SEC:
        return  # already sent in this window
    _alert_sent_windows[key] = now

    import urllib.parse, urllib.request
    text = (
        f"[news_bot] ÏóêÎü¨ ÏßëÏ§ë Î∞úÏÉù\n"
        f"- Ïú†Ìòï: {error_type}\n"
        f"- ÏÜåÏä§: {source}\n"
        f"- 10Î∂ÑÎÇ¥ ÌöüÏàò: {count}\n"
        f"- ÏÉòÌîå: {sample_msg[:100]}"
    )
    try:
        token, chat_id = _load_tg_config()
        if token and chat_id:
            url = f'https://api.telegram.org/bot{token}/sendMessage'
            data = urllib.parse.urlencode({'chat_id': chat_id, 'text': text}).encode('utf-8')
            req = urllib.request.Request(url, data=data, method='POST')
            urllib.request.urlopen(req, timeout=10)
    except Exception:
        pass


_WINDOW_DICT_MAX_KEYS = 100  # max distinct error_type:source keys before pruning

def _prune_window_dicts():
    """Remove stale entries from _error_windows and _alert_sent_windows to prevent unbounded growth."""
    now = time.time()
    cutoff = now - _ALERT_WINDOW_SEC
    # Prune _error_windows: remove keys with no recent entries
    stale = [k for k, dq in _error_windows.items() if not dq or dq[-1] < cutoff]
    for k in stale:
        del _error_windows[k]
    # Prune _alert_sent_windows: remove expired entries
    stale_alerts = [k for k, ts in _alert_sent_windows.items() if now - ts > _ALERT_WINDOW_SEC]
    for k in stale_alerts:
        del _alert_sent_windows[k]


def _check_error_threshold(error_type, source, msg=''):
    """Track error and fire alert if threshold reached in 10-min window.
    Gated by ff_news_alert_throttle_v2; fallback to old 30-min cooldown if disabled.
    """
    _ff_v2 = True
    try:
        import feature_flags
        _ff_v2 = feature_flags.is_enabled('ff_news_alert_throttle_v2')
    except Exception:
        pass
    if not _ff_v2:
        # Fallback to legacy: single-alert via _send_error_alert
        _send_error_alert(source, '', Exception(f'{error_type}: {msg}'),
                          dedup_key=f'{error_type}:{source}')
        return
    key = f'{error_type}:{source}'
    now = time.time()
    # Periodic pruning when dicts grow too large
    if len(_error_windows) > _WINDOW_DICT_MAX_KEYS:
        _prune_window_dicts()
    if key not in _error_windows:
        _error_windows[key] = deque(maxlen=200)
    _error_windows[key].append(now)
    # Prune old entries from this key's deque
    cutoff = now - _ALERT_WINDOW_SEC
    while _error_windows[key] and _error_windows[key][0] < cutoff:
        _error_windows[key].popleft()
    count = len(_error_windows[key])
    if count >= _ALERT_THRESHOLD_COUNT:
        _send_threshold_alert(error_type, source, count, msg)


def _record_raw_error(db, source, error_type, raw_title='', raw_url='', exception_msg=''):
    """Insert a row into news_raw_errors for parse/fetch failure tracking."""
    try:
        with db.cursor() as cur:
            cur.execute("""
                INSERT INTO news_raw_errors (source, error_type, raw_title, raw_url, exception_msg)
                VALUES (%s, %s, %s, %s, %s);
            """, (source, error_type, (raw_title or '')[:500],
                  (raw_url or '')[:500], (exception_msg or '')[:500]))
    except Exception:
        pass  # never block main flow


def _flush_stats():
    """Write stats to JSON file for /debug news_health to read."""
    from datetime import datetime, timezone
    _stats['tick_ts'] = datetime.now(timezone.utc).isoformat()
    _stats['parse_fail_10m'] = _count_events_10m('parse_fail')
    _stats['fetch_fail_10m'] = _count_events_10m('fetch_fail')
    _stats['duplicate_ignored_10m'] = _count_events_10m('duplicate_ignored')
    _stats['insert_ok_10m'] = _count_events_10m('insert_ok')
    try:
        with open(_STATS_FILE, 'w') as f:
            json.dump(_stats, f, default=str)
    except Exception:
        pass


def _sanitize_title(raw_title):
    """Strip, HTML unescape, whitespace collapse."""
    if not raw_title:
        return ''
    t = html.unescape(raw_title).strip()
    t = re.sub(r'\s+', ' ', t)
    return t


def log(msg):
    print(msg, flush=True)

def _load_watch_keywords(db):
    """Load keywords from openclaw_policies. Falls back to hardcoded KEYWORDS."""
    try:
        with db.cursor() as cur:
            cur.execute("""
                SELECT value FROM openclaw_policies WHERE key = 'watch_keywords';
            """)
            row = cur.fetchone()
            if row and row[0]:
                custom = row[0] if isinstance(row[0], list) else json.loads(row[0])
                if isinstance(custom, list) and len(custom) > 0:
                    return custom
    except Exception:
        pass
    return list(KEYWORDS)


def worth_llm(title: str, active_keywords=None, source: str = '') -> bool:
    """AND-Í∏∞Î∞ò ÌÇ§ÏõåÎìú Îß§Ïπ≠. crypto+impact ÎèôÏãú Ï∂©Ï°± ÎòêÎäî macro standalone."""
    t = (title or "").lower()
    # ÌÅ¨Î¶ΩÌÜ† Ï†ÑÏö© ÌîºÎìúÎäî Î∞îÏù¥Ìå®Ïä§
    if (source or '').lower() in CRYPTO_FEEDS:
        return True
    # Îß§ÌÅ¨Î°ú Ïä§ÌÉ†ÎìúÏñºÎ°†: Îã®ÎèÖ ÌÜµÍ≥º
    if _kw_match(t, _RE_MACRO_STANDALONE):
        return True
    # AND Ï°∞Í±¥: crypto + impact ÎèôÏãú
    has_crypto = _kw_match(t, _RE_CRYPTO_CORE)
    has_impact = _kw_match(t, _RE_IMPACT)
    if has_crypto and has_impact:
        return True
    # Í∏∞Ï°¥ KEYWORDS fallback (ÌïòÏúÑÌò∏Ìôò)
    if active_keywords is not None:
        return _kw_match(t, _get_active_kw_regex(active_keywords))
    return _kw_match(t, _RE_KEYWORDS)

def extract_keywords(title: str, active_keywords=None):
    t = (title or "").lower()
    kw_list = active_keywords if active_keywords is not None else KEYWORDS
    hits = [k for k in kw_list if re.search(
        r'\b' + re.escape(k) + r'\b' if len(k) <= _SHORT_KW_THRESHOLD and k.isascii()
        else re.escape(k), t, re.IGNORECASE)]
    return hits

def get_openai():
    key = os.getenv("OPENAI_API_KEY", "")
    if not key:
        return None
    return OpenAI(api_key=key)

def llm_analyze(client, title):
    prompt = {
        "title": title,
        "task": "ÎπÑÌä∏ÏΩîÏù∏ ÏÑ†Î¨ºÏóê ÎØ∏Ïπ† ÏòÅÌñ• ÌèâÍ∞Ä",
        "schema": {
            "impact_score": "0~10 (0=Î¨¥Í¥Ä, 5=Î≥¥ÌÜµ, 8+=ÎÜíÏùå)",
            "direction": "up/down/neutral",
            "category": "WAR/US_POLITICS/US_POLITICS_ELECTION/US_FISCAL/US_SCANDAL_LEGAL/FED_RATES/CPI_JOBS/NASDAQ_EQUITIES/TECH_NASDAQ/REGULATION_SEC_ETF/JAPAN_BOJ/CHINA/EUROPE_ECB/FIN_STRESS/WALLSTREET_SIGNAL/IMMIGRATION_POLICY/MACRO_RATES/CRYPTO_SPECIFIC/OTHER",
            "relevance": "HIGH/MED/LOW/GOSSIP ‚Äî ÏïîÌò∏ÌôîÌèê/Í±∞ÏãúÍ≤ΩÏ†ú Î¨¥Í¥ÄÏù¥Î©¥ GOSSIP",
            "impact_path": "Ïòà: Í∏àÎ¶¨Ïù∏ÏÉÅ‚ÜíÎã¨Îü¨Í∞ïÏÑ∏‚ÜíBTCÌïòÎùΩ",
            "summary_kr": "ÌïúÍµ≠Ïñ¥ 1~2Î¨∏Ïû•",
            "title_ko": "Îâ¥Ïä§ Ï†úÎ™© ÌïúÍµ≠Ïñ¥ Î≤àÏó≠",
            "tier": "TIER1/TIER2/TIER3/TIERX Î∂ÑÎ•ò",
            "relevance_score": "0.0~1.0 BTC ÏÑ†Î¨º Ïã§Ïßà Ïó∞Í¥ÄÎèÑ",
            "topic_class": "macro/crypto/noise ‚Äî 3-way ÎåÄÎ∂ÑÎ•ò",
            "asset_relevance": "BTC_DIRECT/BTC_INDIRECT/NONE",
        },
        "tier_guide": {
            "TIER1": "Ïó∞Ï§Ä/FOMC/Powell, CPI/PPI/NFP ÌïµÏã¨ÏßÄÌëú, BTC ETF ÎåÄÍ∑úÎ™® ÏûêÍ∏àÌùêÎ¶Ñ, SEC/Í∑úÏ†ú, ÏßÄÏ†ïÌïô(Ï†ÑÏüÅÍ∏â), ÎåÄÌòïÍ∏∞Í¥Ä BTC Îß§Ïàò/Îß§ÎèÑ, Íµ≠Ï±Ñ Í∏àÎ¶¨ Í∏âÎ≥Ä(MACRO_RATES), Î∂ÄÏ±ÑÌïúÎèÑ ÏúÑÍ∏∞(US_FISCAL)",
            "TIER2": "ÎÇòÏä§Îã•/QQQ 1%+ Î≥ÄÎèô ÏõêÏù∏, Í∏àÏúµÏãúÏä§ÌÖú Î¶¨Ïä§ÌÅ¨(ÏùÄÌñâ/Ï±ÑÍ∂å Í∏âÎ≥Ä), Ï£ºÏöîÍµ≠ Ï†ïÏ±Ö, ÎåÄÏÑ†/Ï§ëÍ∞ÑÏÑ†Í±∞(US_POLITICS_ELECTION), Í∏∞Ïà†/Î∞òÎèÑÏ≤¥(TECH_NASDAQ), Í∏∞ÏÜå/ÌÉÑÌïµ(US_SCANDAL_LEGAL), ÏõîÍ∞Ä ÏãúÍ∑∏ÎÑê(WALLSTREET_SIGNAL), Ïù¥ÎØºÏ†ïÏ±Ö(IMMIGRATION_POLICY)",
            "TIER3": "ÏùºÎ∞ò ÌÅ¨Î¶ΩÌÜ† ÏãúÌô©, BTC ÏßÅÏ†ë Ïó∞Í≤∞ ÏïΩÌïú Í∞úÎ≥Ñ Í∏∞ÏóÖ/Ïù¥Ïäà",
            "TIERX": "Í∞úÏù∏ÏÇ¨Ïó∞, Ï£ºÏãùÏ∂îÏ≤ú, ÏπºÎüº, ÌÅ¥Î¶≠Ïú†ÎèÑ, ÎÖ∏Ïù¥Ï¶à, ÏïîÌò∏ÌôîÌèê/Í±∞ÏãúÍ≤ΩÏ†ú Î¨¥Í¥Ä",
        },
        "classification_rules": (
            "AI Ï£ºÏãù Ï∂îÏ≤ú, Ï§ëÍµ≠ Í∏∞ÏóÖ ÎπÑÍµê, Í∞úÏù∏ Ìà¨Ïûê Ïä§ÌÜ†Î¶¨Îäî topic_class=noiseÎ°ú Î∂ÑÎ•ò. "
            "bitcoin/btc/crypto Îã®Ïñ¥ Ìè¨Ìï®ÎßåÏúºÎ°ú asset_relevance=BTC_DIRECT Î∂ÑÎ•ò Í∏àÏßÄ. "
            "BTC ETF, BTC Î∞òÍ∞êÍ∏∞, BTC ÏßÅÏ†ë Í∑úÏ†úÎßå CRYPTO_SPECIFIC+BTC_DIRECT. "
            "ÏùºÎ∞ò AI/Í∏∞Ïà†Ï£º Îâ¥Ïä§Îäî asset_relevance=NONE."
        ),
    }
    resp = client.responses.create(
        model="gpt-4o-mini",
        input=json.dumps(prompt, ensure_ascii=False)
    )
    text = resp.output_text or ""
    try:
        data = json.loads(text)
        impact = int(data.get("impact_score", 0) or 0)
        direction = (data.get("direction", "neutral") or "neutral").strip()
        category = (data.get("category", "OTHER") or "OTHER").strip()
        relevance = (data.get("relevance", "MED") or "MED").strip().upper()
        impact_path = (data.get("impact_path", "") or "").strip()
        summary_kr = (data.get("summary_kr", "") or "").strip()
        title_ko = (data.get("title_ko", "") or "").strip()
        tier = (data.get("tier", "UNKNOWN") or "UNKNOWN").strip().upper()
        try:
            rel_score = float(data.get("relevance_score", 0) or 0)
            rel_score = max(0.0, min(1.0, rel_score))
        except (TypeError, ValueError):
            rel_score = 0.0
        topic_class = (data.get("topic_class", "noise") or "noise").strip().lower()
        if topic_class not in ('macro', 'crypto', 'noise'):
            topic_class = 'noise'
        asset_relevance = (data.get("asset_relevance", "NONE") or "NONE").strip().upper()
        if asset_relevance not in ('BTC_DIRECT', 'BTC_INDIRECT', 'NONE'):
            asset_relevance = 'NONE'
        if not title_ko:
            title_ko = summary_kr.split('.')[0] if summary_kr else ""
        # Encode category + impact_path into summary field
        summary = f"[{direction}] [{category}] {summary_kr}"
        if impact_path:
            summary += f" | {impact_path}"
        return impact, direction, summary, title_ko, relevance, tier, rel_score, topic_class, asset_relevance
    except Exception:
        return 0, "neutral", text[:200], "", "MED", "UNKNOWN", 0.0, "noise", "NONE"

def ensure_table(db):
    """
    ÌòÑÏû¨ Ïã§DBÏóê Ï°¥Ïû¨ÌïòÎäî Ïä§ÌÇ§ÎßàÏôÄ ÎßûÏ∂§:
      id, ts(timestamptz), source, title, url, summary, impact_score, keywords(text[])
    (ÌÖåÏù¥Î∏îÏù¥ ÏóÜÏùÑ ÎïåÎßå ÏÉùÏÑ±)
    """
    with db.cursor() as cur:
        cur.execute("""
        CREATE TABLE IF NOT EXISTS public.news (
          id BIGSERIAL PRIMARY KEY,
          ts TIMESTAMPTZ NOT NULL DEFAULT now(),
          source TEXT,
          title TEXT,
          url TEXT UNIQUE,
          summary TEXT,
          impact_score INT,
          keywords TEXT[],
          title_ko TEXT
        );
        """)
        db.commit()

def db_news_summary(db, minutes=60, limit=20) -> str:
    with db.cursor() as cur:
        cur.execute("""
            WITH recent AS (
              SELECT id, ts, source, COALESCE(impact_score,0) AS impact_score,
                     COALESCE(title_ko, title, '(Ï†úÎ™© ÏóÜÏùå)') AS display_title, url
              FROM public.news
              WHERE ts >= now() - (%s || ' minutes')::interval
              ORDER BY id DESC
              LIMIT %s
            )
            SELECT
              'üì∞ DB Îâ¥Ïä§ (ÏµúÍ∑º ' || %s || 'Î∂Ñ) Í±¥Ïàò=' || (SELECT count(*) FROM recent) || E'\n'
              || COALESCE(string_agg(
                  '‚Ä¢ [' || to_char(ts AT TIME ZONE 'Asia/Seoul', 'MM-DD HH24:MI') || ' KST] '
                  || '(' || impact_score || ') '
                  || display_title || E'\n  ' || COALESCE(url,'')
                , E'\n'), '‚Ä¢ (ÏóÜÏùå)')
            FROM recent;
        """, (str(minutes), int(limit), str(minutes)))
        row = cur.fetchone()
        return (row[0] if row and row[0] else "üì∞ DB Îâ¥Ïä§\n‚Ä¢ (ÏóÜÏùå)")

def run_summary_once():
    db = get_conn(autocommit=True)
    ensure_table(db)
    minutes = int(os.getenv("NEWS_SUMMARY_MINUTES", "60"))
    limit = int(os.getenv("NEWS_SUMMARY_LIMIT", "20"))
    print(db_news_summary(db, minutes=minutes, limit=limit))
    db.close()


def _ensure_conn(db):
    """DB Ïª§ÎÑ•ÏÖò ÏÉÅÌÉú ÌôïÏù∏ ÌõÑ ÎÅäÍ≤ºÏúºÎ©¥ Ïû¨Ïó∞Í≤∞."""
    if db is None or db.closed:
        log("[news_bot] DB Ïû¨Ïó∞Í≤∞ ÏãúÎèÑ...")
        return get_conn(autocommit=True)
    try:
        with db.cursor() as cur:
            cur.execute("SELECT 1")
        return db
    except (psycopg2.OperationalError, psycopg2.InterfaceError):
        log("[news_bot] DB Ïª§ÎÑ•ÏÖò ÎÅäÍπÄ, Ïû¨Ïó∞Í≤∞...")
        try:
            db.close()
        except Exception:
            pass
        return get_conn(autocommit=True)


def main():
    # ÏöîÏïΩÎßå Ï∂úÎ†•ÌïòÍ≥† Ï¢ÖÎ£å (ÌÖîÎ†àÍ∑∏Îû® "DB Îâ¥Ïä§ ÏöîÏïΩ"Ïö©)
    if len(sys.argv) > 1 and sys.argv[1] == "--summary":
        run_summary_once()
        return

    log(f"[news_bot] START poll={NEWS_POLL_SEC}s")
    client = get_openai()
    if client is None:
        log("[news_bot] NOTE: OPENAI_API_KEY ÏóÜÏùå/ÏûêÎ¶¨ÌëúÏãúÏûê -> LLM ÏóÜÏù¥ Ï†ÄÏû•Îßå ÏßÑÌñâ")

    db = get_conn(autocommit=True)
    ensure_table(db)

    while True:
        try:
            # Îß§ TICKÎßàÎã§ Ïª§ÎÑ•ÏÖò ÏÉÅÌÉú ÌôïÏù∏
            db = _ensure_conn(db)
        except Exception as e:
            log(f"[news_bot] DB Ïû¨Ïó∞Í≤∞ Ïã§Ìå®: {e}")
            time.sleep(30)
            continue

        log("[news_bot] TICK")
        inserted = 0
        db_errors = 0
        duplicate_ignored = 0
        circuit_break = False
        skipped_gossip = 0
        skipped_low_relevance = 0
        skipped_hard_exclude = 0
        skipped_keyword_and = 0
        active_keywords = _load_watch_keywords(db)
        log(f"[news_bot] active keywords: {len(active_keywords)}")

        for source, url in FEEDS:
            if circuit_break:
                break
            try:
                feed = feedparser.parse(url, agent=FEED_AGENT)
                entries = getattr(feed, "entries", []) or []
                log(f"[news_bot] feed={source} entries={len(entries)}")
                _record_event('fetch_ok')
                _stats['last_fetch_ok_ts'] = time.strftime('%Y-%m-%d %H:%M:%S')
            except Exception as _feed_err:
                log("[news_bot] ERROR feed parse")
                log(traceback.format_exc())
                _record_event('fetch_fail')
                _record_raw_error(db, source, 'feed_parse_error',
                                  exception_msg=str(_feed_err)[:200])
                _check_error_threshold('feed_parse', source, str(_feed_err))
                continue

            # Batch URL dedup: collect all URLs first, then check DB in one query
            _feed_entries = []
            for e in entries[:20]:
                raw_title = getattr(e, "title", "") or ""
                link = (getattr(e, "link", "") or "").strip()
                # Input validation: sanitize title, validate link
                title = _sanitize_title(raw_title)
                if not title or len(title) < 3:
                    log(f"[news_bot] DEBUG skip bad title: {raw_title[:60]!r}")
                    _record_event('parse_fail')
                    _record_raw_error(db, source, 'bad_title',
                                      raw_title=raw_title, raw_url=link)
                    continue
                if not link or not link.startswith('http'):
                    log(f"[news_bot] DEBUG skip bad link: {link[:60]!r}")
                    _record_event('parse_fail')
                    _record_raw_error(db, source, 'bad_link',
                                      raw_title=title, raw_url=link)
                    continue
                _feed_entries.append((title, link))
            _existing_urls = set()
            if _feed_entries:
                try:
                    _all_urls = [link for _, link in _feed_entries]
                    with db.cursor() as cur:
                        cur.execute("SELECT url FROM public.news WHERE url = ANY(%s)", (_all_urls,))
                        _existing_urls = {r[0] for r in cur.fetchall()}
                except Exception:
                    pass  # fallback: treat all as new

            for title, link in _feed_entries:
                if link in _existing_urls:
                    continue

                try:

                    # 2) ÌïòÎìú Ï†úÏô∏ Ìå®ÌÑ¥ (GPT Ìò∏Ï∂ú Ï†Ñ)
                    if _is_hard_excluded(title):
                        skipped_hard_exclude += 1
                        continue

                    # 3) Í∞ÄÏã≠/ÎÖ∏Ïù¥Ï¶à ÌïòÎìú ÌïÑÌÑ∞ (GPT Ìò∏Ï∂ú Ï†Ñ)
                    if _is_gossip(title):
                        skipped_gossip += 1
                        continue

                    # 4) ÏÜåÏä§ Ìã∞Ïñ¥
                    source_tier = _get_source_tier(source)

                    # 5) AND-Í∏∞Î∞ò ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨ (crypto ÌîºÎìúÎäî Î∞îÏù¥Ìå®Ïä§)
                    if not worth_llm(title, active_keywords, source):
                        skipped_keyword_and += 1
                        continue

                    # 6) GPT Î∂ÑÎ•ò
                    impact, direction, summary, title_ko, relevance = 0, "neutral", "", "", "MED"
                    tier, rel_score = "UNKNOWN", 0.0
                    topic_class, asset_relevance = "noise", "NONE"
                    if client:
                        try:
                            impact, direction, summary, title_ko, relevance, tier, rel_score, topic_class, asset_relevance = llm_analyze(client, title)
                        except Exception as llm_err:
                            log(f"[news_bot] LLM error: {llm_err}")

                    # 7) ÏÜåÏä§ Í∞ÄÏ§ëÏπò Í∏∞Î∞ò Ìã∞Ïñ¥ Ï∫° (weight < 0.6 ‚Üí TIER3)
                    _DENY_THRESHOLD = 0.60
                    try:
                        _sw = _ncc.get_source_weight(source)
                        _DENY_THRESHOLD = _ncc.DENY_SOURCE_WEIGHT_THRESHOLD
                    except Exception:
                        _sw = 0.55 if source_tier == 'REFERENCE_ONLY' else 0.70
                    if _sw < _DENY_THRESHOLD and tier in ('TIER1', 'TIER2'):
                        tier = 'TIER3'

                    # 8) Ïú†Ìö® Ìã∞Ïñ¥ Í∞ÄÎìú
                    valid_tiers = {'TIER1', 'TIER2', 'TIER3', 'TIERX'}
                    if tier not in valid_tiers:
                        tier = 'UNKNOWN'

                    # 9) exclusion_reason ‚Äî v2 three-condition deny
                    exclusion_reason = None
                    _low_tier = tier in ('TIER3', 'TIERX')
                    _low_rel = (rel_score > 0 and rel_score < 0.55) or relevance in ('GOSSIP', 'LOW')
                    _low_src = _sw < _DENY_THRESHOLD  # from step 7

                    if tier == 'TIERX':
                        exclusion_reason = 'TIERX: noise/column/stock_pick'
                    elif _low_tier and _low_rel and _low_src:
                        exclusion_reason = f'triple_low: tier={tier} rel={rel_score:.2f} w={_sw:.2f}'

                    # Ï†úÏô∏Îêú Îâ¥Ïä§ÎèÑ DBÏóê Ï†ÄÏû• (Ï∂îÏ†ÅÏö©), Ïπ¥Ïö¥ÌÑ∞ Ï¶ùÍ∞Ä
                    if exclusion_reason:
                        skipped_low_relevance += 1

                    kw = extract_keywords(title, active_keywords)

                    # 9.5) Shadow classifier ‚Äî preview_classify (two-tier allow)
                    allow_storage = False
                    allow_trading = False
                    _shadow = {}
                    try:
                        _shadow = _ncc.preview_classify(
                            title, source, impact,
                            summary=summary or '', title_ko=title_ko or '')
                        # If GPT returned generic/noise topic, use shadow's detailed classification
                        _shadow_topic_preview = _shadow.get('topic_class_preview', 'unclassified')
                        if _shadow_topic_preview not in ('unclassified', '', None):
                            if topic_class in ('noise', 'macro', '', None):
                                # Normalize shadow enum ‚Üí GPT enum for consistency
                                topic_class = _SHADOW_TO_GPT_CATEGORY.get(
                                    _shadow_topic_preview, _shadow_topic_preview)
                        # If GPT didn't assign tier, use shadow
                        if tier in ('UNKNOWN', None) and _shadow.get('tier_preview'):
                            tier = _shadow['tier_preview']
                        # If relevance_score is missing, use shadow
                        if (not rel_score or rel_score <= 0) and _shadow.get('relevance_score_preview'):
                            rel_score = _shadow['relevance_score_preview']
                        # Two-tier allow decisions
                        allow_storage = _shadow.get('allow_for_storage', False)
                        allow_trading = _shadow.get('allow_for_trading', False)
                    except Exception:
                        pass  # shadow classifier failure should never block insertion

                    # 9.6) Scandal confirmation gate
                    _shadow_topic = _shadow.get('topic_class_preview', '')
                    _scandal_topic = _shadow_topic if _shadow_topic == 'US_SCANDAL_LEGAL' else (
                        topic_class if topic_class == 'US_SCANDAL_LEGAL' else '')
                    if _scandal_topic == 'US_SCANDAL_LEGAL':
                        _scandal_cur = None
                        try:
                            try:
                                _scandal_cur = db.cursor()
                            except Exception:
                                pass
                            _scandal = _ncc.scandal_confirmation_check(
                                'US_SCANDAL_LEGAL', title, source, impact,
                                db_cursor=_scandal_cur)
                            if not _scandal['confirmed']:
                                allow_trading = False
                                exclusion_reason = exclusion_reason or f"scandal_unconfirmed: {_scandal['reason']}"
                            if _scandal.get('impact_cap') and impact > _scandal['impact_cap']:
                                impact = _scandal['impact_cap']
                        except Exception:
                            pass
                        finally:
                            if _scandal_cur:
                                try:
                                    _scandal_cur.close()
                                except Exception:
                                    pass

                    # 9.7) BTC keyword fallback for still-unclassified items
                    if topic_class in ('unclassified', 'noise', '', None):
                        _btc_kws = {'bitcoin', 'btc', 'crypto', 'cryptocurrency',
                                    'blockchain', 'halving', 'mining', 'defi', 'exchange'}
                        title_lower = (title or '').lower()
                        if any(kw in title_lower for kw in _btc_kws):
                            topic_class = 'CRYPTO_GENERAL'
                            if not allow_storage:
                                allow_storage = True

                    # 10) Compute trading_impact_weight = source_weight * relevance_score
                    try:
                        _src_w = _ncc.get_source_weight(source)
                        _trading_impact_weight = round(_src_w * (rel_score or 0), 4)
                    except Exception:
                        _trading_impact_weight = 0

                    # 11) DB INSERT (tier, relevance_score, source_tier, exclusion_reason, topic_class, asset_relevance, allow_storage, allow_trading, trading_impact_weight)
                    with db.cursor() as cur:
                        cur.execute("""
                            INSERT INTO public.news(source, title, url, summary, impact_score,
                                                    keywords, title_ko, tier, relevance_score,
                                                    source_tier, exclusion_reason,
                                                    topic_class, asset_relevance,
                                                    allow_storage, allow_trading,
                                                    trading_impact_weight)
                            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
                            ON CONFLICT (url) DO UPDATE SET
                                summary = EXCLUDED.summary,
                                impact_score = EXCLUDED.impact_score,
                                keywords = EXCLUDED.keywords,
                                title_ko = EXCLUDED.title_ko,
                                tier = EXCLUDED.tier,
                                relevance_score = EXCLUDED.relevance_score,
                                source_tier = EXCLUDED.source_tier,
                                exclusion_reason = EXCLUDED.exclusion_reason,
                                topic_class = EXCLUDED.topic_class,
                                asset_relevance = EXCLUDED.asset_relevance,
                                allow_storage = EXCLUDED.allow_storage,
                                allow_trading = EXCLUDED.allow_trading,
                                trading_impact_weight = EXCLUDED.trading_impact_weight
                            WHERE EXCLUDED.impact_score >= COALESCE(news.impact_score, 0)
                               OR news.tier IS NULL
                               OR news.topic_class IS NULL
                               OR news.topic_class IN ('macro', 'crypto', 'noise', 'unclassified')
                        """, (
                            source,
                            title,
                            link,
                            summary if summary else f"[{direction}]",
                            int(impact),
                            kw if kw else None,
                            title_ko if title_ko else None,
                            tier,
                            rel_score if rel_score > 0 else None,
                            source_tier,
                            exclusion_reason,
                            topic_class,
                            asset_relevance,
                            allow_storage,
                            allow_trading,
                            _trading_impact_weight,
                        ))
                    inserted += 1
                    _record_event('insert_ok')
                    _stats['last_insert_ok_ts'] = time.strftime('%Y-%m-%d %H:%M:%S')

                except (psycopg2.OperationalError, psycopg2.InterfaceError) as db_err:
                    db_errors += 1
                    if db_errors <= 1:
                        log(f"[news_bot] DB Ïª§ÎÑ•ÏÖò Ïò§Î•ò: {db_err}")
                        _send_error_alert(source, title, db_err)
                    try:
                        db.close()
                    except Exception:
                        pass
                    try:
                        db = get_conn(autocommit=True)
                        log("[news_bot] DB Ïû¨Ïó∞Í≤∞ ÏÑ±Í≥µ")
                    except Exception as re_err:
                        log(f"[news_bot] DB Ïû¨Ïó∞Í≤∞ Ïã§Ìå®: {re_err}")
                        break
                    # Circuit breaker: too many consecutive DB errors
                    if db_errors >= MAX_CONSECUTIVE_DB_ERRORS:
                        backoff = min(MAX_BACKOFF_SEC, BACKOFF_BASE_SEC * (2 ** min(db_errors - MAX_CONSECUTIVE_DB_ERRORS, 5)))
                        log(f"[news_bot] ÏÑúÌÇ∑Î∏åÎ†àÏù¥Ïª§ Î∞úÎèô: Ïó∞ÏÜç DB Ïò§Î•ò {db_errors}Ìöå, {backoff}Ï¥à ÎåÄÍ∏∞")
                        time.sleep(backoff)
                        circuit_break = True
                        break
                    continue

                except psycopg2.DataError as de:
                    log(f"[news_bot] WARN DataError (skip): {de}")
                    _record_event('parse_fail')
                    _record_raw_error(db, source, 'DataError',
                                      raw_title=title, raw_url=link,
                                      exception_msg=str(de)[:200])
                    _check_error_threshold('DataError', source, str(de))
                    db_errors += 1
                    continue

                except psycopg2.IntegrityError as ie:
                    # UniqueViolation (23505) = duplicate = normal operation
                    if ie.pgcode == '23505':
                        duplicate_ignored += 1
                        _record_event('duplicate_ignored')
                        log(f"[news_bot] DEBUG duplicate skip: {title[:60]}")
                    else:
                        # Other IntegrityErrors (check constraint, FK, etc.) = real error
                        log(f"[news_bot] WARN IntegrityError (pgcode={ie.pgcode}): {ie}")
                        _record_event('parse_fail')
                        _record_raw_error(db, source, f'IntegrityError_{ie.pgcode}',
                                          raw_title=title, raw_url=link,
                                          exception_msg=str(ie)[:200])
                        _check_error_threshold('IntegrityError', source, str(ie))
                        db_errors += 1
                    continue

                except Exception as ex:
                    log(f"[news_bot] ERROR insert: {ex}")
                    log(traceback.format_exc())
                    _record_event('parse_fail')
                    _stats['last_error_type'] = type(ex).__name__
                    _stats['last_error_source'] = source
                    _stats['last_error_msg'] = str(ex)[:200]
                    _stats['last_error_ts'] = time.strftime('%Y-%m-%d %H:%M:%S')
                    _check_error_threshold(type(ex).__name__, source, str(ex))
                    db_errors += 1
                    continue

        # Flush stats file each tick
        _flush_stats()
        log(f"[news_bot] DONE inserted={inserted}, duplicate_ignored={duplicate_ignored}, skipped_hard_exclude={skipped_hard_exclude}, skipped_gossip={skipped_gossip}, skipped_keyword_and={skipped_keyword_and}, skipped_low_relevance={skipped_low_relevance}, db_errors={db_errors}, sleep={NEWS_POLL_SEC}s")
        time.sleep(NEWS_POLL_SEC)

if __name__ == "__main__":
    main()
